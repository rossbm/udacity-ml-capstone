{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook develops artificial neural network (ANN) models. The focus will be on recurrent neural networks (RNNs). The idea is that in order to properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Load Packages and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import luigi\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize, wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras import optimizers\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.clean import CleanData\n",
    "from src.data.download import DownloadFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Luigi tasks ensures that the cleaned test and train sets are available, and produces them if they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if CleanData() is complete\n",
      "INFO: Informed scheduler that task   CleanData__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=838591748, workers=1, host=DESKTOP-6UJS098, username=wertu, pid=4416) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 CleanData()\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([CleanData()], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have ensutre that the data is present, load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "train = joblib.load('data/interim/train.pkl')\n",
    "test = joblib.load('data/interim/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Luigi task ensures that the Glove embeddings are availables, and downloads them if they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if DownloadFile(out_name=data/external/GloveVectors, url=http://nlp.stanford.edu/data/glove.6B.zip, filetype=zip) is complete\n",
      "INFO: Informed scheduler that task   DownloadFile_zip_data_external_Gl_http___nlp_stanf_8240f740b5   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=442427354, workers=1, host=DESKTOP-6UJS098, username=wertu, pid=4416) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 DownloadFile(out_name=data/external/GloveVectors, url=http://nlp.stanford.edu/data/glove.6B.zip, filetype=zip)\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([DownloadFile(url='http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "                           out_name='data/external/GloveVectors', filetype='zip')], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Create Emedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn countvectorizer to create disctironary of tokens with indexes. Use nltk tokenizxer isntead of built in tokeinzer  in skleanr sinxze nltk doenst, comeply igonore puntation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=wordpunct_tokenize, min_df=5)\n",
    "vectorizer.fit(train[\"full_text\"])\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does stuff\n",
    "#it tkaes two iunputs, path to embeding file and owd index\n",
    "#word index is a dictionary, with key a word and item as index number \n",
    "#this funtion reads the embedding file into a dictionary, one key perline/word\n",
    "#item is ebedding\n",
    "#after doing this, creates an embedding matrix with nuimber of rows equal to number of word ined\n",
    "# it then iterates over \n",
    "def create_embedmatrix(embedding_file, word_index):\n",
    "    #word embedding\n",
    "    embeddings_index = {}\n",
    "    not_found = {}\n",
    "    f = open(embedding_file, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    \n",
    "    #now make embedding\n",
    "    #reserving row 0 for for the padding character - will be all 0s so can be masked laer\n",
    "    #reserving row 1 for words that are not in vocab (will bve al 0s and masked alter)\n",
    "    #words that are in vocab but not in the embeddings will get their own rows\n",
    "    #might want to train them later (intitliaze with random)\n",
    "    #make from truncated normal, parameters from loading the embddeing\n",
    "    lower = -2\n",
    "    upper = 2\n",
    "    mu = 0.0 # mean\n",
    "    sigma = 0.5 #standard deviation\n",
    "    embedding_matrix = scipy.stats.truncnorm.rvs(\n",
    "              (lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma,size=(len(word_index),embedding_dim))\n",
    "    #make first row all zeroes, for masking of padding\n",
    "    embedding_matrix[0,:] = 0.\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i+1] = embedding_vector\n",
    "        else:\n",
    "            not_found[word] = not_found.get(word, 0) + 1   \n",
    "    return embedding_matrix, embedding_dim, not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, embedding_dim, not_found = create_embedmatrix('data/external/GloveVectors/glove.6B.200d.txt', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows the need to be able to update word embeddings, isntead of just ignoring..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform into sequences. Should really make into pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_seqs(texts, vocab, max_len):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.append(wordpunct_tokenize(text.lower()))\n",
    "    seqs = np.zeros((len(tokens), max_len), dtype=np.int32)\n",
    "    \n",
    "    for i, text in enumerate(tokens):\n",
    "        for j, word in enumerate(text):\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            #need to increment by 1 since first row in embedding matrix is reserved\n",
    "            #if word doesn't exist, it will return -1, whicll be incrmented to 1\n",
    "            seqs[i,j] = vocab.get(word, -1) + 2\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs = create_seqs(train[\"full_text\"], vocab, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need validation set\n",
    "seqs_train, seqs_valid, y_train, y_valid = train_test_split(seqs, train.funny.values, test_size = 0.125, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep it very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, Masking, BatchNormalization\n",
    "from keras.layers import Dropout, Embedding, SpatialDropout1D\n",
    "from keras.layers import  LSTM\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "### TODO: Train the model.\n",
    "\n",
    "\n",
    "adam = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(L2):\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False, name = 'embedding')\n",
    "    \n",
    "    sequence_input = Input(shape=(max_len,), dtype='int32', name='joke_seq')\n",
    "\n",
    "    embedded_input = embedding_layer(sequence_input)\n",
    "\n",
    "    mask_pads = Masking(mask_value=0.)(embedded_input)\n",
    "\n",
    "    lstm = LSTM(200, implementation=2, unroll=True)(mask_pads)\n",
    "\n",
    "    dense = Dense(200, activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=l2(L2))(lstm)\n",
    "\n",
    "    preds = Dense(1, activation = 'sigmoid',\n",
    "                 kernel_regularizer=l2(L2))(dense)\n",
    "\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iteration 0 with an L2 lambda of 0.10000000 and a patience of 8\n",
      "\n",
      "Train on 171668 samples, validate on 24524 samples\n",
      "Epoch 1/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 4.6827 - acc: 0.5948 - binary_crossentropy: 0.6687Epoch 00001: val_binary_crossentropy improved from inf to 0.68080, saving model to models/neural_decrease_l2.hdf5\n",
      "171668/171668 [==============================] - 161s 935us/step - loss: 4.6672 - acc: 0.5948 - binary_crossentropy: 0.6687 - val_loss: 0.6966 - val_acc: 0.5756 - val_binary_crossentropy: 0.6808\n",
      "Epoch 2/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.5977 - binary_crossentropy: 0.6711Epoch 00002: val_binary_crossentropy improved from 0.68080 to 0.65909, saving model to models/neural_decrease_l2.hdf5\n",
      "171668/171668 [==============================] - 121s 702us/step - loss: 0.6833 - acc: 0.5977 - binary_crossentropy: 0.6711 - val_loss: 0.6747 - val_acc: 0.6075 - val_binary_crossentropy: 0.6591\n",
      "Epoch 3/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 0.6766 - acc: 0.6006 - binary_crossentropy: 0.6632Epoch 00003: val_binary_crossentropy improved from 0.65909 to 0.65646, saving model to models/neural_decrease_l2.hdf5\n",
      "171668/171668 [==============================] - 119s 694us/step - loss: 0.6766 - acc: 0.6006 - binary_crossentropy: 0.6632 - val_loss: 0.6718 - val_acc: 0.6092 - val_binary_crossentropy: 0.6565\n",
      "Epoch 4/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.6098 - binary_crossentropy: 0.6557Epoch 00004: val_binary_crossentropy improved from 0.65646 to 0.65169, saving model to models/neural_decrease_l2.hdf5\n",
      "171668/171668 [==============================] - 119s 693us/step - loss: 0.6709 - acc: 0.6098 - binary_crossentropy: 0.6557 - val_loss: 0.6671 - val_acc: 0.6164 - val_binary_crossentropy: 0.6517\n",
      "Epoch 5/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 0.6673 - acc: 0.6143 - binary_crossentropy: 0.6513Epoch 00005: val_binary_crossentropy improved from 0.65169 to 0.64614, saving model to models/neural_decrease_l2.hdf5\n",
      "171668/171668 [==============================] - 119s 695us/step - loss: 0.6672 - acc: 0.6143 - binary_crossentropy: 0.6513 - val_loss: 0.6636 - val_acc: 0.6198 - val_binary_crossentropy: 0.6461\n",
      "Epoch 6/100\n",
      "171000/171668 [============================>.] - ETA: 0s - loss: 0.6665 - acc: 0.6175 - binary_crossentropy: 0.6498Epoch 00006: val_binary_crossentropy did not improve\n",
      "171668/171668 [==============================] - 118s 690us/step - loss: 0.6665 - acc: 0.6175 - binary_crossentropy: 0.6498 - val_loss: 0.6647 - val_acc: 0.6216 - val_binary_crossentropy: 0.6498\n",
      "Epoch 7/100\n",
      "138000/171668 [=======================>......] - ETA: 21s - loss: 0.6719 - acc: 0.6062 - binary_crossentropy: 0.6573"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-21b6fb5ff1b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     history = model.fit(x = seqs_train, y = y_train, epochs=100, batch_size=1500,\n\u001b[1;32m---> 45\u001b[1;33m                     validation_data=(seqs_valid, y_valid), callbacks=[checkpointer, earlystopper], verbose=1)\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mbest_val_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_binary_crossentropy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#once validation loss stops decreasing, resume training with decrease amount of regularization\n",
    "#stop this proicess once regulriatoin is really low, or validation loss has not improved....\n",
    "#starting with a L2 lambda of 0.1, and going to a min of 1e-8, means that there will 73 iterations....\n",
    "model_path=\"models/neural_decrease_l2.hdf5\"\n",
    "model_prev_path = \"models/neural_decrease_l2_prev.hdf5\"\n",
    "history_list = list()\n",
    "best_val_loss_list = list()\n",
    "prev_best_val_loss = np.inf\n",
    "i = 0\n",
    "L2 = 0.1\n",
    "checkpointer = ModelCheckpoint(filepath=model_path,\n",
    "                               monitor='val_binary_crossentropy',\n",
    "                               verbose=1,\n",
    "                               mode='min',\n",
    "                               save_best_only=True)\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_binary_crossentropy',\n",
    "                             min_delta=0,\n",
    "                             patience=3,\n",
    "                             verbose=1,\n",
    "                             mode='min')\n",
    "\n",
    "while L2 > 1e-8:\n",
    "    #the stronger the reg, more patience\n",
    "    patience = max(3, int(8-i/8))\n",
    "    print(\"\\nStarting iteration {0:} with an L2 lambda of {1:0.8f} and a patience of {2:}\\n\".format(i, L2, patience))\n",
    "\n",
    "    model = create_model(L2)\n",
    "    if i != 0:\n",
    "        model.load_weights(model_path_prev)\n",
    "        \n",
    "\n",
    "    \n",
    "    earlystopper = EarlyStopping(monitor='val_binary_crossentropy',\n",
    "                             min_delta=0,\n",
    "                             patience=patience,\n",
    "                             verbose=1,\n",
    "                             mode='min')\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc', losses.binary_crossentropy])\n",
    "    \n",
    "    history = model.fit(x = seqs_train, y = y_train, epochs=100, batch_size=2500,\n",
    "                    validation_data=(seqs_valid, y_valid), callbacks=[checkpointer, earlystopper], verbose=1)\n",
    "    best_val_loss = min(history.history[\"val_binary_crossentropy\"])\n",
    "    \n",
    "    print(\"\\nValidation loss has gone from {0:0.5f} to {1:0.5f}\\n\".format(prev_best_val_loss, best_val_loss))\n",
    "    \n",
    "    if best_val_loss > prev_best_val_loss:\n",
    "        print(\"\\nValidation loss has NOT improved. Ignoring new history\\n\")\n",
    "    else:\n",
    "        print(\"\\nValidation HAS improved. Incorporating new history\\n\")\n",
    "        #save model (even the most recent iterations that have not led to an increase in validation score)\n",
    "        #the checkpointer will ensure that we always have model with best validation score on hand\n",
    "        #at model_path\n",
    "        #saving to \"prev\" path, this is what is load and used for next iteration\n",
    "        model.save(model_prev_path)\n",
    "        best_val_loss_list.append(best_val_loss)\n",
    "        history_list.append(history.history)\n",
    "        prev_best_val_loss = best_val_loss\n",
    "        \n",
    "    L2 = L2 * 0.8\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(history.history[\"val_binary_crossentropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
