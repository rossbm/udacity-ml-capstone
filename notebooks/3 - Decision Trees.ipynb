{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Prelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import luigi\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import numpy as np\n",
    "PROJECT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "os.chdir(PROJECT_DIR)\n",
    "from src.features.dtm import CreateDTM\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"C:\\\\Users\\\\wertu\\\\Documents\\\\Datascience\\\\udacity-ml-capstone\"\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if CreateDTM() is complete\n",
      "INFO: Informed scheduler that task   CreateDTM__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=480276968, workers=1, host=DESKTOP-6UJS098, username=wertu, pid=21392) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 CreateDTM()\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This task ensures that the document-term matrices for both train and test are available.\n",
    "luigi.build([CreateDTM()], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "features = joblib.load(\"data/processed/dtm_features.pkl\")\n",
    "train = joblib.load(\"data/interim/train.pkl\")\n",
    "test = joblib.load(\"data/interim/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks explores whether there are non-linear relationships between words that could explain the \"funniniess\" of a joke. Decision trees are a good algorithm for non linearities.\n",
    "\n",
    "Pariualry good since they can produce good praiblity sestimes, that are neded for cimputing aureau udner the cruve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a validation set to be used for parameter searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Basic Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I first train some \"basic\" trees. A single decision tree by itself is prone to overfitting. Therefore some pruning is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 3.1 - Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 - Bagged Trees (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that I will try will be random forest. Random forests are good because they do not require many hyper parmerts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The random forests algorithm, which is also an ensemble-of-trees method, is generally regarded to be among the very best\n",
    "commonly used classifiers (Manuel FernÂ´andez-Delgado and Amorim, 2014)\" from \"AdaBoost and Random Forests: the Power of Interpolation<\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite random forests not needing too many\n",
    "\n",
    "I will use the out of bag error as the criteria. I will search over some parameters.\n",
    "\n",
    "The morst import parameters are the numer of trees (n_estimators) and the number of features that are considered at each split (max_features). For max_features, I will try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_\"The study of error estimates for bagged classifiers in Breiman [1996b], gives empirical evidence to show that the out-   of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log10 = int(log(features[\"train\"].shape[0],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators' : [50, 100, 300, 500, 1000],\n",
    "         'max_features':['log2', log10]}\n",
    "grid = (dict(zip(params, x)) for x in product(*params.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'max_features': 'log2'}\n",
      "0.659267452292\n",
      "{'n_estimators': 50, 'max_features': 5}\n",
      "0.657677173381\n",
      "{'n_estimators': 100, 'max_features': 'log2'}\n",
      "0.677051052031\n",
      "{'n_estimators': 100, 'max_features': 5}\n",
      "0.675608587506\n",
      "{'n_estimators': 300, 'max_features': 'log2'}\n",
      "0.689640760072\n",
      "{'n_estimators': 300, 'max_features': 5}\n",
      "0.690823275159\n",
      "{'n_estimators': 500, 'max_features': 'log2'}\n",
      "0.692923258848\n",
      "{'n_estimators': 500, 'max_features': 5}\n",
      "0.69522712445\n",
      "{'n_estimators': 1000, 'max_features': 'log2'}\n",
      "0.695813284945\n",
      "{'n_estimators': 1000, 'max_features': 5}\n",
      "0.696812306312\n",
      "Wall time: 7h 18min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for params in grid:\n",
    "    print(params)\n",
    "    rf = RandomForestClassifier(**params, n_jobs=10, oob_score = True)\n",
    "    rf.fit(features[\"train\"], train.funny)\n",
    "    score = rf.oob_score_\n",
    "    print(score)\n",
    "    results.append((params, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69965748537177108"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(features[\"test\"], test.funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'max_features': 'log2'}\n",
      "0.660500937857\n",
      "{'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "0.659466237155\n",
      "{'n_estimators': 50, 'max_features': 6}\n",
      "0.65897182352\n",
      "{'n_estimators': 100, 'max_features': 'log2'}\n",
      "0.67555251998\n",
      "{'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "0.672203759582\n",
      "{'n_estimators': 100, 'max_features': 6}\n",
      "0.676836976023\n",
      "{'n_estimators': 300, 'max_features': 'log2'}\n",
      "0.690430802479\n",
      "{'n_estimators': 300, 'max_features': 'sqrt'}\n",
      "0.680787188061\n",
      "{'n_estimators': 300, 'max_features': 6}\n",
      "0.689656051215\n",
      "{'n_estimators': 500, 'max_features': 'log2'}\n",
      "0.693550195727\n",
      "{'n_estimators': 500, 'max_features': 'sqrt'}\n",
      "0.683524302724\n",
      "{'n_estimators': 500, 'max_features': 6}\n",
      "0.694727613766\n",
      "Wall time: 5h 10min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for params in grid:\n",
    "    print(params)\n",
    "    rf = RandomForestClassifier(**params, n_jobs=-1, oob_score = True)\n",
    "    rf.fit(features[\"train\"], train.funny)\n",
    "    score = rf.oob_score_\n",
    "    print(score)\n",
    "    results.append((params, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69965748537177108"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(features[\"test\"], test.funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_probs = rf.predict_proba(features[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45      ,  0.55      ],\n",
       "       [ 0.738     ,  0.262     ],\n",
       "       [ 0.50133333,  0.49866667],\n",
       "       ..., \n",
       "       [ 0.464     ,  0.536     ],\n",
       "       [ 0.65983333,  0.34016667],\n",
       "       [ 0.529     ,  0.471     ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75443582651108088"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test.funny.astype('int'), test_probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr[0], tpr[0], _ = roc_curve(test.funny.values, test_probs[:, 1])\n",
    "roc_auc[0] = auc(fpr[0], tpr[0])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test.funny.values.ravel(), test_probs[:, 1])\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([  0.00000000e+00,   9.03342367e-04,   9.72830241e-04, ...,\n",
       "          9.99861024e-01,   9.99861024e-01,   1.00000000e+00]),\n",
       " 'micro': array([  0.00000000e+00,   9.03342367e-04,   9.72830241e-04, ...,\n",
       "          9.99861024e-01,   9.99861024e-01,   1.00000000e+00])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2686cebdc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test error and feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopper():\n",
    "    def __init__(self, validation_set, validation_labels, patience = 0, verbose = 0):\n",
    "        self.validation_set = validation_set\n",
    "        self.validation_labels = validation_labels\n",
    "        self.accuracy = []\n",
    "        self.loss = []\n",
    "        self.preds = None\n",
    "        self.probs = None\n",
    "        self.patience = patience\n",
    "        self.lowest_loss = np.inf\n",
    "        self.lowest_loss_iter = 0\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def __call__(self, i, estimator, fit_stage_locals):\n",
    "        \n",
    "        if i == 0:\n",
    "            self.preds = estimator.staged_predict(self.validation_set)\n",
    "            self.probs = estimator.staged_predict_proba(self.validation_set)\n",
    "            \n",
    "        preds = next(self.preds)\n",
    "        probs = next(self.probs)\n",
    "        \n",
    "        acc = np.mean(preds == self.validation_labels )\n",
    "        loss = log_loss(self.validation_labels, probs)\n",
    "        \n",
    "        self.accuracy.append(acc)\n",
    "        self.loss.append(loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        log = [\"Iteration: {}\".format(i), \"Loss: {0:.5f}\".format(loss), \"Accuracy: {0:.3f}\".format(acc)]\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"  \".join(log))\n",
    "        \n",
    "        if  loss < self.lowest_loss:\n",
    "            self.lowest_loss = loss\n",
    "            self.lowest_loss_iter = i\n",
    "            new_low = True\n",
    "        \n",
    "        if (min(self.loss[max((i-self.patience),0):i+1]) > self.lowest_loss) & (i > self.patience):\n",
    "            return True\n",
    "        else:\n",
    "         \n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  Loss: 0.68711  Accuracy: 0.590\n",
      "Iteration: 1  Loss: 0.68238  Accuracy: 0.592\n",
      "Iteration: 2  Loss: 0.67846  Accuracy: 0.597\n",
      "Iteration: 3  Loss: 0.67520  Accuracy: 0.598\n",
      "Iteration: 4  Loss: 0.67256  Accuracy: 0.599\n",
      "Iteration: 5  Loss: 0.67014  Accuracy: 0.601\n",
      "Iteration: 6  Loss: 0.66798  Accuracy: 0.601\n",
      "Iteration: 7  Loss: 0.66638  Accuracy: 0.602\n",
      "Iteration: 8  Loss: 0.66479  Accuracy: 0.603\n",
      "Iteration: 9  Loss: 0.66349  Accuracy: 0.604\n",
      "Iteration: 10  Loss: 0.66232  Accuracy: 0.605\n",
      "Iteration: 11  Loss: 0.66129  Accuracy: 0.606\n",
      "Iteration: 12  Loss: 0.66041  Accuracy: 0.605\n",
      "Iteration: 13  Loss: 0.65958  Accuracy: 0.606\n",
      "Iteration: 14  Loss: 0.65889  Accuracy: 0.606\n",
      "Iteration: 15  Loss: 0.65821  Accuracy: 0.606\n",
      "Iteration: 16  Loss: 0.65754  Accuracy: 0.606\n",
      "Iteration: 17  Loss: 0.65692  Accuracy: 0.607\n",
      "Iteration: 18  Loss: 0.65645  Accuracy: 0.608\n",
      "Iteration: 19  Loss: 0.65600  Accuracy: 0.608\n",
      "Iteration: 20  Loss: 0.65557  Accuracy: 0.608\n",
      "Iteration: 21  Loss: 0.65518  Accuracy: 0.608\n",
      "Iteration: 22  Loss: 0.65473  Accuracy: 0.609\n",
      "Iteration: 23  Loss: 0.65434  Accuracy: 0.608\n",
      "Iteration: 24  Loss: 0.65397  Accuracy: 0.609\n",
      "Iteration: 25  Loss: 0.65366  Accuracy: 0.610\n",
      "Iteration: 26  Loss: 0.65335  Accuracy: 0.610\n",
      "Iteration: 27  Loss: 0.65303  Accuracy: 0.610\n",
      "Iteration: 28  Loss: 0.65270  Accuracy: 0.610\n",
      "Iteration: 29  Loss: 0.65246  Accuracy: 0.611\n",
      "Iteration: 30  Loss: 0.65226  Accuracy: 0.610\n",
      "Iteration: 31  Loss: 0.65199  Accuracy: 0.610\n",
      "Iteration: 32  Loss: 0.65173  Accuracy: 0.611\n",
      "Iteration: 33  Loss: 0.65155  Accuracy: 0.612\n",
      "Iteration: 34  Loss: 0.65133  Accuracy: 0.611\n",
      "Iteration: 35  Loss: 0.65102  Accuracy: 0.612\n",
      "Iteration: 36  Loss: 0.65078  Accuracy: 0.613\n",
      "Iteration: 37  Loss: 0.65052  Accuracy: 0.613\n",
      "Iteration: 38  Loss: 0.65031  Accuracy: 0.613\n",
      "Iteration: 39  Loss: 0.65015  Accuracy: 0.613\n",
      "Iteration: 40  Loss: 0.65002  Accuracy: 0.614\n",
      "Iteration: 41  Loss: 0.64991  Accuracy: 0.614\n",
      "Iteration: 42  Loss: 0.64973  Accuracy: 0.614\n",
      "Iteration: 43  Loss: 0.64958  Accuracy: 0.615\n",
      "Iteration: 44  Loss: 0.64950  Accuracy: 0.614\n",
      "Iteration: 45  Loss: 0.64933  Accuracy: 0.615\n",
      "Iteration: 46  Loss: 0.64915  Accuracy: 0.616\n",
      "Iteration: 47  Loss: 0.64903  Accuracy: 0.616\n",
      "Iteration: 48  Loss: 0.64889  Accuracy: 0.616\n",
      "Iteration: 49  Loss: 0.64874  Accuracy: 0.616\n",
      "Iteration: 50  Loss: 0.64857  Accuracy: 0.616\n",
      "Iteration: 51  Loss: 0.64843  Accuracy: 0.617\n",
      "Iteration: 52  Loss: 0.64835  Accuracy: 0.617\n",
      "Iteration: 53  Loss: 0.64823  Accuracy: 0.617\n",
      "Iteration: 54  Loss: 0.64813  Accuracy: 0.617\n",
      "Iteration: 55  Loss: 0.64798  Accuracy: 0.617\n",
      "Iteration: 56  Loss: 0.64788  Accuracy: 0.617\n",
      "Iteration: 57  Loss: 0.64782  Accuracy: 0.617\n",
      "Iteration: 58  Loss: 0.64771  Accuracy: 0.617\n",
      "Iteration: 59  Loss: 0.64760  Accuracy: 0.618\n",
      "Iteration: 60  Loss: 0.64741  Accuracy: 0.618\n",
      "Iteration: 61  Loss: 0.64726  Accuracy: 0.618\n",
      "Iteration: 62  Loss: 0.64713  Accuracy: 0.619\n",
      "Iteration: 63  Loss: 0.64705  Accuracy: 0.619\n",
      "Iteration: 64  Loss: 0.64696  Accuracy: 0.619\n",
      "Iteration: 65  Loss: 0.64692  Accuracy: 0.619\n",
      "Iteration: 66  Loss: 0.64683  Accuracy: 0.619\n",
      "Iteration: 67  Loss: 0.64673  Accuracy: 0.619\n",
      "Iteration: 68  Loss: 0.64657  Accuracy: 0.619\n",
      "Iteration: 69  Loss: 0.64647  Accuracy: 0.619\n",
      "Iteration: 70  Loss: 0.64642  Accuracy: 0.619\n",
      "Iteration: 71  Loss: 0.64636  Accuracy: 0.619\n",
      "Iteration: 72  Loss: 0.64629  Accuracy: 0.619\n",
      "Iteration: 73  Loss: 0.64616  Accuracy: 0.619\n",
      "Iteration: 74  Loss: 0.64603  Accuracy: 0.620\n",
      "Iteration: 75  Loss: 0.64597  Accuracy: 0.620\n",
      "Iteration: 76  Loss: 0.64588  Accuracy: 0.620\n",
      "Iteration: 77  Loss: 0.64581  Accuracy: 0.620\n",
      "Iteration: 78  Loss: 0.64575  Accuracy: 0.620\n",
      "Iteration: 79  Loss: 0.64563  Accuracy: 0.620\n",
      "Iteration: 80  Loss: 0.64560  Accuracy: 0.620\n",
      "Iteration: 81  Loss: 0.64556  Accuracy: 0.621\n",
      "Iteration: 82  Loss: 0.64545  Accuracy: 0.620\n",
      "Iteration: 83  Loss: 0.64535  Accuracy: 0.621\n",
      "Iteration: 84  Loss: 0.64530  Accuracy: 0.621\n",
      "Iteration: 85  Loss: 0.64523  Accuracy: 0.621\n",
      "Iteration: 86  Loss: 0.64514  Accuracy: 0.621\n",
      "Iteration: 87  Loss: 0.64508  Accuracy: 0.621\n",
      "Iteration: 88  Loss: 0.64502  Accuracy: 0.621\n",
      "Iteration: 89  Loss: 0.64497  Accuracy: 0.621\n",
      "Iteration: 90  Loss: 0.64489  Accuracy: 0.621\n",
      "Iteration: 91  Loss: 0.64477  Accuracy: 0.621\n",
      "Iteration: 92  Loss: 0.64465  Accuracy: 0.621\n",
      "Iteration: 93  Loss: 0.64458  Accuracy: 0.621\n",
      "Iteration: 94  Loss: 0.64448  Accuracy: 0.621\n",
      "Iteration: 95  Loss: 0.64438  Accuracy: 0.622\n",
      "Iteration: 96  Loss: 0.64425  Accuracy: 0.623\n",
      "Iteration: 97  Loss: 0.64419  Accuracy: 0.623\n",
      "Iteration: 98  Loss: 0.64410  Accuracy: 0.623\n",
      "Iteration: 99  Loss: 0.64407  Accuracy: 0.622\n",
      "Iteration: 100  Loss: 0.64399  Accuracy: 0.622\n",
      "Iteration: 101  Loss: 0.64393  Accuracy: 0.622\n",
      "Iteration: 102  Loss: 0.64384  Accuracy: 0.622\n",
      "Iteration: 103  Loss: 0.64376  Accuracy: 0.622\n",
      "Iteration: 104  Loss: 0.64369  Accuracy: 0.623\n",
      "Iteration: 105  Loss: 0.64360  Accuracy: 0.623\n",
      "Iteration: 106  Loss: 0.64354  Accuracy: 0.623\n",
      "Iteration: 107  Loss: 0.64349  Accuracy: 0.623\n",
      "Iteration: 108  Loss: 0.64342  Accuracy: 0.623\n",
      "Iteration: 109  Loss: 0.64337  Accuracy: 0.623\n",
      "Iteration: 110  Loss: 0.64333  Accuracy: 0.624\n",
      "Iteration: 111  Loss: 0.64324  Accuracy: 0.624\n",
      "Iteration: 112  Loss: 0.64316  Accuracy: 0.624\n",
      "Iteration: 113  Loss: 0.64308  Accuracy: 0.624\n",
      "Iteration: 114  Loss: 0.64304  Accuracy: 0.624\n",
      "Iteration: 115  Loss: 0.64303  Accuracy: 0.624\n",
      "Iteration: 116  Loss: 0.64297  Accuracy: 0.624\n",
      "Iteration: 117  Loss: 0.64289  Accuracy: 0.624\n",
      "Iteration: 118  Loss: 0.64281  Accuracy: 0.625\n",
      "Iteration: 119  Loss: 0.64266  Accuracy: 0.625\n",
      "Iteration: 120  Loss: 0.64262  Accuracy: 0.625\n",
      "Iteration: 121  Loss: 0.64257  Accuracy: 0.625\n",
      "Iteration: 122  Loss: 0.64255  Accuracy: 0.625\n",
      "Iteration: 123  Loss: 0.64249  Accuracy: 0.625\n",
      "Iteration: 124  Loss: 0.64241  Accuracy: 0.625\n",
      "Iteration: 125  Loss: 0.64235  Accuracy: 0.625\n",
      "Iteration: 126  Loss: 0.64231  Accuracy: 0.625\n",
      "Iteration: 127  Loss: 0.64226  Accuracy: 0.625\n",
      "Iteration: 128  Loss: 0.64214  Accuracy: 0.625\n",
      "Iteration: 129  Loss: 0.64207  Accuracy: 0.625\n",
      "Iteration: 130  Loss: 0.64201  Accuracy: 0.625\n",
      "Iteration: 131  Loss: 0.64194  Accuracy: 0.625\n",
      "Iteration: 132  Loss: 0.64190  Accuracy: 0.625\n",
      "Iteration: 133  Loss: 0.64190  Accuracy: 0.626\n",
      "Iteration: 134  Loss: 0.64184  Accuracy: 0.626\n",
      "Iteration: 135  Loss: 0.64182  Accuracy: 0.626\n",
      "Iteration: 136  Loss: 0.64179  Accuracy: 0.626\n",
      "Iteration: 137  Loss: 0.64174  Accuracy: 0.626\n",
      "Iteration: 138  Loss: 0.64174  Accuracy: 0.626\n",
      "Iteration: 139  Loss: 0.64169  Accuracy: 0.626\n",
      "Iteration: 140  Loss: 0.64164  Accuracy: 0.626\n",
      "Iteration: 141  Loss: 0.64159  Accuracy: 0.626\n",
      "Iteration: 142  Loss: 0.64150  Accuracy: 0.627\n",
      "Iteration: 143  Loss: 0.64145  Accuracy: 0.627\n",
      "Iteration: 144  Loss: 0.64135  Accuracy: 0.627\n",
      "Iteration: 145  Loss: 0.64129  Accuracy: 0.627\n",
      "Iteration: 146  Loss: 0.64126  Accuracy: 0.627\n",
      "Iteration: 147  Loss: 0.64122  Accuracy: 0.627\n",
      "Iteration: 148  Loss: 0.64117  Accuracy: 0.627\n",
      "Iteration: 149  Loss: 0.64113  Accuracy: 0.627\n",
      "Iteration: 150  Loss: 0.64110  Accuracy: 0.627\n",
      "Iteration: 151  Loss: 0.64104  Accuracy: 0.628\n",
      "Iteration: 152  Loss: 0.64098  Accuracy: 0.628\n",
      "Iteration: 153  Loss: 0.64093  Accuracy: 0.628\n",
      "Iteration: 154  Loss: 0.64087  Accuracy: 0.628\n",
      "Iteration: 155  Loss: 0.64080  Accuracy: 0.628\n",
      "Iteration: 156  Loss: 0.64078  Accuracy: 0.628\n",
      "Iteration: 157  Loss: 0.64073  Accuracy: 0.629\n",
      "Iteration: 158  Loss: 0.64069  Accuracy: 0.629\n",
      "Iteration: 159  Loss: 0.64063  Accuracy: 0.629\n",
      "Iteration: 160  Loss: 0.64062  Accuracy: 0.629\n",
      "Iteration: 161  Loss: 0.64057  Accuracy: 0.629\n",
      "Iteration: 162  Loss: 0.64050  Accuracy: 0.629\n",
      "Iteration: 163  Loss: 0.64052  Accuracy: 0.629\n",
      "Iteration: 164  Loss: 0.64049  Accuracy: 0.629\n",
      "Iteration: 165  Loss: 0.64042  Accuracy: 0.629\n",
      "Iteration: 166  Loss: 0.64037  Accuracy: 0.629\n",
      "Iteration: 167  Loss: 0.64031  Accuracy: 0.629\n",
      "Iteration: 168  Loss: 0.64025  Accuracy: 0.629\n",
      "Iteration: 169  Loss: 0.64021  Accuracy: 0.629\n",
      "Iteration: 170  Loss: 0.64016  Accuracy: 0.629\n",
      "Iteration: 171  Loss: 0.64014  Accuracy: 0.629\n",
      "Iteration: 172  Loss: 0.64009  Accuracy: 0.629\n",
      "Iteration: 173  Loss: 0.64005  Accuracy: 0.629\n",
      "Iteration: 174  Loss: 0.64003  Accuracy: 0.629\n",
      "Iteration: 175  Loss: 0.64000  Accuracy: 0.629\n",
      "Iteration: 176  Loss: 0.63998  Accuracy: 0.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 177  Loss: 0.63996  Accuracy: 0.629\n",
      "Iteration: 178  Loss: 0.63988  Accuracy: 0.629\n",
      "Iteration: 179  Loss: 0.63984  Accuracy: 0.630\n",
      "Iteration: 180  Loss: 0.63978  Accuracy: 0.630\n",
      "Iteration: 181  Loss: 0.63971  Accuracy: 0.630\n",
      "Iteration: 182  Loss: 0.63968  Accuracy: 0.630\n",
      "Iteration: 183  Loss: 0.63961  Accuracy: 0.630\n",
      "Iteration: 184  Loss: 0.63958  Accuracy: 0.630\n",
      "Iteration: 185  Loss: 0.63957  Accuracy: 0.630\n",
      "Iteration: 186  Loss: 0.63952  Accuracy: 0.630\n",
      "Iteration: 187  Loss: 0.63948  Accuracy: 0.630\n",
      "Iteration: 188  Loss: 0.63945  Accuracy: 0.630\n",
      "Iteration: 189  Loss: 0.63941  Accuracy: 0.631\n",
      "Iteration: 190  Loss: 0.63939  Accuracy: 0.631\n",
      "Iteration: 191  Loss: 0.63931  Accuracy: 0.631\n",
      "Iteration: 192  Loss: 0.63926  Accuracy: 0.631\n",
      "Iteration: 193  Loss: 0.63923  Accuracy: 0.631\n",
      "Iteration: 194  Loss: 0.63919  Accuracy: 0.631\n",
      "Iteration: 195  Loss: 0.63915  Accuracy: 0.631\n",
      "Iteration: 196  Loss: 0.63910  Accuracy: 0.631\n",
      "Iteration: 197  Loss: 0.63902  Accuracy: 0.631\n",
      "Iteration: 198  Loss: 0.63901  Accuracy: 0.632\n",
      "Iteration: 199  Loss: 0.63899  Accuracy: 0.632\n",
      "Iteration: 200  Loss: 0.63892  Accuracy: 0.632\n",
      "Iteration: 201  Loss: 0.63885  Accuracy: 0.632\n",
      "Iteration: 202  Loss: 0.63882  Accuracy: 0.632\n",
      "Iteration: 203  Loss: 0.63880  Accuracy: 0.632\n",
      "Iteration: 204  Loss: 0.63873  Accuracy: 0.632\n",
      "Iteration: 205  Loss: 0.63868  Accuracy: 0.632\n",
      "Iteration: 206  Loss: 0.63867  Accuracy: 0.632\n",
      "Iteration: 207  Loss: 0.63861  Accuracy: 0.632\n",
      "Iteration: 208  Loss: 0.63859  Accuracy: 0.632\n",
      "Iteration: 209  Loss: 0.63854  Accuracy: 0.632\n",
      "Iteration: 210  Loss: 0.63848  Accuracy: 0.632\n",
      "Iteration: 211  Loss: 0.63841  Accuracy: 0.632\n",
      "Iteration: 212  Loss: 0.63838  Accuracy: 0.632\n",
      "Iteration: 213  Loss: 0.63837  Accuracy: 0.632\n",
      "Iteration: 214  Loss: 0.63832  Accuracy: 0.632\n",
      "Iteration: 215  Loss: 0.63830  Accuracy: 0.632\n",
      "Iteration: 216  Loss: 0.63824  Accuracy: 0.633\n",
      "Iteration: 217  Loss: 0.63823  Accuracy: 0.632\n",
      "Iteration: 218  Loss: 0.63821  Accuracy: 0.633\n",
      "Iteration: 219  Loss: 0.63817  Accuracy: 0.633\n",
      "Iteration: 220  Loss: 0.63813  Accuracy: 0.632\n",
      "Iteration: 221  Loss: 0.63810  Accuracy: 0.633\n",
      "Iteration: 222  Loss: 0.63808  Accuracy: 0.632\n",
      "Iteration: 223  Loss: 0.63806  Accuracy: 0.632\n",
      "Iteration: 224  Loss: 0.63800  Accuracy: 0.632\n",
      "Iteration: 225  Loss: 0.63798  Accuracy: 0.633\n",
      "Iteration: 226  Loss: 0.63792  Accuracy: 0.633\n",
      "Iteration: 227  Loss: 0.63787  Accuracy: 0.633\n",
      "Iteration: 228  Loss: 0.63785  Accuracy: 0.633\n",
      "Iteration: 229  Loss: 0.63779  Accuracy: 0.633\n",
      "Iteration: 230  Loss: 0.63778  Accuracy: 0.633\n",
      "Iteration: 231  Loss: 0.63773  Accuracy: 0.633\n",
      "Iteration: 232  Loss: 0.63771  Accuracy: 0.633\n",
      "Iteration: 233  Loss: 0.63767  Accuracy: 0.633\n",
      "Iteration: 234  Loss: 0.63763  Accuracy: 0.633\n",
      "Iteration: 235  Loss: 0.63761  Accuracy: 0.633\n",
      "Iteration: 236  Loss: 0.63755  Accuracy: 0.633\n",
      "Iteration: 237  Loss: 0.63751  Accuracy: 0.634\n",
      "Iteration: 238  Loss: 0.63748  Accuracy: 0.634\n",
      "Iteration: 239  Loss: 0.63745  Accuracy: 0.634\n",
      "Iteration: 240  Loss: 0.63740  Accuracy: 0.634\n",
      "Iteration: 241  Loss: 0.63737  Accuracy: 0.634\n",
      "Iteration: 242  Loss: 0.63737  Accuracy: 0.634\n",
      "Iteration: 243  Loss: 0.63734  Accuracy: 0.634\n",
      "Iteration: 244  Loss: 0.63731  Accuracy: 0.634\n",
      "Iteration: 245  Loss: 0.63725  Accuracy: 0.634\n",
      "Iteration: 246  Loss: 0.63721  Accuracy: 0.635\n",
      "Iteration: 247  Loss: 0.63716  Accuracy: 0.635\n",
      "Iteration: 248  Loss: 0.63712  Accuracy: 0.635\n",
      "Iteration: 249  Loss: 0.63711  Accuracy: 0.635\n",
      "Iteration: 250  Loss: 0.63710  Accuracy: 0.635\n",
      "Iteration: 251  Loss: 0.63711  Accuracy: 0.635\n",
      "Iteration: 252  Loss: 0.63706  Accuracy: 0.636\n",
      "Iteration: 253  Loss: 0.63701  Accuracy: 0.636\n",
      "Iteration: 254  Loss: 0.63697  Accuracy: 0.636\n",
      "Iteration: 255  Loss: 0.63695  Accuracy: 0.636\n",
      "Iteration: 256  Loss: 0.63692  Accuracy: 0.635\n",
      "Iteration: 257  Loss: 0.63692  Accuracy: 0.635\n",
      "Iteration: 258  Loss: 0.63691  Accuracy: 0.635\n",
      "Iteration: 259  Loss: 0.63686  Accuracy: 0.635\n",
      "Iteration: 260  Loss: 0.63684  Accuracy: 0.635\n",
      "Iteration: 261  Loss: 0.63680  Accuracy: 0.635\n",
      "Iteration: 262  Loss: 0.63677  Accuracy: 0.635\n",
      "Iteration: 263  Loss: 0.63674  Accuracy: 0.636\n",
      "Iteration: 264  Loss: 0.63672  Accuracy: 0.636\n",
      "Iteration: 265  Loss: 0.63668  Accuracy: 0.635\n",
      "Iteration: 266  Loss: 0.63665  Accuracy: 0.636\n",
      "Iteration: 267  Loss: 0.63660  Accuracy: 0.636\n",
      "Iteration: 268  Loss: 0.63658  Accuracy: 0.636\n",
      "Iteration: 269  Loss: 0.63656  Accuracy: 0.636\n",
      "Iteration: 270  Loss: 0.63654  Accuracy: 0.636\n",
      "Iteration: 271  Loss: 0.63648  Accuracy: 0.636\n",
      "Iteration: 272  Loss: 0.63646  Accuracy: 0.636\n",
      "Iteration: 273  Loss: 0.63641  Accuracy: 0.636\n",
      "Iteration: 274  Loss: 0.63637  Accuracy: 0.636\n",
      "Iteration: 275  Loss: 0.63634  Accuracy: 0.636\n",
      "Iteration: 276  Loss: 0.63632  Accuracy: 0.636\n",
      "Iteration: 277  Loss: 0.63629  Accuracy: 0.636\n",
      "Iteration: 278  Loss: 0.63627  Accuracy: 0.636\n",
      "Iteration: 279  Loss: 0.63620  Accuracy: 0.636\n",
      "Iteration: 280  Loss: 0.63620  Accuracy: 0.636\n",
      "Iteration: 281  Loss: 0.63618  Accuracy: 0.636\n",
      "Iteration: 282  Loss: 0.63615  Accuracy: 0.636\n",
      "Iteration: 283  Loss: 0.63615  Accuracy: 0.636\n",
      "Iteration: 284  Loss: 0.63614  Accuracy: 0.636\n",
      "Iteration: 285  Loss: 0.63612  Accuracy: 0.636\n",
      "Iteration: 286  Loss: 0.63613  Accuracy: 0.636\n",
      "Iteration: 287  Loss: 0.63611  Accuracy: 0.636\n",
      "Iteration: 288  Loss: 0.63610  Accuracy: 0.636\n",
      "Iteration: 289  Loss: 0.63608  Accuracy: 0.636\n",
      "Iteration: 290  Loss: 0.63607  Accuracy: 0.636\n",
      "Iteration: 291  Loss: 0.63604  Accuracy: 0.636\n",
      "Iteration: 292  Loss: 0.63602  Accuracy: 0.636\n",
      "Iteration: 293  Loss: 0.63600  Accuracy: 0.636\n",
      "Iteration: 294  Loss: 0.63597  Accuracy: 0.636\n",
      "Iteration: 295  Loss: 0.63592  Accuracy: 0.636\n",
      "Iteration: 296  Loss: 0.63591  Accuracy: 0.636\n",
      "Iteration: 297  Loss: 0.63588  Accuracy: 0.636\n",
      "Iteration: 298  Loss: 0.63585  Accuracy: 0.636\n",
      "Iteration: 299  Loss: 0.63579  Accuracy: 0.636\n",
      "Iteration: 300  Loss: 0.63574  Accuracy: 0.636\n",
      "Iteration: 301  Loss: 0.63571  Accuracy: 0.636\n",
      "Iteration: 302  Loss: 0.63566  Accuracy: 0.636\n",
      "Iteration: 303  Loss: 0.63561  Accuracy: 0.637\n",
      "Iteration: 304  Loss: 0.63555  Accuracy: 0.637\n",
      "Iteration: 305  Loss: 0.63554  Accuracy: 0.637\n",
      "Iteration: 306  Loss: 0.63552  Accuracy: 0.637\n",
      "Iteration: 307  Loss: 0.63548  Accuracy: 0.637\n",
      "Iteration: 308  Loss: 0.63544  Accuracy: 0.637\n",
      "Iteration: 309  Loss: 0.63542  Accuracy: 0.637\n",
      "Iteration: 310  Loss: 0.63539  Accuracy: 0.637\n",
      "Iteration: 311  Loss: 0.63532  Accuracy: 0.637\n",
      "Iteration: 312  Loss: 0.63529  Accuracy: 0.637\n",
      "Iteration: 313  Loss: 0.63527  Accuracy: 0.637\n",
      "Iteration: 314  Loss: 0.63522  Accuracy: 0.638\n",
      "Iteration: 315  Loss: 0.63518  Accuracy: 0.638\n",
      "Iteration: 316  Loss: 0.63515  Accuracy: 0.638\n",
      "Iteration: 317  Loss: 0.63511  Accuracy: 0.638\n",
      "Iteration: 318  Loss: 0.63509  Accuracy: 0.638\n",
      "Iteration: 319  Loss: 0.63505  Accuracy: 0.638\n",
      "Iteration: 320  Loss: 0.63500  Accuracy: 0.638\n",
      "Iteration: 321  Loss: 0.63499  Accuracy: 0.638\n",
      "Iteration: 322  Loss: 0.63497  Accuracy: 0.638\n",
      "Iteration: 323  Loss: 0.63497  Accuracy: 0.638\n",
      "Iteration: 324  Loss: 0.63491  Accuracy: 0.639\n",
      "Iteration: 325  Loss: 0.63485  Accuracy: 0.639\n",
      "Iteration: 326  Loss: 0.63481  Accuracy: 0.639\n",
      "Iteration: 327  Loss: 0.63476  Accuracy: 0.639\n",
      "Iteration: 328  Loss: 0.63475  Accuracy: 0.639\n",
      "Iteration: 329  Loss: 0.63466  Accuracy: 0.639\n",
      "Iteration: 330  Loss: 0.63463  Accuracy: 0.638\n",
      "Iteration: 331  Loss: 0.63462  Accuracy: 0.638\n",
      "Iteration: 332  Loss: 0.63461  Accuracy: 0.639\n",
      "Iteration: 333  Loss: 0.63460  Accuracy: 0.639\n",
      "Iteration: 334  Loss: 0.63458  Accuracy: 0.639\n",
      "Iteration: 335  Loss: 0.63454  Accuracy: 0.639\n",
      "Iteration: 336  Loss: 0.63451  Accuracy: 0.639\n",
      "Iteration: 337  Loss: 0.63450  Accuracy: 0.639\n",
      "Iteration: 338  Loss: 0.63446  Accuracy: 0.639\n",
      "Iteration: 339  Loss: 0.63445  Accuracy: 0.639\n",
      "Iteration: 340  Loss: 0.63442  Accuracy: 0.638\n",
      "Iteration: 341  Loss: 0.63441  Accuracy: 0.638\n",
      "Iteration: 342  Loss: 0.63438  Accuracy: 0.638\n",
      "Iteration: 343  Loss: 0.63433  Accuracy: 0.638\n",
      "Iteration: 344  Loss: 0.63431  Accuracy: 0.639\n",
      "Iteration: 345  Loss: 0.63425  Accuracy: 0.639\n",
      "Iteration: 346  Loss: 0.63425  Accuracy: 0.639\n",
      "Iteration: 347  Loss: 0.63422  Accuracy: 0.639\n",
      "Iteration: 348  Loss: 0.63419  Accuracy: 0.639\n",
      "Iteration: 349  Loss: 0.63417  Accuracy: 0.639\n",
      "Iteration: 350  Loss: 0.63416  Accuracy: 0.639\n",
      "Iteration: 351  Loss: 0.63416  Accuracy: 0.639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 352  Loss: 0.63414  Accuracy: 0.639\n",
      "Iteration: 353  Loss: 0.63414  Accuracy: 0.639\n",
      "Iteration: 354  Loss: 0.63410  Accuracy: 0.639\n",
      "Iteration: 355  Loss: 0.63409  Accuracy: 0.639\n",
      "Iteration: 356  Loss: 0.63407  Accuracy: 0.639\n",
      "Iteration: 357  Loss: 0.63406  Accuracy: 0.639\n",
      "Iteration: 358  Loss: 0.63404  Accuracy: 0.639\n",
      "Iteration: 359  Loss: 0.63400  Accuracy: 0.639\n",
      "Iteration: 360  Loss: 0.63397  Accuracy: 0.639\n",
      "Iteration: 361  Loss: 0.63393  Accuracy: 0.639\n",
      "Iteration: 362  Loss: 0.63391  Accuracy: 0.639\n",
      "Iteration: 363  Loss: 0.63388  Accuracy: 0.640\n",
      "Iteration: 364  Loss: 0.63383  Accuracy: 0.640\n",
      "Iteration: 365  Loss: 0.63379  Accuracy: 0.640\n",
      "Iteration: 366  Loss: 0.63378  Accuracy: 0.640\n",
      "Iteration: 367  Loss: 0.63376  Accuracy: 0.640\n",
      "Iteration: 368  Loss: 0.63375  Accuracy: 0.640\n",
      "Iteration: 369  Loss: 0.63370  Accuracy: 0.640\n",
      "Iteration: 370  Loss: 0.63367  Accuracy: 0.640\n",
      "Iteration: 371  Loss: 0.63364  Accuracy: 0.640\n",
      "Iteration: 372  Loss: 0.63362  Accuracy: 0.640\n",
      "Iteration: 373  Loss: 0.63358  Accuracy: 0.640\n",
      "Iteration: 374  Loss: 0.63356  Accuracy: 0.640\n",
      "Iteration: 375  Loss: 0.63356  Accuracy: 0.640\n",
      "Iteration: 376  Loss: 0.63354  Accuracy: 0.640\n",
      "Iteration: 377  Loss: 0.63350  Accuracy: 0.640\n",
      "Iteration: 378  Loss: 0.63349  Accuracy: 0.640\n",
      "Iteration: 379  Loss: 0.63348  Accuracy: 0.640\n",
      "Iteration: 380  Loss: 0.63344  Accuracy: 0.640\n",
      "Iteration: 381  Loss: 0.63339  Accuracy: 0.641\n",
      "Iteration: 382  Loss: 0.63336  Accuracy: 0.641\n",
      "Iteration: 383  Loss: 0.63335  Accuracy: 0.641\n",
      "Iteration: 384  Loss: 0.63331  Accuracy: 0.641\n",
      "Iteration: 385  Loss: 0.63328  Accuracy: 0.641\n",
      "Iteration: 386  Loss: 0.63328  Accuracy: 0.641\n",
      "Iteration: 387  Loss: 0.63326  Accuracy: 0.641\n",
      "Iteration: 388  Loss: 0.63323  Accuracy: 0.641\n",
      "Iteration: 389  Loss: 0.63320  Accuracy: 0.641\n",
      "Iteration: 390  Loss: 0.63316  Accuracy: 0.641\n",
      "Iteration: 391  Loss: 0.63313  Accuracy: 0.641\n",
      "Iteration: 392  Loss: 0.63311  Accuracy: 0.641\n",
      "Iteration: 393  Loss: 0.63308  Accuracy: 0.641\n",
      "Iteration: 394  Loss: 0.63307  Accuracy: 0.641\n",
      "Iteration: 395  Loss: 0.63304  Accuracy: 0.641\n",
      "Iteration: 396  Loss: 0.63297  Accuracy: 0.641\n",
      "Iteration: 397  Loss: 0.63298  Accuracy: 0.641\n",
      "Iteration: 398  Loss: 0.63294  Accuracy: 0.641\n",
      "Iteration: 399  Loss: 0.63290  Accuracy: 0.641\n",
      "Iteration: 400  Loss: 0.63288  Accuracy: 0.641\n",
      "Iteration: 401  Loss: 0.63286  Accuracy: 0.641\n",
      "Iteration: 402  Loss: 0.63285  Accuracy: 0.641\n",
      "Iteration: 403  Loss: 0.63282  Accuracy: 0.641\n",
      "Iteration: 404  Loss: 0.63280  Accuracy: 0.641\n",
      "Iteration: 405  Loss: 0.63278  Accuracy: 0.641\n",
      "Iteration: 406  Loss: 0.63278  Accuracy: 0.640\n",
      "Iteration: 407  Loss: 0.63274  Accuracy: 0.641\n",
      "Iteration: 408  Loss: 0.63274  Accuracy: 0.641\n",
      "Iteration: 409  Loss: 0.63274  Accuracy: 0.641\n",
      "Iteration: 410  Loss: 0.63269  Accuracy: 0.641\n",
      "Iteration: 411  Loss: 0.63268  Accuracy: 0.641\n",
      "Iteration: 412  Loss: 0.63265  Accuracy: 0.641\n",
      "Iteration: 413  Loss: 0.63265  Accuracy: 0.641\n",
      "Iteration: 414  Loss: 0.63263  Accuracy: 0.641\n",
      "Iteration: 415  Loss: 0.63261  Accuracy: 0.641\n",
      "Iteration: 416  Loss: 0.63257  Accuracy: 0.641\n",
      "Iteration: 417  Loss: 0.63253  Accuracy: 0.641\n",
      "Iteration: 418  Loss: 0.63251  Accuracy: 0.641\n",
      "Iteration: 419  Loss: 0.63247  Accuracy: 0.641\n",
      "Iteration: 420  Loss: 0.63247  Accuracy: 0.641\n",
      "Iteration: 421  Loss: 0.63244  Accuracy: 0.641\n",
      "Iteration: 422  Loss: 0.63245  Accuracy: 0.641\n",
      "Iteration: 423  Loss: 0.63244  Accuracy: 0.641\n",
      "Iteration: 424  Loss: 0.63240  Accuracy: 0.641\n",
      "Iteration: 425  Loss: 0.63241  Accuracy: 0.641\n",
      "Iteration: 426  Loss: 0.63239  Accuracy: 0.641\n",
      "Iteration: 427  Loss: 0.63236  Accuracy: 0.641\n",
      "Iteration: 428  Loss: 0.63233  Accuracy: 0.641\n",
      "Iteration: 429  Loss: 0.63230  Accuracy: 0.641\n",
      "Iteration: 430  Loss: 0.63230  Accuracy: 0.641\n",
      "Iteration: 431  Loss: 0.63228  Accuracy: 0.641\n",
      "Iteration: 432  Loss: 0.63226  Accuracy: 0.641\n",
      "Iteration: 433  Loss: 0.63225  Accuracy: 0.641\n",
      "Iteration: 434  Loss: 0.63223  Accuracy: 0.641\n",
      "Iteration: 435  Loss: 0.63222  Accuracy: 0.642\n",
      "Iteration: 436  Loss: 0.63219  Accuracy: 0.641\n",
      "Iteration: 437  Loss: 0.63214  Accuracy: 0.642\n",
      "Iteration: 438  Loss: 0.63214  Accuracy: 0.641\n",
      "Iteration: 439  Loss: 0.63216  Accuracy: 0.641\n",
      "Iteration: 440  Loss: 0.63214  Accuracy: 0.641\n",
      "Iteration: 441  Loss: 0.63212  Accuracy: 0.641\n",
      "Iteration: 442  Loss: 0.63208  Accuracy: 0.642\n",
      "Iteration: 443  Loss: 0.63207  Accuracy: 0.642\n",
      "Iteration: 444  Loss: 0.63204  Accuracy: 0.642\n",
      "Iteration: 445  Loss: 0.63203  Accuracy: 0.642\n",
      "Iteration: 446  Loss: 0.63200  Accuracy: 0.642\n",
      "Iteration: 447  Loss: 0.63201  Accuracy: 0.642\n",
      "Iteration: 448  Loss: 0.63200  Accuracy: 0.642\n",
      "Iteration: 449  Loss: 0.63196  Accuracy: 0.642\n",
      "Iteration: 450  Loss: 0.63194  Accuracy: 0.642\n",
      "Iteration: 451  Loss: 0.63190  Accuracy: 0.642\n",
      "Iteration: 452  Loss: 0.63188  Accuracy: 0.642\n",
      "Iteration: 453  Loss: 0.63185  Accuracy: 0.642\n",
      "Iteration: 454  Loss: 0.63183  Accuracy: 0.642\n",
      "Iteration: 455  Loss: 0.63180  Accuracy: 0.642\n",
      "Iteration: 456  Loss: 0.63179  Accuracy: 0.642\n",
      "Iteration: 457  Loss: 0.63176  Accuracy: 0.642\n",
      "Iteration: 458  Loss: 0.63179  Accuracy: 0.642\n",
      "Iteration: 459  Loss: 0.63173  Accuracy: 0.642\n",
      "Iteration: 460  Loss: 0.63172  Accuracy: 0.642\n",
      "Iteration: 461  Loss: 0.63170  Accuracy: 0.642\n",
      "Iteration: 462  Loss: 0.63169  Accuracy: 0.642\n",
      "Iteration: 463  Loss: 0.63169  Accuracy: 0.643\n",
      "Iteration: 464  Loss: 0.63166  Accuracy: 0.643\n",
      "Iteration: 465  Loss: 0.63164  Accuracy: 0.643\n",
      "Iteration: 466  Loss: 0.63162  Accuracy: 0.643\n",
      "Iteration: 467  Loss: 0.63159  Accuracy: 0.643\n",
      "Iteration: 468  Loss: 0.63154  Accuracy: 0.643\n",
      "Iteration: 469  Loss: 0.63149  Accuracy: 0.643\n",
      "Iteration: 470  Loss: 0.63146  Accuracy: 0.643\n",
      "Iteration: 471  Loss: 0.63143  Accuracy: 0.643\n",
      "Iteration: 472  Loss: 0.63140  Accuracy: 0.643\n",
      "Iteration: 473  Loss: 0.63137  Accuracy: 0.643\n",
      "Iteration: 474  Loss: 0.63135  Accuracy: 0.643\n",
      "Iteration: 475  Loss: 0.63132  Accuracy: 0.643\n",
      "Iteration: 476  Loss: 0.63128  Accuracy: 0.643\n",
      "Iteration: 477  Loss: 0.63128  Accuracy: 0.643\n",
      "Iteration: 478  Loss: 0.63127  Accuracy: 0.643\n",
      "Iteration: 479  Loss: 0.63124  Accuracy: 0.643\n",
      "Iteration: 480  Loss: 0.63123  Accuracy: 0.643\n",
      "Iteration: 481  Loss: 0.63120  Accuracy: 0.643\n",
      "Iteration: 482  Loss: 0.63118  Accuracy: 0.643\n",
      "Iteration: 483  Loss: 0.63117  Accuracy: 0.643\n",
      "Iteration: 484  Loss: 0.63116  Accuracy: 0.643\n",
      "Iteration: 485  Loss: 0.63115  Accuracy: 0.643\n",
      "Iteration: 486  Loss: 0.63115  Accuracy: 0.643\n",
      "Iteration: 487  Loss: 0.63112  Accuracy: 0.643\n",
      "Iteration: 488  Loss: 0.63111  Accuracy: 0.643\n",
      "Iteration: 489  Loss: 0.63112  Accuracy: 0.643\n",
      "Iteration: 490  Loss: 0.63110  Accuracy: 0.643\n",
      "Iteration: 491  Loss: 0.63109  Accuracy: 0.643\n",
      "Iteration: 492  Loss: 0.63107  Accuracy: 0.643\n",
      "Iteration: 493  Loss: 0.63106  Accuracy: 0.643\n",
      "Iteration: 494  Loss: 0.63105  Accuracy: 0.643\n",
      "Iteration: 495  Loss: 0.63103  Accuracy: 0.643\n",
      "Iteration: 496  Loss: 0.63100  Accuracy: 0.644\n",
      "Iteration: 497  Loss: 0.63099  Accuracy: 0.644\n",
      "Iteration: 498  Loss: 0.63096  Accuracy: 0.644\n",
      "Iteration: 499  Loss: 0.63094  Accuracy: 0.644\n",
      "Iteration: 500  Loss: 0.63094  Accuracy: 0.644\n",
      "Iteration: 501  Loss: 0.63090  Accuracy: 0.644\n",
      "Iteration: 502  Loss: 0.63087  Accuracy: 0.644\n",
      "Iteration: 503  Loss: 0.63082  Accuracy: 0.644\n",
      "Iteration: 504  Loss: 0.63080  Accuracy: 0.644\n",
      "Iteration: 505  Loss: 0.63078  Accuracy: 0.644\n",
      "Iteration: 506  Loss: 0.63074  Accuracy: 0.644\n",
      "Iteration: 507  Loss: 0.63074  Accuracy: 0.644\n",
      "Iteration: 508  Loss: 0.63075  Accuracy: 0.644\n",
      "Iteration: 509  Loss: 0.63075  Accuracy: 0.644\n",
      "Iteration: 510  Loss: 0.63074  Accuracy: 0.644\n",
      "Iteration: 511  Loss: 0.63073  Accuracy: 0.644\n",
      "Iteration: 512  Loss: 0.63071  Accuracy: 0.644\n",
      "Iteration: 513  Loss: 0.63070  Accuracy: 0.644\n",
      "Iteration: 514  Loss: 0.63070  Accuracy: 0.643\n",
      "Iteration: 515  Loss: 0.63068  Accuracy: 0.644\n",
      "Iteration: 516  Loss: 0.63066  Accuracy: 0.644\n",
      "Iteration: 517  Loss: 0.63061  Accuracy: 0.644\n",
      "Iteration: 518  Loss: 0.63059  Accuracy: 0.644\n",
      "Iteration: 519  Loss: 0.63059  Accuracy: 0.644\n",
      "Iteration: 520  Loss: 0.63058  Accuracy: 0.644\n",
      "Iteration: 521  Loss: 0.63057  Accuracy: 0.644\n",
      "Iteration: 522  Loss: 0.63054  Accuracy: 0.644\n",
      "Iteration: 523  Loss: 0.63051  Accuracy: 0.644\n",
      "Iteration: 524  Loss: 0.63048  Accuracy: 0.644\n",
      "Iteration: 525  Loss: 0.63048  Accuracy: 0.644\n",
      "Iteration: 526  Loss: 0.63047  Accuracy: 0.644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 527  Loss: 0.63042  Accuracy: 0.644\n",
      "Iteration: 528  Loss: 0.63039  Accuracy: 0.644\n",
      "Iteration: 529  Loss: 0.63041  Accuracy: 0.644\n",
      "Iteration: 530  Loss: 0.63035  Accuracy: 0.644\n",
      "Iteration: 531  Loss: 0.63034  Accuracy: 0.644\n",
      "Iteration: 532  Loss: 0.63035  Accuracy: 0.644\n",
      "Iteration: 533  Loss: 0.63034  Accuracy: 0.644\n",
      "Iteration: 534  Loss: 0.63031  Accuracy: 0.644\n",
      "Iteration: 535  Loss: 0.63031  Accuracy: 0.644\n",
      "Iteration: 536  Loss: 0.63031  Accuracy: 0.644\n",
      "Iteration: 537  Loss: 0.63029  Accuracy: 0.644\n",
      "Iteration: 538  Loss: 0.63026  Accuracy: 0.644\n",
      "Iteration: 539  Loss: 0.63023  Accuracy: 0.644\n",
      "Iteration: 540  Loss: 0.63020  Accuracy: 0.644\n",
      "Iteration: 541  Loss: 0.63019  Accuracy: 0.645\n",
      "Iteration: 542  Loss: 0.63018  Accuracy: 0.645\n",
      "Iteration: 543  Loss: 0.63015  Accuracy: 0.644\n",
      "Iteration: 544  Loss: 0.63012  Accuracy: 0.644\n",
      "Iteration: 545  Loss: 0.63010  Accuracy: 0.644\n",
      "Iteration: 546  Loss: 0.63007  Accuracy: 0.645\n",
      "Iteration: 547  Loss: 0.63005  Accuracy: 0.645\n",
      "Iteration: 548  Loss: 0.63004  Accuracy: 0.645\n",
      "Iteration: 549  Loss: 0.63004  Accuracy: 0.644\n",
      "Iteration: 550  Loss: 0.63003  Accuracy: 0.644\n",
      "Iteration: 551  Loss: 0.62999  Accuracy: 0.644\n",
      "Iteration: 552  Loss: 0.62996  Accuracy: 0.644\n",
      "Iteration: 553  Loss: 0.62994  Accuracy: 0.644\n",
      "Iteration: 554  Loss: 0.62993  Accuracy: 0.644\n",
      "Iteration: 555  Loss: 0.62991  Accuracy: 0.645\n",
      "Iteration: 556  Loss: 0.62990  Accuracy: 0.645\n",
      "Iteration: 557  Loss: 0.62985  Accuracy: 0.645\n",
      "Iteration: 558  Loss: 0.62984  Accuracy: 0.645\n",
      "Iteration: 559  Loss: 0.62981  Accuracy: 0.645\n",
      "Iteration: 560  Loss: 0.62979  Accuracy: 0.645\n",
      "Iteration: 561  Loss: 0.62977  Accuracy: 0.645\n",
      "Iteration: 562  Loss: 0.62975  Accuracy: 0.645\n",
      "Iteration: 563  Loss: 0.62974  Accuracy: 0.645\n",
      "Iteration: 564  Loss: 0.62971  Accuracy: 0.645\n",
      "Iteration: 565  Loss: 0.62973  Accuracy: 0.645\n",
      "Iteration: 566  Loss: 0.62973  Accuracy: 0.645\n",
      "Iteration: 567  Loss: 0.62971  Accuracy: 0.645\n",
      "Iteration: 568  Loss: 0.62970  Accuracy: 0.645\n",
      "Iteration: 569  Loss: 0.62969  Accuracy: 0.645\n",
      "Iteration: 570  Loss: 0.62967  Accuracy: 0.645\n",
      "Iteration: 571  Loss: 0.62967  Accuracy: 0.645\n",
      "Iteration: 572  Loss: 0.62967  Accuracy: 0.645\n",
      "Iteration: 573  Loss: 0.62966  Accuracy: 0.645\n",
      "Iteration: 574  Loss: 0.62966  Accuracy: 0.645\n",
      "Iteration: 575  Loss: 0.62964  Accuracy: 0.645\n",
      "Iteration: 576  Loss: 0.62962  Accuracy: 0.645\n",
      "Iteration: 577  Loss: 0.62961  Accuracy: 0.645\n",
      "Iteration: 578  Loss: 0.62956  Accuracy: 0.645\n",
      "Iteration: 579  Loss: 0.62954  Accuracy: 0.645\n",
      "Iteration: 580  Loss: 0.62953  Accuracy: 0.645\n",
      "Iteration: 581  Loss: 0.62951  Accuracy: 0.645\n",
      "Iteration: 582  Loss: 0.62946  Accuracy: 0.645\n",
      "Iteration: 583  Loss: 0.62944  Accuracy: 0.646\n",
      "Iteration: 584  Loss: 0.62940  Accuracy: 0.646\n",
      "Iteration: 585  Loss: 0.62939  Accuracy: 0.645\n",
      "Iteration: 586  Loss: 0.62935  Accuracy: 0.645\n",
      "Iteration: 587  Loss: 0.62934  Accuracy: 0.645\n",
      "Iteration: 588  Loss: 0.62935  Accuracy: 0.645\n",
      "Iteration: 589  Loss: 0.62932  Accuracy: 0.645\n",
      "Iteration: 590  Loss: 0.62928  Accuracy: 0.645\n",
      "Iteration: 591  Loss: 0.62926  Accuracy: 0.645\n",
      "Iteration: 592  Loss: 0.62923  Accuracy: 0.645\n",
      "Iteration: 593  Loss: 0.62921  Accuracy: 0.645\n",
      "Iteration: 594  Loss: 0.62920  Accuracy: 0.645\n",
      "Iteration: 595  Loss: 0.62919  Accuracy: 0.646\n",
      "Iteration: 596  Loss: 0.62920  Accuracy: 0.645\n",
      "Iteration: 597  Loss: 0.62918  Accuracy: 0.646\n",
      "Iteration: 598  Loss: 0.62915  Accuracy: 0.646\n",
      "Iteration: 599  Loss: 0.62912  Accuracy: 0.646\n",
      "Iteration: 600  Loss: 0.62908  Accuracy: 0.646\n",
      "Iteration: 601  Loss: 0.62907  Accuracy: 0.646\n",
      "Iteration: 602  Loss: 0.62908  Accuracy: 0.646\n",
      "Iteration: 603  Loss: 0.62905  Accuracy: 0.647\n",
      "Iteration: 604  Loss: 0.62903  Accuracy: 0.646\n",
      "Iteration: 605  Loss: 0.62900  Accuracy: 0.646\n",
      "Iteration: 606  Loss: 0.62896  Accuracy: 0.646\n",
      "Iteration: 607  Loss: 0.62897  Accuracy: 0.646\n",
      "Iteration: 608  Loss: 0.62896  Accuracy: 0.646\n",
      "Iteration: 609  Loss: 0.62895  Accuracy: 0.646\n",
      "Iteration: 610  Loss: 0.62893  Accuracy: 0.647\n",
      "Iteration: 611  Loss: 0.62892  Accuracy: 0.647\n",
      "Iteration: 612  Loss: 0.62890  Accuracy: 0.646\n",
      "Iteration: 613  Loss: 0.62889  Accuracy: 0.646\n",
      "Iteration: 614  Loss: 0.62887  Accuracy: 0.647\n",
      "Iteration: 615  Loss: 0.62888  Accuracy: 0.647\n",
      "Iteration: 616  Loss: 0.62889  Accuracy: 0.647\n",
      "Iteration: 617  Loss: 0.62889  Accuracy: 0.647\n",
      "Iteration: 618  Loss: 0.62887  Accuracy: 0.647\n",
      "Iteration: 619  Loss: 0.62886  Accuracy: 0.647\n",
      "Iteration: 620  Loss: 0.62887  Accuracy: 0.647\n",
      "Iteration: 621  Loss: 0.62886  Accuracy: 0.647\n",
      "Iteration: 622  Loss: 0.62882  Accuracy: 0.647\n",
      "Iteration: 623  Loss: 0.62883  Accuracy: 0.647\n",
      "Iteration: 624  Loss: 0.62880  Accuracy: 0.647\n",
      "Iteration: 625  Loss: 0.62879  Accuracy: 0.647\n",
      "Iteration: 626  Loss: 0.62877  Accuracy: 0.647\n",
      "Iteration: 627  Loss: 0.62875  Accuracy: 0.647\n",
      "Iteration: 628  Loss: 0.62874  Accuracy: 0.647\n",
      "Iteration: 629  Loss: 0.62873  Accuracy: 0.647\n",
      "Iteration: 630  Loss: 0.62871  Accuracy: 0.647\n",
      "Iteration: 631  Loss: 0.62869  Accuracy: 0.647\n",
      "Iteration: 632  Loss: 0.62866  Accuracy: 0.647\n",
      "Iteration: 633  Loss: 0.62864  Accuracy: 0.647\n",
      "Iteration: 634  Loss: 0.62859  Accuracy: 0.647\n",
      "Iteration: 635  Loss: 0.62856  Accuracy: 0.647\n",
      "Iteration: 636  Loss: 0.62852  Accuracy: 0.648\n",
      "Iteration: 637  Loss: 0.62851  Accuracy: 0.648\n",
      "Iteration: 638  Loss: 0.62849  Accuracy: 0.648\n",
      "Iteration: 639  Loss: 0.62847  Accuracy: 0.648\n",
      "Iteration: 640  Loss: 0.62845  Accuracy: 0.648\n",
      "Iteration: 641  Loss: 0.62843  Accuracy: 0.648\n",
      "Iteration: 642  Loss: 0.62842  Accuracy: 0.648\n",
      "Iteration: 643  Loss: 0.62841  Accuracy: 0.648\n",
      "Iteration: 644  Loss: 0.62839  Accuracy: 0.648\n",
      "Iteration: 645  Loss: 0.62837  Accuracy: 0.648\n",
      "Iteration: 646  Loss: 0.62833  Accuracy: 0.648\n",
      "Iteration: 647  Loss: 0.62832  Accuracy: 0.648\n",
      "Iteration: 648  Loss: 0.62831  Accuracy: 0.648\n",
      "Iteration: 649  Loss: 0.62832  Accuracy: 0.648\n",
      "Iteration: 650  Loss: 0.62831  Accuracy: 0.648\n",
      "Iteration: 651  Loss: 0.62831  Accuracy: 0.648\n",
      "Iteration: 652  Loss: 0.62829  Accuracy: 0.648\n",
      "Iteration: 653  Loss: 0.62829  Accuracy: 0.648\n",
      "Iteration: 654  Loss: 0.62825  Accuracy: 0.648\n",
      "Iteration: 655  Loss: 0.62822  Accuracy: 0.648\n",
      "Iteration: 656  Loss: 0.62821  Accuracy: 0.648\n",
      "Iteration: 657  Loss: 0.62815  Accuracy: 0.648\n",
      "Iteration: 658  Loss: 0.62815  Accuracy: 0.648\n",
      "Iteration: 659  Loss: 0.62813  Accuracy: 0.648\n",
      "Iteration: 660  Loss: 0.62810  Accuracy: 0.649\n",
      "Iteration: 661  Loss: 0.62809  Accuracy: 0.649\n",
      "Iteration: 662  Loss: 0.62808  Accuracy: 0.649\n",
      "Iteration: 663  Loss: 0.62806  Accuracy: 0.649\n",
      "Iteration: 664  Loss: 0.62804  Accuracy: 0.648\n",
      "Iteration: 665  Loss: 0.62801  Accuracy: 0.649\n",
      "Iteration: 666  Loss: 0.62798  Accuracy: 0.648\n",
      "Iteration: 667  Loss: 0.62795  Accuracy: 0.648\n",
      "Iteration: 668  Loss: 0.62791  Accuracy: 0.649\n",
      "Iteration: 669  Loss: 0.62790  Accuracy: 0.649\n",
      "Iteration: 670  Loss: 0.62791  Accuracy: 0.649\n",
      "Iteration: 671  Loss: 0.62787  Accuracy: 0.649\n",
      "Iteration: 672  Loss: 0.62784  Accuracy: 0.649\n",
      "Iteration: 673  Loss: 0.62784  Accuracy: 0.649\n",
      "Iteration: 674  Loss: 0.62782  Accuracy: 0.649\n",
      "Iteration: 675  Loss: 0.62778  Accuracy: 0.649\n",
      "Iteration: 676  Loss: 0.62776  Accuracy: 0.649\n",
      "Iteration: 677  Loss: 0.62776  Accuracy: 0.649\n",
      "Iteration: 678  Loss: 0.62777  Accuracy: 0.649\n",
      "Iteration: 679  Loss: 0.62776  Accuracy: 0.649\n",
      "Iteration: 680  Loss: 0.62775  Accuracy: 0.649\n",
      "Iteration: 681  Loss: 0.62774  Accuracy: 0.649\n",
      "Iteration: 682  Loss: 0.62772  Accuracy: 0.649\n",
      "Iteration: 683  Loss: 0.62773  Accuracy: 0.649\n",
      "Iteration: 684  Loss: 0.62771  Accuracy: 0.649\n",
      "Iteration: 685  Loss: 0.62772  Accuracy: 0.649\n",
      "Iteration: 686  Loss: 0.62770  Accuracy: 0.649\n",
      "Iteration: 687  Loss: 0.62770  Accuracy: 0.649\n",
      "Iteration: 688  Loss: 0.62767  Accuracy: 0.649\n",
      "Iteration: 689  Loss: 0.62765  Accuracy: 0.649\n",
      "Iteration: 690  Loss: 0.62766  Accuracy: 0.649\n",
      "Iteration: 691  Loss: 0.62765  Accuracy: 0.649\n",
      "Iteration: 692  Loss: 0.62763  Accuracy: 0.649\n",
      "Iteration: 693  Loss: 0.62760  Accuracy: 0.649\n",
      "Iteration: 694  Loss: 0.62757  Accuracy: 0.649\n",
      "Iteration: 695  Loss: 0.62753  Accuracy: 0.649\n",
      "Iteration: 696  Loss: 0.62753  Accuracy: 0.649\n",
      "Iteration: 697  Loss: 0.62752  Accuracy: 0.649\n",
      "Iteration: 698  Loss: 0.62752  Accuracy: 0.649\n",
      "Iteration: 699  Loss: 0.62752  Accuracy: 0.649\n",
      "Iteration: 700  Loss: 0.62749  Accuracy: 0.649\n",
      "Iteration: 701  Loss: 0.62746  Accuracy: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 702  Loss: 0.62749  Accuracy: 0.649\n",
      "Iteration: 703  Loss: 0.62745  Accuracy: 0.649\n",
      "Iteration: 704  Loss: 0.62742  Accuracy: 0.649\n",
      "Iteration: 705  Loss: 0.62743  Accuracy: 0.649\n",
      "Iteration: 706  Loss: 0.62737  Accuracy: 0.649\n",
      "Iteration: 707  Loss: 0.62735  Accuracy: 0.649\n",
      "Iteration: 708  Loss: 0.62732  Accuracy: 0.649\n",
      "Iteration: 709  Loss: 0.62729  Accuracy: 0.649\n",
      "Iteration: 710  Loss: 0.62727  Accuracy: 0.649\n",
      "Iteration: 711  Loss: 0.62725  Accuracy: 0.649\n",
      "Iteration: 712  Loss: 0.62724  Accuracy: 0.649\n",
      "Iteration: 713  Loss: 0.62722  Accuracy: 0.649\n",
      "Iteration: 714  Loss: 0.62723  Accuracy: 0.649\n",
      "Iteration: 715  Loss: 0.62723  Accuracy: 0.649\n",
      "Iteration: 716  Loss: 0.62722  Accuracy: 0.649\n",
      "Iteration: 717  Loss: 0.62719  Accuracy: 0.649\n",
      "Iteration: 718  Loss: 0.62719  Accuracy: 0.649\n",
      "Iteration: 719  Loss: 0.62716  Accuracy: 0.649\n",
      "Iteration: 720  Loss: 0.62715  Accuracy: 0.649\n",
      "Iteration: 721  Loss: 0.62713  Accuracy: 0.649\n",
      "Iteration: 722  Loss: 0.62712  Accuracy: 0.649\n",
      "Iteration: 723  Loss: 0.62713  Accuracy: 0.649\n",
      "Iteration: 724  Loss: 0.62712  Accuracy: 0.649\n",
      "Iteration: 725  Loss: 0.62711  Accuracy: 0.649\n",
      "Iteration: 726  Loss: 0.62711  Accuracy: 0.649\n",
      "Iteration: 727  Loss: 0.62711  Accuracy: 0.649\n",
      "Iteration: 728  Loss: 0.62708  Accuracy: 0.649\n",
      "Iteration: 729  Loss: 0.62707  Accuracy: 0.649\n",
      "Iteration: 730  Loss: 0.62707  Accuracy: 0.649\n",
      "Iteration: 731  Loss: 0.62706  Accuracy: 0.649\n",
      "Iteration: 732  Loss: 0.62705  Accuracy: 0.649\n",
      "Iteration: 733  Loss: 0.62704  Accuracy: 0.649\n",
      "Iteration: 734  Loss: 0.62701  Accuracy: 0.650\n",
      "Iteration: 735  Loss: 0.62700  Accuracy: 0.650\n",
      "Iteration: 736  Loss: 0.62698  Accuracy: 0.650\n",
      "Iteration: 737  Loss: 0.62697  Accuracy: 0.650\n",
      "Iteration: 738  Loss: 0.62696  Accuracy: 0.650\n",
      "Iteration: 739  Loss: 0.62695  Accuracy: 0.650\n",
      "Iteration: 740  Loss: 0.62695  Accuracy: 0.650\n",
      "Iteration: 741  Loss: 0.62694  Accuracy: 0.650\n",
      "Iteration: 742  Loss: 0.62691  Accuracy: 0.650\n",
      "Iteration: 743  Loss: 0.62690  Accuracy: 0.650\n",
      "Iteration: 744  Loss: 0.62689  Accuracy: 0.650\n",
      "Iteration: 745  Loss: 0.62685  Accuracy: 0.650\n",
      "Iteration: 746  Loss: 0.62685  Accuracy: 0.650\n",
      "Iteration: 747  Loss: 0.62683  Accuracy: 0.650\n",
      "Iteration: 748  Loss: 0.62681  Accuracy: 0.650\n",
      "Iteration: 749  Loss: 0.62678  Accuracy: 0.650\n",
      "Iteration: 750  Loss: 0.62674  Accuracy: 0.650\n",
      "Iteration: 751  Loss: 0.62673  Accuracy: 0.650\n",
      "Iteration: 752  Loss: 0.62671  Accuracy: 0.650\n",
      "Iteration: 753  Loss: 0.62667  Accuracy: 0.650\n",
      "Iteration: 754  Loss: 0.62667  Accuracy: 0.650\n",
      "Iteration: 755  Loss: 0.62665  Accuracy: 0.650\n",
      "Iteration: 756  Loss: 0.62664  Accuracy: 0.650\n",
      "Iteration: 757  Loss: 0.62662  Accuracy: 0.650\n",
      "Iteration: 758  Loss: 0.62661  Accuracy: 0.650\n",
      "Iteration: 759  Loss: 0.62661  Accuracy: 0.650\n",
      "Iteration: 760  Loss: 0.62659  Accuracy: 0.650\n",
      "Iteration: 761  Loss: 0.62658  Accuracy: 0.650\n",
      "Iteration: 762  Loss: 0.62655  Accuracy: 0.650\n",
      "Iteration: 763  Loss: 0.62653  Accuracy: 0.650\n",
      "Iteration: 764  Loss: 0.62651  Accuracy: 0.650\n",
      "Iteration: 765  Loss: 0.62652  Accuracy: 0.650\n",
      "Iteration: 766  Loss: 0.62652  Accuracy: 0.650\n",
      "Iteration: 767  Loss: 0.62651  Accuracy: 0.650\n",
      "Iteration: 768  Loss: 0.62649  Accuracy: 0.650\n",
      "Iteration: 769  Loss: 0.62645  Accuracy: 0.650\n",
      "Iteration: 770  Loss: 0.62646  Accuracy: 0.650\n",
      "Iteration: 771  Loss: 0.62643  Accuracy: 0.650\n",
      "Iteration: 772  Loss: 0.62640  Accuracy: 0.650\n",
      "Iteration: 773  Loss: 0.62637  Accuracy: 0.650\n",
      "Iteration: 774  Loss: 0.62637  Accuracy: 0.650\n",
      "Iteration: 775  Loss: 0.62635  Accuracy: 0.650\n",
      "Iteration: 776  Loss: 0.62635  Accuracy: 0.650\n",
      "Iteration: 777  Loss: 0.62636  Accuracy: 0.650\n",
      "Iteration: 778  Loss: 0.62635  Accuracy: 0.650\n",
      "Iteration: 779  Loss: 0.62632  Accuracy: 0.650\n",
      "Iteration: 780  Loss: 0.62632  Accuracy: 0.650\n",
      "Iteration: 781  Loss: 0.62631  Accuracy: 0.650\n",
      "Iteration: 782  Loss: 0.62627  Accuracy: 0.650\n",
      "Iteration: 783  Loss: 0.62627  Accuracy: 0.650\n",
      "Iteration: 784  Loss: 0.62629  Accuracy: 0.650\n",
      "Iteration: 785  Loss: 0.62628  Accuracy: 0.650\n",
      "Iteration: 786  Loss: 0.62627  Accuracy: 0.650\n",
      "Iteration: 787  Loss: 0.62624  Accuracy: 0.650\n",
      "Iteration: 788  Loss: 0.62622  Accuracy: 0.650\n",
      "Iteration: 789  Loss: 0.62620  Accuracy: 0.651\n",
      "Iteration: 790  Loss: 0.62616  Accuracy: 0.651\n",
      "Iteration: 791  Loss: 0.62615  Accuracy: 0.651\n",
      "Iteration: 792  Loss: 0.62614  Accuracy: 0.651\n",
      "Iteration: 793  Loss: 0.62611  Accuracy: 0.651\n",
      "Iteration: 794  Loss: 0.62611  Accuracy: 0.651\n",
      "Iteration: 795  Loss: 0.62607  Accuracy: 0.651\n",
      "Iteration: 796  Loss: 0.62604  Accuracy: 0.651\n",
      "Iteration: 797  Loss: 0.62604  Accuracy: 0.651\n",
      "Iteration: 798  Loss: 0.62602  Accuracy: 0.651\n",
      "Iteration: 799  Loss: 0.62604  Accuracy: 0.651\n",
      "Iteration: 800  Loss: 0.62604  Accuracy: 0.651\n",
      "Iteration: 801  Loss: 0.62604  Accuracy: 0.651\n",
      "Iteration: 802  Loss: 0.62602  Accuracy: 0.651\n",
      "Iteration: 803  Loss: 0.62602  Accuracy: 0.651\n",
      "Iteration: 804  Loss: 0.62600  Accuracy: 0.651\n",
      "Iteration: 805  Loss: 0.62598  Accuracy: 0.651\n",
      "Iteration: 806  Loss: 0.62600  Accuracy: 0.651\n",
      "Iteration: 807  Loss: 0.62597  Accuracy: 0.651\n",
      "Iteration: 808  Loss: 0.62596  Accuracy: 0.651\n",
      "Iteration: 809  Loss: 0.62592  Accuracy: 0.651\n",
      "Iteration: 810  Loss: 0.62590  Accuracy: 0.651\n",
      "Iteration: 811  Loss: 0.62591  Accuracy: 0.651\n",
      "Iteration: 812  Loss: 0.62590  Accuracy: 0.651\n",
      "Iteration: 813  Loss: 0.62586  Accuracy: 0.651\n",
      "Iteration: 814  Loss: 0.62583  Accuracy: 0.651\n",
      "Iteration: 815  Loss: 0.62580  Accuracy: 0.651\n",
      "Iteration: 816  Loss: 0.62577  Accuracy: 0.651\n",
      "Iteration: 817  Loss: 0.62575  Accuracy: 0.651\n",
      "Iteration: 818  Loss: 0.62571  Accuracy: 0.651\n",
      "Iteration: 819  Loss: 0.62569  Accuracy: 0.651\n",
      "Iteration: 820  Loss: 0.62568  Accuracy: 0.651\n",
      "Iteration: 821  Loss: 0.62567  Accuracy: 0.651\n",
      "Iteration: 822  Loss: 0.62566  Accuracy: 0.651\n",
      "Iteration: 823  Loss: 0.62564  Accuracy: 0.651\n",
      "Iteration: 824  Loss: 0.62563  Accuracy: 0.651\n",
      "Iteration: 825  Loss: 0.62561  Accuracy: 0.651\n",
      "Iteration: 826  Loss: 0.62557  Accuracy: 0.651\n",
      "Iteration: 827  Loss: 0.62554  Accuracy: 0.651\n",
      "Iteration: 828  Loss: 0.62553  Accuracy: 0.651\n",
      "Iteration: 829  Loss: 0.62552  Accuracy: 0.651\n",
      "Iteration: 830  Loss: 0.62552  Accuracy: 0.651\n",
      "Iteration: 831  Loss: 0.62552  Accuracy: 0.651\n",
      "Iteration: 832  Loss: 0.62551  Accuracy: 0.651\n",
      "Iteration: 833  Loss: 0.62550  Accuracy: 0.651\n",
      "Iteration: 834  Loss: 0.62550  Accuracy: 0.651\n",
      "Iteration: 835  Loss: 0.62546  Accuracy: 0.651\n",
      "Iteration: 836  Loss: 0.62545  Accuracy: 0.651\n",
      "Iteration: 837  Loss: 0.62545  Accuracy: 0.651\n",
      "Iteration: 838  Loss: 0.62544  Accuracy: 0.651\n",
      "Iteration: 839  Loss: 0.62542  Accuracy: 0.651\n",
      "Iteration: 840  Loss: 0.62540  Accuracy: 0.651\n",
      "Iteration: 841  Loss: 0.62540  Accuracy: 0.651\n",
      "Iteration: 842  Loss: 0.62539  Accuracy: 0.652\n",
      "Iteration: 843  Loss: 0.62537  Accuracy: 0.651\n",
      "Iteration: 844  Loss: 0.62535  Accuracy: 0.651\n",
      "Iteration: 845  Loss: 0.62532  Accuracy: 0.651\n",
      "Iteration: 846  Loss: 0.62532  Accuracy: 0.651\n",
      "Iteration: 847  Loss: 0.62530  Accuracy: 0.651\n",
      "Iteration: 848  Loss: 0.62529  Accuracy: 0.651\n",
      "Iteration: 849  Loss: 0.62527  Accuracy: 0.651\n",
      "Iteration: 850  Loss: 0.62527  Accuracy: 0.652\n",
      "Iteration: 851  Loss: 0.62528  Accuracy: 0.652\n",
      "Iteration: 852  Loss: 0.62528  Accuracy: 0.652\n",
      "Iteration: 853  Loss: 0.62526  Accuracy: 0.652\n",
      "Iteration: 854  Loss: 0.62524  Accuracy: 0.652\n",
      "Iteration: 855  Loss: 0.62523  Accuracy: 0.652\n",
      "Iteration: 856  Loss: 0.62522  Accuracy: 0.652\n",
      "Iteration: 857  Loss: 0.62520  Accuracy: 0.652\n",
      "Iteration: 858  Loss: 0.62515  Accuracy: 0.652\n",
      "Iteration: 859  Loss: 0.62515  Accuracy: 0.652\n",
      "Iteration: 860  Loss: 0.62513  Accuracy: 0.652\n",
      "Iteration: 861  Loss: 0.62513  Accuracy: 0.652\n",
      "Iteration: 862  Loss: 0.62510  Accuracy: 0.652\n",
      "Iteration: 863  Loss: 0.62508  Accuracy: 0.652\n",
      "Iteration: 864  Loss: 0.62508  Accuracy: 0.652\n",
      "Iteration: 865  Loss: 0.62508  Accuracy: 0.652\n",
      "Iteration: 866  Loss: 0.62507  Accuracy: 0.652\n",
      "Iteration: 867  Loss: 0.62505  Accuracy: 0.652\n",
      "Iteration: 868  Loss: 0.62501  Accuracy: 0.652\n",
      "Iteration: 869  Loss: 0.62501  Accuracy: 0.652\n",
      "Iteration: 870  Loss: 0.62499  Accuracy: 0.652\n",
      "Iteration: 871  Loss: 0.62498  Accuracy: 0.652\n",
      "Iteration: 872  Loss: 0.62497  Accuracy: 0.652\n",
      "Iteration: 873  Loss: 0.62496  Accuracy: 0.652\n",
      "Iteration: 874  Loss: 0.62494  Accuracy: 0.652\n",
      "Iteration: 875  Loss: 0.62491  Accuracy: 0.653\n",
      "Iteration: 876  Loss: 0.62488  Accuracy: 0.653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 877  Loss: 0.62488  Accuracy: 0.652\n",
      "Iteration: 878  Loss: 0.62487  Accuracy: 0.652\n",
      "Iteration: 879  Loss: 0.62487  Accuracy: 0.653\n",
      "Iteration: 880  Loss: 0.62487  Accuracy: 0.653\n",
      "Iteration: 881  Loss: 0.62488  Accuracy: 0.653\n",
      "Iteration: 882  Loss: 0.62486  Accuracy: 0.653\n",
      "Iteration: 883  Loss: 0.62484  Accuracy: 0.653\n",
      "Iteration: 884  Loss: 0.62484  Accuracy: 0.653\n",
      "Iteration: 885  Loss: 0.62485  Accuracy: 0.653\n",
      "Iteration: 886  Loss: 0.62485  Accuracy: 0.653\n",
      "Iteration: 887  Loss: 0.62483  Accuracy: 0.653\n",
      "Iteration: 888  Loss: 0.62482  Accuracy: 0.653\n",
      "Iteration: 889  Loss: 0.62480  Accuracy: 0.653\n",
      "Iteration: 890  Loss: 0.62479  Accuracy: 0.653\n",
      "Iteration: 891  Loss: 0.62478  Accuracy: 0.653\n",
      "Iteration: 892  Loss: 0.62477  Accuracy: 0.653\n",
      "Iteration: 893  Loss: 0.62476  Accuracy: 0.653\n",
      "Iteration: 894  Loss: 0.62475  Accuracy: 0.653\n",
      "Iteration: 895  Loss: 0.62474  Accuracy: 0.653\n",
      "Iteration: 896  Loss: 0.62474  Accuracy: 0.653\n",
      "Iteration: 897  Loss: 0.62475  Accuracy: 0.653\n",
      "Iteration: 898  Loss: 0.62475  Accuracy: 0.653\n",
      "Iteration: 899  Loss: 0.62473  Accuracy: 0.653\n",
      "Iteration: 900  Loss: 0.62471  Accuracy: 0.653\n",
      "Iteration: 901  Loss: 0.62471  Accuracy: 0.653\n",
      "Iteration: 902  Loss: 0.62470  Accuracy: 0.653\n",
      "Iteration: 903  Loss: 0.62469  Accuracy: 0.653\n",
      "Iteration: 904  Loss: 0.62467  Accuracy: 0.653\n",
      "Iteration: 905  Loss: 0.62462  Accuracy: 0.653\n",
      "Iteration: 906  Loss: 0.62462  Accuracy: 0.653\n",
      "Iteration: 907  Loss: 0.62462  Accuracy: 0.653\n",
      "Iteration: 908  Loss: 0.62460  Accuracy: 0.653\n",
      "Iteration: 909  Loss: 0.62458  Accuracy: 0.653\n",
      "Iteration: 910  Loss: 0.62457  Accuracy: 0.653\n",
      "Iteration: 911  Loss: 0.62455  Accuracy: 0.653\n",
      "Iteration: 912  Loss: 0.62454  Accuracy: 0.653\n",
      "Iteration: 913  Loss: 0.62452  Accuracy: 0.653\n",
      "Iteration: 914  Loss: 0.62449  Accuracy: 0.653\n",
      "Iteration: 915  Loss: 0.62448  Accuracy: 0.653\n",
      "Iteration: 916  Loss: 0.62442  Accuracy: 0.653\n",
      "Iteration: 917  Loss: 0.62442  Accuracy: 0.653\n",
      "Iteration: 918  Loss: 0.62440  Accuracy: 0.653\n",
      "Iteration: 919  Loss: 0.62439  Accuracy: 0.653\n",
      "Iteration: 920  Loss: 0.62437  Accuracy: 0.654\n",
      "Iteration: 921  Loss: 0.62438  Accuracy: 0.654\n",
      "Iteration: 922  Loss: 0.62435  Accuracy: 0.654\n",
      "Iteration: 923  Loss: 0.62435  Accuracy: 0.654\n",
      "Iteration: 924  Loss: 0.62433  Accuracy: 0.654\n",
      "Iteration: 925  Loss: 0.62434  Accuracy: 0.654\n",
      "Iteration: 926  Loss: 0.62432  Accuracy: 0.654\n",
      "Iteration: 927  Loss: 0.62430  Accuracy: 0.654\n",
      "Iteration: 928  Loss: 0.62430  Accuracy: 0.654\n",
      "Iteration: 929  Loss: 0.62429  Accuracy: 0.654\n",
      "Iteration: 930  Loss: 0.62428  Accuracy: 0.654\n",
      "Iteration: 931  Loss: 0.62427  Accuracy: 0.654\n",
      "Iteration: 932  Loss: 0.62424  Accuracy: 0.654\n",
      "Iteration: 933  Loss: 0.62424  Accuracy: 0.653\n",
      "Iteration: 934  Loss: 0.62424  Accuracy: 0.653\n",
      "Iteration: 935  Loss: 0.62423  Accuracy: 0.654\n",
      "Iteration: 936  Loss: 0.62422  Accuracy: 0.654\n",
      "Iteration: 937  Loss: 0.62422  Accuracy: 0.654\n",
      "Iteration: 938  Loss: 0.62420  Accuracy: 0.654\n",
      "Iteration: 939  Loss: 0.62418  Accuracy: 0.654\n",
      "Iteration: 940  Loss: 0.62416  Accuracy: 0.654\n",
      "Iteration: 941  Loss: 0.62416  Accuracy: 0.654\n",
      "Iteration: 942  Loss: 0.62414  Accuracy: 0.654\n",
      "Iteration: 943  Loss: 0.62414  Accuracy: 0.654\n",
      "Iteration: 944  Loss: 0.62410  Accuracy: 0.654\n",
      "Iteration: 945  Loss: 0.62405  Accuracy: 0.654\n",
      "Iteration: 946  Loss: 0.62403  Accuracy: 0.654\n",
      "Iteration: 947  Loss: 0.62402  Accuracy: 0.654\n",
      "Iteration: 948  Loss: 0.62397  Accuracy: 0.654\n",
      "Iteration: 949  Loss: 0.62396  Accuracy: 0.654\n",
      "Iteration: 950  Loss: 0.62397  Accuracy: 0.654\n",
      "Iteration: 951  Loss: 0.62397  Accuracy: 0.654\n",
      "Iteration: 952  Loss: 0.62391  Accuracy: 0.654\n",
      "Iteration: 953  Loss: 0.62390  Accuracy: 0.654\n",
      "Iteration: 954  Loss: 0.62389  Accuracy: 0.654\n",
      "Iteration: 955  Loss: 0.62388  Accuracy: 0.654\n",
      "Iteration: 956  Loss: 0.62389  Accuracy: 0.654\n",
      "Iteration: 957  Loss: 0.62387  Accuracy: 0.654\n",
      "Iteration: 958  Loss: 0.62384  Accuracy: 0.654\n",
      "Iteration: 959  Loss: 0.62385  Accuracy: 0.654\n",
      "Iteration: 960  Loss: 0.62382  Accuracy: 0.654\n",
      "Iteration: 961  Loss: 0.62384  Accuracy: 0.654\n",
      "Iteration: 962  Loss: 0.62384  Accuracy: 0.654\n",
      "Iteration: 963  Loss: 0.62380  Accuracy: 0.654\n",
      "Iteration: 964  Loss: 0.62380  Accuracy: 0.655\n",
      "Iteration: 965  Loss: 0.62378  Accuracy: 0.655\n",
      "Iteration: 966  Loss: 0.62377  Accuracy: 0.655\n",
      "Iteration: 967  Loss: 0.62372  Accuracy: 0.655\n",
      "Iteration: 968  Loss: 0.62370  Accuracy: 0.655\n",
      "Iteration: 969  Loss: 0.62366  Accuracy: 0.655\n",
      "Iteration: 970  Loss: 0.62365  Accuracy: 0.655\n",
      "Iteration: 971  Loss: 0.62362  Accuracy: 0.655\n",
      "Iteration: 972  Loss: 0.62361  Accuracy: 0.655\n",
      "Iteration: 973  Loss: 0.62360  Accuracy: 0.655\n",
      "Iteration: 974  Loss: 0.62361  Accuracy: 0.655\n",
      "Iteration: 975  Loss: 0.62362  Accuracy: 0.655\n",
      "Iteration: 976  Loss: 0.62362  Accuracy: 0.655\n",
      "Iteration: 977  Loss: 0.62360  Accuracy: 0.655\n",
      "Iteration: 978  Loss: 0.62359  Accuracy: 0.655\n",
      "Iteration: 979  Loss: 0.62358  Accuracy: 0.655\n",
      "Iteration: 980  Loss: 0.62356  Accuracy: 0.655\n",
      "Iteration: 981  Loss: 0.62353  Accuracy: 0.655\n",
      "Iteration: 982  Loss: 0.62350  Accuracy: 0.655\n",
      "Iteration: 983  Loss: 0.62346  Accuracy: 0.655\n",
      "Iteration: 984  Loss: 0.62346  Accuracy: 0.655\n",
      "Iteration: 985  Loss: 0.62347  Accuracy: 0.655\n",
      "Iteration: 986  Loss: 0.62345  Accuracy: 0.655\n",
      "Iteration: 987  Loss: 0.62345  Accuracy: 0.655\n",
      "Iteration: 988  Loss: 0.62342  Accuracy: 0.655\n",
      "Iteration: 989  Loss: 0.62338  Accuracy: 0.655\n",
      "Iteration: 990  Loss: 0.62336  Accuracy: 0.655\n",
      "Iteration: 991  Loss: 0.62336  Accuracy: 0.655\n",
      "Iteration: 992  Loss: 0.62336  Accuracy: 0.655\n",
      "Iteration: 993  Loss: 0.62334  Accuracy: 0.655\n",
      "Iteration: 994  Loss: 0.62331  Accuracy: 0.655\n",
      "Iteration: 995  Loss: 0.62330  Accuracy: 0.655\n",
      "Iteration: 996  Loss: 0.62329  Accuracy: 0.655\n",
      "Iteration: 997  Loss: 0.62327  Accuracy: 0.655\n",
      "Iteration: 998  Loss: 0.62326  Accuracy: 0.655\n",
      "Iteration: 999  Loss: 0.62325  Accuracy: 0.655\n",
      "Iteration: 1000  Loss: 0.62323  Accuracy: 0.655\n",
      "Iteration: 1001  Loss: 0.62322  Accuracy: 0.655\n",
      "Iteration: 1002  Loss: 0.62323  Accuracy: 0.655\n",
      "Iteration: 1003  Loss: 0.62321  Accuracy: 0.655\n",
      "Iteration: 1004  Loss: 0.62320  Accuracy: 0.655\n",
      "Iteration: 1005  Loss: 0.62319  Accuracy: 0.655\n",
      "Iteration: 1006  Loss: 0.62318  Accuracy: 0.655\n",
      "Iteration: 1007  Loss: 0.62317  Accuracy: 0.655\n",
      "Iteration: 1008  Loss: 0.62317  Accuracy: 0.655\n",
      "Iteration: 1009  Loss: 0.62315  Accuracy: 0.655\n",
      "Iteration: 1010  Loss: 0.62312  Accuracy: 0.655\n",
      "Iteration: 1011  Loss: 0.62312  Accuracy: 0.655\n",
      "Iteration: 1012  Loss: 0.62311  Accuracy: 0.655\n",
      "Iteration: 1013  Loss: 0.62310  Accuracy: 0.655\n",
      "Iteration: 1014  Loss: 0.62309  Accuracy: 0.655\n",
      "Iteration: 1015  Loss: 0.62308  Accuracy: 0.655\n",
      "Iteration: 1016  Loss: 0.62307  Accuracy: 0.655\n",
      "Iteration: 1017  Loss: 0.62306  Accuracy: 0.655\n",
      "Iteration: 1018  Loss: 0.62305  Accuracy: 0.655\n",
      "Iteration: 1019  Loss: 0.62301  Accuracy: 0.655\n",
      "Iteration: 1020  Loss: 0.62301  Accuracy: 0.655\n",
      "Iteration: 1021  Loss: 0.62300  Accuracy: 0.655\n",
      "Iteration: 1022  Loss: 0.62299  Accuracy: 0.655\n",
      "Iteration: 1023  Loss: 0.62295  Accuracy: 0.655\n",
      "Iteration: 1024  Loss: 0.62294  Accuracy: 0.655\n",
      "Iteration: 1025  Loss: 0.62297  Accuracy: 0.655\n",
      "Iteration: 1026  Loss: 0.62295  Accuracy: 0.656\n",
      "Iteration: 1027  Loss: 0.62292  Accuracy: 0.656\n",
      "Iteration: 1028  Loss: 0.62291  Accuracy: 0.656\n",
      "Iteration: 1029  Loss: 0.62291  Accuracy: 0.656\n",
      "Iteration: 1030  Loss: 0.62285  Accuracy: 0.656\n",
      "Iteration: 1031  Loss: 0.62283  Accuracy: 0.656\n",
      "Iteration: 1032  Loss: 0.62283  Accuracy: 0.656\n",
      "Iteration: 1033  Loss: 0.62283  Accuracy: 0.656\n",
      "Iteration: 1034  Loss: 0.62283  Accuracy: 0.656\n",
      "Iteration: 1035  Loss: 0.62281  Accuracy: 0.656\n",
      "Iteration: 1036  Loss: 0.62282  Accuracy: 0.656\n",
      "Iteration: 1037  Loss: 0.62281  Accuracy: 0.656\n",
      "Iteration: 1038  Loss: 0.62281  Accuracy: 0.656\n",
      "Iteration: 1039  Loss: 0.62279  Accuracy: 0.656\n",
      "Iteration: 1040  Loss: 0.62278  Accuracy: 0.655\n",
      "Iteration: 1041  Loss: 0.62275  Accuracy: 0.655\n",
      "Iteration: 1042  Loss: 0.62274  Accuracy: 0.655\n",
      "Iteration: 1043  Loss: 0.62272  Accuracy: 0.655\n",
      "Iteration: 1044  Loss: 0.62273  Accuracy: 0.655\n",
      "Iteration: 1045  Loss: 0.62272  Accuracy: 0.655\n",
      "Iteration: 1046  Loss: 0.62270  Accuracy: 0.656\n",
      "Iteration: 1047  Loss: 0.62269  Accuracy: 0.655\n",
      "Iteration: 1048  Loss: 0.62268  Accuracy: 0.656\n",
      "Iteration: 1049  Loss: 0.62265  Accuracy: 0.656\n",
      "Iteration: 1050  Loss: 0.62263  Accuracy: 0.656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1051  Loss: 0.62264  Accuracy: 0.656\n",
      "Iteration: 1052  Loss: 0.62262  Accuracy: 0.655\n",
      "Iteration: 1053  Loss: 0.62261  Accuracy: 0.655\n",
      "Iteration: 1054  Loss: 0.62260  Accuracy: 0.655\n",
      "Iteration: 1055  Loss: 0.62259  Accuracy: 0.655\n",
      "Iteration: 1056  Loss: 0.62257  Accuracy: 0.655\n",
      "Iteration: 1057  Loss: 0.62257  Accuracy: 0.655\n",
      "Iteration: 1058  Loss: 0.62257  Accuracy: 0.656\n",
      "Iteration: 1059  Loss: 0.62256  Accuracy: 0.656\n",
      "Iteration: 1060  Loss: 0.62255  Accuracy: 0.656\n",
      "Iteration: 1061  Loss: 0.62252  Accuracy: 0.656\n",
      "Iteration: 1062  Loss: 0.62251  Accuracy: 0.656\n",
      "Iteration: 1063  Loss: 0.62248  Accuracy: 0.656\n",
      "Iteration: 1064  Loss: 0.62246  Accuracy: 0.656\n",
      "Iteration: 1065  Loss: 0.62249  Accuracy: 0.656\n",
      "Iteration: 1066  Loss: 0.62247  Accuracy: 0.656\n",
      "Iteration: 1067  Loss: 0.62245  Accuracy: 0.656\n",
      "Iteration: 1068  Loss: 0.62244  Accuracy: 0.656\n",
      "Iteration: 1069  Loss: 0.62243  Accuracy: 0.656\n",
      "Iteration: 1070  Loss: 0.62242  Accuracy: 0.656\n",
      "Iteration: 1071  Loss: 0.62241  Accuracy: 0.656\n",
      "Iteration: 1072  Loss: 0.62240  Accuracy: 0.656\n",
      "Iteration: 1073  Loss: 0.62239  Accuracy: 0.656\n",
      "Iteration: 1074  Loss: 0.62238  Accuracy: 0.656\n",
      "Iteration: 1075  Loss: 0.62238  Accuracy: 0.656\n",
      "Iteration: 1076  Loss: 0.62232  Accuracy: 0.656\n",
      "Iteration: 1077  Loss: 0.62231  Accuracy: 0.656\n",
      "Iteration: 1078  Loss: 0.62230  Accuracy: 0.656\n",
      "Iteration: 1079  Loss: 0.62228  Accuracy: 0.656\n",
      "Iteration: 1080  Loss: 0.62226  Accuracy: 0.656\n",
      "Iteration: 1081  Loss: 0.62224  Accuracy: 0.656\n",
      "Iteration: 1082  Loss: 0.62223  Accuracy: 0.656\n",
      "Iteration: 1083  Loss: 0.62222  Accuracy: 0.656\n",
      "Iteration: 1084  Loss: 0.62221  Accuracy: 0.656\n",
      "Iteration: 1085  Loss: 0.62222  Accuracy: 0.656\n",
      "Iteration: 1086  Loss: 0.62222  Accuracy: 0.656\n",
      "Iteration: 1087  Loss: 0.62223  Accuracy: 0.656\n",
      "Iteration: 1088  Loss: 0.62222  Accuracy: 0.656\n",
      "Iteration: 1089  Loss: 0.62222  Accuracy: 0.656\n",
      "Iteration: 1090  Loss: 0.62220  Accuracy: 0.657\n",
      "Iteration: 1091  Loss: 0.62219  Accuracy: 0.657\n",
      "Iteration: 1092  Loss: 0.62219  Accuracy: 0.657\n",
      "Iteration: 1093  Loss: 0.62217  Accuracy: 0.656\n",
      "Iteration: 1094  Loss: 0.62215  Accuracy: 0.657\n",
      "Iteration: 1095  Loss: 0.62215  Accuracy: 0.656\n",
      "Iteration: 1096  Loss: 0.62214  Accuracy: 0.656\n",
      "Iteration: 1097  Loss: 0.62213  Accuracy: 0.656\n",
      "Iteration: 1098  Loss: 0.62214  Accuracy: 0.656\n",
      "Iteration: 1099  Loss: 0.62214  Accuracy: 0.656\n",
      "Iteration: 1100  Loss: 0.62212  Accuracy: 0.657\n",
      "Iteration: 1101  Loss: 0.62212  Accuracy: 0.657\n",
      "Iteration: 1102  Loss: 0.62211  Accuracy: 0.656\n",
      "Iteration: 1103  Loss: 0.62209  Accuracy: 0.656\n",
      "Iteration: 1104  Loss: 0.62209  Accuracy: 0.656\n",
      "Iteration: 1105  Loss: 0.62206  Accuracy: 0.657\n",
      "Iteration: 1106  Loss: 0.62206  Accuracy: 0.657\n",
      "Iteration: 1107  Loss: 0.62204  Accuracy: 0.657\n",
      "Iteration: 1108  Loss: 0.62203  Accuracy: 0.657\n",
      "Iteration: 1109  Loss: 0.62202  Accuracy: 0.657\n",
      "Iteration: 1110  Loss: 0.62202  Accuracy: 0.657\n",
      "Iteration: 1111  Loss: 0.62201  Accuracy: 0.657\n",
      "Iteration: 1112  Loss: 0.62201  Accuracy: 0.657\n",
      "Iteration: 1113  Loss: 0.62200  Accuracy: 0.657\n",
      "Iteration: 1114  Loss: 0.62199  Accuracy: 0.657\n",
      "Iteration: 1115  Loss: 0.62195  Accuracy: 0.657\n",
      "Iteration: 1116  Loss: 0.62195  Accuracy: 0.657\n",
      "Iteration: 1117  Loss: 0.62196  Accuracy: 0.657\n",
      "Iteration: 1118  Loss: 0.62193  Accuracy: 0.657\n",
      "Iteration: 1119  Loss: 0.62190  Accuracy: 0.657\n",
      "Iteration: 1120  Loss: 0.62187  Accuracy: 0.657\n",
      "Iteration: 1121  Loss: 0.62188  Accuracy: 0.657\n",
      "Iteration: 1122  Loss: 0.62187  Accuracy: 0.657\n",
      "Iteration: 1123  Loss: 0.62183  Accuracy: 0.657\n",
      "Iteration: 1124  Loss: 0.62184  Accuracy: 0.657\n",
      "Iteration: 1125  Loss: 0.62183  Accuracy: 0.657\n",
      "Iteration: 1126  Loss: 0.62181  Accuracy: 0.657\n",
      "Iteration: 1127  Loss: 0.62178  Accuracy: 0.657\n",
      "Iteration: 1128  Loss: 0.62177  Accuracy: 0.657\n",
      "Iteration: 1129  Loss: 0.62175  Accuracy: 0.657\n",
      "Iteration: 1130  Loss: 0.62173  Accuracy: 0.657\n",
      "Iteration: 1131  Loss: 0.62174  Accuracy: 0.657\n",
      "Iteration: 1132  Loss: 0.62172  Accuracy: 0.657\n",
      "Iteration: 1133  Loss: 0.62170  Accuracy: 0.657\n",
      "Iteration: 1134  Loss: 0.62169  Accuracy: 0.657\n",
      "Iteration: 1135  Loss: 0.62168  Accuracy: 0.657\n",
      "Iteration: 1136  Loss: 0.62168  Accuracy: 0.657\n",
      "Iteration: 1137  Loss: 0.62168  Accuracy: 0.657\n",
      "Iteration: 1138  Loss: 0.62166  Accuracy: 0.657\n",
      "Iteration: 1139  Loss: 0.62166  Accuracy: 0.657\n",
      "Iteration: 1140  Loss: 0.62165  Accuracy: 0.657\n",
      "Iteration: 1141  Loss: 0.62165  Accuracy: 0.657\n",
      "Iteration: 1142  Loss: 0.62162  Accuracy: 0.657\n",
      "Iteration: 1143  Loss: 0.62161  Accuracy: 0.657\n",
      "Iteration: 1144  Loss: 0.62162  Accuracy: 0.657\n",
      "Iteration: 1145  Loss: 0.62163  Accuracy: 0.657\n",
      "Iteration: 1146  Loss: 0.62161  Accuracy: 0.657\n",
      "Iteration: 1147  Loss: 0.62159  Accuracy: 0.658\n",
      "Iteration: 1148  Loss: 0.62159  Accuracy: 0.658\n",
      "Iteration: 1149  Loss: 0.62159  Accuracy: 0.657\n",
      "Iteration: 1150  Loss: 0.62158  Accuracy: 0.657\n",
      "Iteration: 1151  Loss: 0.62157  Accuracy: 0.657\n",
      "Iteration: 1152  Loss: 0.62156  Accuracy: 0.658\n",
      "Iteration: 1153  Loss: 0.62153  Accuracy: 0.658\n",
      "Iteration: 1154  Loss: 0.62152  Accuracy: 0.658\n",
      "Iteration: 1155  Loss: 0.62153  Accuracy: 0.658\n",
      "Iteration: 1156  Loss: 0.62151  Accuracy: 0.658\n",
      "Iteration: 1157  Loss: 0.62149  Accuracy: 0.657\n",
      "Iteration: 1158  Loss: 0.62147  Accuracy: 0.658\n",
      "Iteration: 1159  Loss: 0.62147  Accuracy: 0.658\n",
      "Iteration: 1160  Loss: 0.62146  Accuracy: 0.658\n",
      "Iteration: 1161  Loss: 0.62146  Accuracy: 0.658\n",
      "Iteration: 1162  Loss: 0.62146  Accuracy: 0.658\n",
      "Iteration: 1163  Loss: 0.62146  Accuracy: 0.658\n",
      "Iteration: 1164  Loss: 0.62145  Accuracy: 0.658\n",
      "Iteration: 1165  Loss: 0.62144  Accuracy: 0.658\n",
      "Iteration: 1166  Loss: 0.62144  Accuracy: 0.658\n",
      "Iteration: 1167  Loss: 0.62142  Accuracy: 0.658\n",
      "Iteration: 1168  Loss: 0.62139  Accuracy: 0.658\n",
      "Iteration: 1169  Loss: 0.62139  Accuracy: 0.658\n",
      "Iteration: 1170  Loss: 0.62138  Accuracy: 0.658\n",
      "Iteration: 1171  Loss: 0.62139  Accuracy: 0.658\n",
      "Iteration: 1172  Loss: 0.62136  Accuracy: 0.658\n",
      "Iteration: 1173  Loss: 0.62136  Accuracy: 0.658\n",
      "Iteration: 1174  Loss: 0.62135  Accuracy: 0.658\n",
      "Iteration: 1175  Loss: 0.62136  Accuracy: 0.658\n",
      "Iteration: 1176  Loss: 0.62136  Accuracy: 0.658\n",
      "Iteration: 1177  Loss: 0.62135  Accuracy: 0.658\n",
      "Iteration: 1178  Loss: 0.62131  Accuracy: 0.658\n",
      "Iteration: 1179  Loss: 0.62131  Accuracy: 0.658\n",
      "Iteration: 1180  Loss: 0.62129  Accuracy: 0.658\n",
      "Iteration: 1181  Loss: 0.62127  Accuracy: 0.658\n",
      "Iteration: 1182  Loss: 0.62126  Accuracy: 0.658\n",
      "Iteration: 1183  Loss: 0.62122  Accuracy: 0.658\n",
      "Iteration: 1184  Loss: 0.62124  Accuracy: 0.658\n",
      "Iteration: 1185  Loss: 0.62123  Accuracy: 0.658\n",
      "Iteration: 1186  Loss: 0.62123  Accuracy: 0.658\n",
      "Iteration: 1187  Loss: 0.62123  Accuracy: 0.658\n",
      "Iteration: 1188  Loss: 0.62123  Accuracy: 0.658\n",
      "Iteration: 1189  Loss: 0.62119  Accuracy: 0.658\n",
      "Iteration: 1190  Loss: 0.62117  Accuracy: 0.658\n",
      "Iteration: 1191  Loss: 0.62116  Accuracy: 0.658\n",
      "Iteration: 1192  Loss: 0.62115  Accuracy: 0.658\n",
      "Iteration: 1193  Loss: 0.62112  Accuracy: 0.658\n",
      "Iteration: 1194  Loss: 0.62111  Accuracy: 0.658\n",
      "Iteration: 1195  Loss: 0.62110  Accuracy: 0.658\n",
      "Iteration: 1196  Loss: 0.62108  Accuracy: 0.658\n",
      "Iteration: 1197  Loss: 0.62107  Accuracy: 0.658\n",
      "Iteration: 1198  Loss: 0.62106  Accuracy: 0.658\n",
      "Iteration: 1199  Loss: 0.62105  Accuracy: 0.658\n",
      "Iteration: 1200  Loss: 0.62106  Accuracy: 0.658\n",
      "Iteration: 1201  Loss: 0.62104  Accuracy: 0.658\n",
      "Iteration: 1202  Loss: 0.62104  Accuracy: 0.658\n",
      "Iteration: 1203  Loss: 0.62105  Accuracy: 0.658\n",
      "Iteration: 1204  Loss: 0.62105  Accuracy: 0.658\n",
      "Iteration: 1205  Loss: 0.62104  Accuracy: 0.658\n",
      "Iteration: 1206  Loss: 0.62105  Accuracy: 0.658\n",
      "Iteration: 1207  Loss: 0.62104  Accuracy: 0.658\n",
      "Iteration: 1208  Loss: 0.62103  Accuracy: 0.658\n",
      "Iteration: 1209  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1210  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1211  Loss: 0.62102  Accuracy: 0.658\n",
      "Iteration: 1212  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1213  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1214  Loss: 0.62100  Accuracy: 0.658\n",
      "Iteration: 1215  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1216  Loss: 0.62103  Accuracy: 0.658\n",
      "Iteration: 1217  Loss: 0.62104  Accuracy: 0.658\n",
      "Iteration: 1218  Loss: 0.62101  Accuracy: 0.658\n",
      "Iteration: 1219  Loss: 0.62098  Accuracy: 0.658\n",
      "Iteration: 1220  Loss: 0.62099  Accuracy: 0.658\n",
      "Iteration: 1221  Loss: 0.62098  Accuracy: 0.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1222  Loss: 0.62097  Accuracy: 0.658\n",
      "Iteration: 1223  Loss: 0.62099  Accuracy: 0.658\n",
      "Iteration: 1224  Loss: 0.62098  Accuracy: 0.658\n",
      "Iteration: 1225  Loss: 0.62097  Accuracy: 0.658\n",
      "Iteration: 1226  Loss: 0.62096  Accuracy: 0.658\n",
      "Iteration: 1227  Loss: 0.62097  Accuracy: 0.658\n",
      "Iteration: 1228  Loss: 0.62095  Accuracy: 0.658\n",
      "Iteration: 1229  Loss: 0.62094  Accuracy: 0.658\n",
      "Iteration: 1230  Loss: 0.62093  Accuracy: 0.658\n",
      "Iteration: 1231  Loss: 0.62092  Accuracy: 0.658\n",
      "Iteration: 1232  Loss: 0.62090  Accuracy: 0.658\n",
      "Iteration: 1233  Loss: 0.62090  Accuracy: 0.658\n",
      "Iteration: 1234  Loss: 0.62088  Accuracy: 0.658\n",
      "Iteration: 1235  Loss: 0.62088  Accuracy: 0.658\n",
      "Iteration: 1236  Loss: 0.62085  Accuracy: 0.658\n",
      "Iteration: 1237  Loss: 0.62085  Accuracy: 0.658\n",
      "Iteration: 1238  Loss: 0.62083  Accuracy: 0.658\n",
      "Iteration: 1239  Loss: 0.62083  Accuracy: 0.658\n",
      "Iteration: 1240  Loss: 0.62082  Accuracy: 0.658\n",
      "Iteration: 1241  Loss: 0.62079  Accuracy: 0.658\n",
      "Iteration: 1242  Loss: 0.62079  Accuracy: 0.658\n",
      "Iteration: 1243  Loss: 0.62078  Accuracy: 0.658\n",
      "Iteration: 1244  Loss: 0.62076  Accuracy: 0.658\n",
      "Iteration: 1245  Loss: 0.62074  Accuracy: 0.658\n",
      "Iteration: 1246  Loss: 0.62070  Accuracy: 0.658\n",
      "Iteration: 1247  Loss: 0.62069  Accuracy: 0.658\n",
      "Iteration: 1248  Loss: 0.62068  Accuracy: 0.658\n",
      "Iteration: 1249  Loss: 0.62068  Accuracy: 0.659\n",
      "Iteration: 1250  Loss: 0.62067  Accuracy: 0.658\n",
      "Iteration: 1251  Loss: 0.62067  Accuracy: 0.658\n",
      "Iteration: 1252  Loss: 0.62064  Accuracy: 0.658\n",
      "Iteration: 1253  Loss: 0.62059  Accuracy: 0.658\n",
      "Iteration: 1254  Loss: 0.62059  Accuracy: 0.658\n",
      "Iteration: 1255  Loss: 0.62061  Accuracy: 0.658\n",
      "Iteration: 1256  Loss: 0.62061  Accuracy: 0.658\n",
      "Iteration: 1257  Loss: 0.62061  Accuracy: 0.658\n",
      "Iteration: 1258  Loss: 0.62061  Accuracy: 0.658\n",
      "Iteration: 1259  Loss: 0.62060  Accuracy: 0.658\n",
      "Iteration: 1260  Loss: 0.62059  Accuracy: 0.658\n",
      "Iteration: 1261  Loss: 0.62058  Accuracy: 0.658\n",
      "Iteration: 1262  Loss: 0.62058  Accuracy: 0.658\n",
      "Iteration: 1263  Loss: 0.62057  Accuracy: 0.659\n",
      "Iteration: 1264  Loss: 0.62055  Accuracy: 0.659\n",
      "Iteration: 1265  Loss: 0.62054  Accuracy: 0.659\n",
      "Iteration: 1266  Loss: 0.62053  Accuracy: 0.659\n",
      "Iteration: 1267  Loss: 0.62052  Accuracy: 0.659\n",
      "Iteration: 1268  Loss: 0.62051  Accuracy: 0.659\n",
      "Iteration: 1269  Loss: 0.62048  Accuracy: 0.659\n",
      "Iteration: 1270  Loss: 0.62047  Accuracy: 0.659\n",
      "Iteration: 1271  Loss: 0.62045  Accuracy: 0.659\n",
      "Iteration: 1272  Loss: 0.62048  Accuracy: 0.659\n",
      "Iteration: 1273  Loss: 0.62047  Accuracy: 0.659\n",
      "Iteration: 1274  Loss: 0.62047  Accuracy: 0.659\n",
      "Iteration: 1275  Loss: 0.62048  Accuracy: 0.659\n",
      "Iteration: 1276  Loss: 0.62046  Accuracy: 0.659\n",
      "Iteration: 1277  Loss: 0.62042  Accuracy: 0.659\n",
      "Iteration: 1278  Loss: 0.62041  Accuracy: 0.659\n",
      "Iteration: 1279  Loss: 0.62041  Accuracy: 0.659\n",
      "Iteration: 1280  Loss: 0.62041  Accuracy: 0.659\n",
      "Iteration: 1281  Loss: 0.62037  Accuracy: 0.659\n",
      "Iteration: 1282  Loss: 0.62035  Accuracy: 0.659\n",
      "Iteration: 1283  Loss: 0.62033  Accuracy: 0.659\n",
      "Iteration: 1284  Loss: 0.62031  Accuracy: 0.659\n",
      "Iteration: 1285  Loss: 0.62031  Accuracy: 0.659\n",
      "Iteration: 1286  Loss: 0.62030  Accuracy: 0.659\n",
      "Iteration: 1287  Loss: 0.62030  Accuracy: 0.659\n",
      "Iteration: 1288  Loss: 0.62028  Accuracy: 0.659\n",
      "Iteration: 1289  Loss: 0.62026  Accuracy: 0.659\n",
      "Iteration: 1290  Loss: 0.62027  Accuracy: 0.659\n",
      "Iteration: 1291  Loss: 0.62027  Accuracy: 0.659\n",
      "Iteration: 1292  Loss: 0.62026  Accuracy: 0.659\n",
      "Iteration: 1293  Loss: 0.62024  Accuracy: 0.659\n",
      "Iteration: 1294  Loss: 0.62023  Accuracy: 0.659\n",
      "Iteration: 1295  Loss: 0.62022  Accuracy: 0.659\n",
      "Iteration: 1296  Loss: 0.62019  Accuracy: 0.659\n",
      "Iteration: 1297  Loss: 0.62017  Accuracy: 0.659\n",
      "Iteration: 1298  Loss: 0.62015  Accuracy: 0.659\n",
      "Iteration: 1299  Loss: 0.62015  Accuracy: 0.659\n",
      "Iteration: 1300  Loss: 0.62015  Accuracy: 0.659\n",
      "Iteration: 1301  Loss: 0.62014  Accuracy: 0.659\n",
      "Iteration: 1302  Loss: 0.62014  Accuracy: 0.659\n",
      "Iteration: 1303  Loss: 0.62013  Accuracy: 0.659\n",
      "Iteration: 1304  Loss: 0.62013  Accuracy: 0.659\n",
      "Iteration: 1305  Loss: 0.62011  Accuracy: 0.659\n",
      "Iteration: 1306  Loss: 0.62010  Accuracy: 0.659\n",
      "Iteration: 1307  Loss: 0.62009  Accuracy: 0.659\n",
      "Iteration: 1308  Loss: 0.62009  Accuracy: 0.659\n",
      "Iteration: 1309  Loss: 0.62009  Accuracy: 0.659\n",
      "Iteration: 1310  Loss: 0.62008  Accuracy: 0.659\n",
      "Iteration: 1311  Loss: 0.62008  Accuracy: 0.659\n",
      "Iteration: 1312  Loss: 0.62008  Accuracy: 0.659\n",
      "Iteration: 1313  Loss: 0.62007  Accuracy: 0.659\n",
      "Iteration: 1314  Loss: 0.62007  Accuracy: 0.659\n",
      "Iteration: 1315  Loss: 0.62004  Accuracy: 0.659\n",
      "Iteration: 1316  Loss: 0.62002  Accuracy: 0.659\n",
      "Iteration: 1317  Loss: 0.62001  Accuracy: 0.659\n",
      "Iteration: 1318  Loss: 0.61999  Accuracy: 0.659\n",
      "Iteration: 1319  Loss: 0.61997  Accuracy: 0.659\n",
      "Iteration: 1320  Loss: 0.61997  Accuracy: 0.659\n",
      "Iteration: 1321  Loss: 0.61997  Accuracy: 0.659\n",
      "Iteration: 1322  Loss: 0.61997  Accuracy: 0.659\n",
      "Iteration: 1323  Loss: 0.61997  Accuracy: 0.659\n",
      "Iteration: 1324  Loss: 0.61996  Accuracy: 0.659\n",
      "Iteration: 1325  Loss: 0.61995  Accuracy: 0.659\n",
      "Iteration: 1326  Loss: 0.61994  Accuracy: 0.659\n",
      "Iteration: 1327  Loss: 0.61993  Accuracy: 0.659\n",
      "Iteration: 1328  Loss: 0.61992  Accuracy: 0.659\n",
      "Iteration: 1329  Loss: 0.61991  Accuracy: 0.659\n",
      "Iteration: 1330  Loss: 0.61993  Accuracy: 0.659\n",
      "Iteration: 1331  Loss: 0.61993  Accuracy: 0.659\n",
      "Iteration: 1332  Loss: 0.61993  Accuracy: 0.659\n",
      "Iteration: 1333  Loss: 0.61990  Accuracy: 0.659\n",
      "Iteration: 1334  Loss: 0.61991  Accuracy: 0.659\n",
      "Iteration: 1335  Loss: 0.61989  Accuracy: 0.659\n",
      "Iteration: 1336  Loss: 0.61990  Accuracy: 0.659\n",
      "Iteration: 1337  Loss: 0.61990  Accuracy: 0.659\n",
      "Iteration: 1338  Loss: 0.61990  Accuracy: 0.659\n",
      "Iteration: 1339  Loss: 0.61988  Accuracy: 0.659\n",
      "Iteration: 1340  Loss: 0.61985  Accuracy: 0.659\n",
      "Iteration: 1341  Loss: 0.61983  Accuracy: 0.659\n",
      "Iteration: 1342  Loss: 0.61982  Accuracy: 0.659\n",
      "Iteration: 1343  Loss: 0.61980  Accuracy: 0.659\n",
      "Iteration: 1344  Loss: 0.61979  Accuracy: 0.659\n",
      "Iteration: 1345  Loss: 0.61978  Accuracy: 0.659\n",
      "Iteration: 1346  Loss: 0.61979  Accuracy: 0.659\n",
      "Iteration: 1347  Loss: 0.61981  Accuracy: 0.659\n",
      "Iteration: 1348  Loss: 0.61979  Accuracy: 0.659\n",
      "Iteration: 1349  Loss: 0.61977  Accuracy: 0.659\n",
      "Iteration: 1350  Loss: 0.61977  Accuracy: 0.659\n",
      "Iteration: 1351  Loss: 0.61978  Accuracy: 0.659\n",
      "Iteration: 1352  Loss: 0.61977  Accuracy: 0.659\n",
      "Iteration: 1353  Loss: 0.61975  Accuracy: 0.659\n",
      "Iteration: 1354  Loss: 0.61977  Accuracy: 0.659\n",
      "Iteration: 1355  Loss: 0.61974  Accuracy: 0.659\n",
      "Iteration: 1356  Loss: 0.61975  Accuracy: 0.659\n",
      "Iteration: 1357  Loss: 0.61974  Accuracy: 0.659\n",
      "Iteration: 1358  Loss: 0.61970  Accuracy: 0.659\n",
      "Iteration: 1359  Loss: 0.61967  Accuracy: 0.659\n",
      "Iteration: 1360  Loss: 0.61966  Accuracy: 0.659\n",
      "Iteration: 1361  Loss: 0.61967  Accuracy: 0.660\n",
      "Iteration: 1362  Loss: 0.61965  Accuracy: 0.659\n",
      "Iteration: 1363  Loss: 0.61963  Accuracy: 0.659\n",
      "Iteration: 1364  Loss: 0.61963  Accuracy: 0.659\n",
      "Iteration: 1365  Loss: 0.61966  Accuracy: 0.659\n",
      "Iteration: 1366  Loss: 0.61965  Accuracy: 0.660\n",
      "Iteration: 1367  Loss: 0.61965  Accuracy: 0.660\n",
      "Iteration: 1368  Loss: 0.61964  Accuracy: 0.660\n",
      "Iteration: 1369  Loss: 0.61963  Accuracy: 0.660\n",
      "Iteration: 1370  Loss: 0.61961  Accuracy: 0.660\n",
      "Iteration: 1371  Loss: 0.61960  Accuracy: 0.660\n",
      "Iteration: 1372  Loss: 0.61957  Accuracy: 0.660\n",
      "Iteration: 1373  Loss: 0.61954  Accuracy: 0.660\n",
      "Iteration: 1374  Loss: 0.61955  Accuracy: 0.660\n",
      "Iteration: 1375  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1376  Loss: 0.61953  Accuracy: 0.660\n",
      "Iteration: 1377  Loss: 0.61954  Accuracy: 0.660\n",
      "Iteration: 1378  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1379  Loss: 0.61953  Accuracy: 0.660\n",
      "Iteration: 1380  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1381  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1382  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1383  Loss: 0.61951  Accuracy: 0.660\n",
      "Iteration: 1384  Loss: 0.61950  Accuracy: 0.660\n",
      "Iteration: 1385  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1386  Loss: 0.61951  Accuracy: 0.660\n",
      "Iteration: 1387  Loss: 0.61953  Accuracy: 0.660\n",
      "Iteration: 1388  Loss: 0.61950  Accuracy: 0.660\n",
      "Iteration: 1389  Loss: 0.61952  Accuracy: 0.660\n",
      "Iteration: 1390  Loss: 0.61950  Accuracy: 0.660\n",
      "Iteration: 1391  Loss: 0.61950  Accuracy: 0.660\n",
      "Iteration: 1392  Loss: 0.61948  Accuracy: 0.660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1393  Loss: 0.61947  Accuracy: 0.660\n",
      "Iteration: 1394  Loss: 0.61946  Accuracy: 0.660\n",
      "Iteration: 1395  Loss: 0.61945  Accuracy: 0.660\n",
      "Iteration: 1396  Loss: 0.61945  Accuracy: 0.660\n",
      "Iteration: 1397  Loss: 0.61945  Accuracy: 0.660\n",
      "Iteration: 1398  Loss: 0.61945  Accuracy: 0.660\n",
      "Iteration: 1399  Loss: 0.61941  Accuracy: 0.660\n",
      "Iteration: 1400  Loss: 0.61939  Accuracy: 0.660\n",
      "Iteration: 1401  Loss: 0.61937  Accuracy: 0.660\n",
      "Iteration: 1402  Loss: 0.61935  Accuracy: 0.660\n",
      "Iteration: 1403  Loss: 0.61933  Accuracy: 0.660\n",
      "Iteration: 1404  Loss: 0.61933  Accuracy: 0.660\n",
      "Iteration: 1405  Loss: 0.61932  Accuracy: 0.660\n",
      "Iteration: 1406  Loss: 0.61930  Accuracy: 0.660\n",
      "Iteration: 1407  Loss: 0.61929  Accuracy: 0.660\n",
      "Iteration: 1408  Loss: 0.61929  Accuracy: 0.660\n",
      "Iteration: 1409  Loss: 0.61929  Accuracy: 0.660\n",
      "Iteration: 1410  Loss: 0.61926  Accuracy: 0.660\n",
      "Iteration: 1411  Loss: 0.61925  Accuracy: 0.660\n",
      "Iteration: 1412  Loss: 0.61925  Accuracy: 0.660\n",
      "Iteration: 1413  Loss: 0.61925  Accuracy: 0.660\n",
      "Iteration: 1414  Loss: 0.61922  Accuracy: 0.660\n",
      "Iteration: 1415  Loss: 0.61922  Accuracy: 0.660\n",
      "Iteration: 1416  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1417  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1418  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1419  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1420  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1421  Loss: 0.61921  Accuracy: 0.660\n",
      "Iteration: 1422  Loss: 0.61920  Accuracy: 0.661\n",
      "Iteration: 1423  Loss: 0.61919  Accuracy: 0.661\n",
      "Iteration: 1424  Loss: 0.61918  Accuracy: 0.661\n",
      "Iteration: 1425  Loss: 0.61916  Accuracy: 0.661\n",
      "Iteration: 1426  Loss: 0.61915  Accuracy: 0.661\n",
      "Iteration: 1427  Loss: 0.61916  Accuracy: 0.661\n",
      "Iteration: 1428  Loss: 0.61916  Accuracy: 0.661\n",
      "Iteration: 1429  Loss: 0.61915  Accuracy: 0.661\n",
      "Iteration: 1430  Loss: 0.61914  Accuracy: 0.661\n",
      "Iteration: 1431  Loss: 0.61914  Accuracy: 0.661\n",
      "Iteration: 1432  Loss: 0.61913  Accuracy: 0.661\n",
      "Iteration: 1433  Loss: 0.61913  Accuracy: 0.661\n",
      "Iteration: 1434  Loss: 0.61912  Accuracy: 0.661\n",
      "Iteration: 1435  Loss: 0.61911  Accuracy: 0.661\n",
      "Iteration: 1436  Loss: 0.61910  Accuracy: 0.661\n",
      "Iteration: 1437  Loss: 0.61910  Accuracy: 0.661\n",
      "Iteration: 1438  Loss: 0.61909  Accuracy: 0.661\n",
      "Iteration: 1439  Loss: 0.61909  Accuracy: 0.661\n",
      "Iteration: 1440  Loss: 0.61908  Accuracy: 0.661\n",
      "Iteration: 1441  Loss: 0.61907  Accuracy: 0.661\n",
      "Iteration: 1442  Loss: 0.61906  Accuracy: 0.661\n",
      "Iteration: 1443  Loss: 0.61903  Accuracy: 0.661\n",
      "Iteration: 1444  Loss: 0.61902  Accuracy: 0.661\n",
      "Iteration: 1445  Loss: 0.61900  Accuracy: 0.661\n",
      "Iteration: 1446  Loss: 0.61896  Accuracy: 0.661\n",
      "Iteration: 1447  Loss: 0.61895  Accuracy: 0.661\n",
      "Iteration: 1448  Loss: 0.61894  Accuracy: 0.661\n",
      "Iteration: 1449  Loss: 0.61894  Accuracy: 0.661\n",
      "Iteration: 1450  Loss: 0.61893  Accuracy: 0.661\n",
      "Iteration: 1451  Loss: 0.61893  Accuracy: 0.661\n",
      "Iteration: 1452  Loss: 0.61893  Accuracy: 0.661\n",
      "Iteration: 1453  Loss: 0.61890  Accuracy: 0.661\n",
      "Iteration: 1454  Loss: 0.61889  Accuracy: 0.661\n",
      "Iteration: 1455  Loss: 0.61887  Accuracy: 0.661\n",
      "Iteration: 1456  Loss: 0.61886  Accuracy: 0.661\n",
      "Iteration: 1457  Loss: 0.61888  Accuracy: 0.661\n",
      "Iteration: 1458  Loss: 0.61887  Accuracy: 0.661\n",
      "Iteration: 1459  Loss: 0.61886  Accuracy: 0.662\n",
      "Iteration: 1460  Loss: 0.61886  Accuracy: 0.662\n",
      "Iteration: 1461  Loss: 0.61885  Accuracy: 0.662\n",
      "Iteration: 1462  Loss: 0.61885  Accuracy: 0.662\n",
      "Iteration: 1463  Loss: 0.61882  Accuracy: 0.662\n",
      "Iteration: 1464  Loss: 0.61882  Accuracy: 0.662\n",
      "Iteration: 1465  Loss: 0.61881  Accuracy: 0.662\n",
      "Iteration: 1466  Loss: 0.61879  Accuracy: 0.662\n",
      "Iteration: 1467  Loss: 0.61879  Accuracy: 0.662\n",
      "Iteration: 1468  Loss: 0.61878  Accuracy: 0.662\n",
      "Iteration: 1469  Loss: 0.61877  Accuracy: 0.662\n",
      "Iteration: 1470  Loss: 0.61875  Accuracy: 0.662\n",
      "Iteration: 1471  Loss: 0.61875  Accuracy: 0.662\n",
      "Iteration: 1472  Loss: 0.61875  Accuracy: 0.662\n",
      "Iteration: 1473  Loss: 0.61876  Accuracy: 0.662\n",
      "Iteration: 1474  Loss: 0.61876  Accuracy: 0.662\n",
      "Iteration: 1475  Loss: 0.61872  Accuracy: 0.662\n",
      "Iteration: 1476  Loss: 0.61869  Accuracy: 0.662\n",
      "Iteration: 1477  Loss: 0.61868  Accuracy: 0.662\n",
      "Iteration: 1478  Loss: 0.61868  Accuracy: 0.662\n",
      "Iteration: 1479  Loss: 0.61867  Accuracy: 0.662\n",
      "Iteration: 1480  Loss: 0.61865  Accuracy: 0.662\n",
      "Iteration: 1481  Loss: 0.61863  Accuracy: 0.662\n",
      "Iteration: 1482  Loss: 0.61863  Accuracy: 0.662\n",
      "Iteration: 1483  Loss: 0.61862  Accuracy: 0.662\n",
      "Iteration: 1484  Loss: 0.61861  Accuracy: 0.662\n",
      "Iteration: 1485  Loss: 0.61858  Accuracy: 0.662\n",
      "Iteration: 1486  Loss: 0.61856  Accuracy: 0.662\n",
      "Iteration: 1487  Loss: 0.61855  Accuracy: 0.662\n",
      "Iteration: 1488  Loss: 0.61854  Accuracy: 0.662\n",
      "Iteration: 1489  Loss: 0.61854  Accuracy: 0.662\n",
      "Iteration: 1490  Loss: 0.61852  Accuracy: 0.662\n",
      "Iteration: 1491  Loss: 0.61852  Accuracy: 0.662\n",
      "Iteration: 1492  Loss: 0.61848  Accuracy: 0.662\n",
      "Iteration: 1493  Loss: 0.61847  Accuracy: 0.662\n",
      "Iteration: 1494  Loss: 0.61846  Accuracy: 0.662\n",
      "Iteration: 1495  Loss: 0.61846  Accuracy: 0.662\n",
      "Iteration: 1496  Loss: 0.61845  Accuracy: 0.662\n",
      "Iteration: 1497  Loss: 0.61845  Accuracy: 0.662\n",
      "Iteration: 1498  Loss: 0.61847  Accuracy: 0.662\n",
      "Iteration: 1499  Loss: 0.61845  Accuracy: 0.662\n",
      "Iteration: 1500  Loss: 0.61843  Accuracy: 0.662\n",
      "Iteration: 1501  Loss: 0.61844  Accuracy: 0.662\n",
      "Iteration: 1502  Loss: 0.61842  Accuracy: 0.662\n",
      "Iteration: 1503  Loss: 0.61841  Accuracy: 0.662\n",
      "Iteration: 1504  Loss: 0.61839  Accuracy: 0.662\n",
      "Iteration: 1505  Loss: 0.61836  Accuracy: 0.662\n",
      "Iteration: 1506  Loss: 0.61834  Accuracy: 0.662\n",
      "Iteration: 1507  Loss: 0.61832  Accuracy: 0.663\n",
      "Iteration: 1508  Loss: 0.61829  Accuracy: 0.663\n",
      "Iteration: 1509  Loss: 0.61829  Accuracy: 0.663\n",
      "Iteration: 1510  Loss: 0.61827  Accuracy: 0.662\n",
      "Iteration: 1511  Loss: 0.61824  Accuracy: 0.663\n",
      "Iteration: 1512  Loss: 0.61821  Accuracy: 0.662\n",
      "Iteration: 1513  Loss: 0.61822  Accuracy: 0.662\n",
      "Iteration: 1514  Loss: 0.61821  Accuracy: 0.662\n",
      "Iteration: 1515  Loss: 0.61819  Accuracy: 0.662\n",
      "Iteration: 1516  Loss: 0.61818  Accuracy: 0.662\n",
      "Iteration: 1517  Loss: 0.61815  Accuracy: 0.662\n",
      "Iteration: 1518  Loss: 0.61814  Accuracy: 0.662\n",
      "Iteration: 1519  Loss: 0.61812  Accuracy: 0.662\n",
      "Iteration: 1520  Loss: 0.61810  Accuracy: 0.662\n",
      "Iteration: 1521  Loss: 0.61810  Accuracy: 0.662\n",
      "Iteration: 1522  Loss: 0.61809  Accuracy: 0.662\n",
      "Iteration: 1523  Loss: 0.61807  Accuracy: 0.662\n",
      "Iteration: 1524  Loss: 0.61806  Accuracy: 0.662\n",
      "Iteration: 1525  Loss: 0.61804  Accuracy: 0.662\n",
      "Iteration: 1526  Loss: 0.61802  Accuracy: 0.662\n",
      "Iteration: 1527  Loss: 0.61800  Accuracy: 0.662\n",
      "Iteration: 1528  Loss: 0.61800  Accuracy: 0.662\n",
      "Iteration: 1529  Loss: 0.61800  Accuracy: 0.662\n",
      "Iteration: 1530  Loss: 0.61800  Accuracy: 0.662\n",
      "Iteration: 1531  Loss: 0.61801  Accuracy: 0.662\n",
      "Iteration: 1532  Loss: 0.61799  Accuracy: 0.662\n",
      "Iteration: 1533  Loss: 0.61801  Accuracy: 0.662\n",
      "Iteration: 1534  Loss: 0.61799  Accuracy: 0.662\n",
      "Iteration: 1535  Loss: 0.61797  Accuracy: 0.662\n",
      "Iteration: 1536  Loss: 0.61796  Accuracy: 0.662\n",
      "Iteration: 1537  Loss: 0.61797  Accuracy: 0.662\n",
      "Iteration: 1538  Loss: 0.61798  Accuracy: 0.662\n",
      "Iteration: 1539  Loss: 0.61795  Accuracy: 0.662\n",
      "Iteration: 1540  Loss: 0.61794  Accuracy: 0.662\n",
      "Iteration: 1541  Loss: 0.61796  Accuracy: 0.662\n",
      "Iteration: 1542  Loss: 0.61795  Accuracy: 0.663\n",
      "Iteration: 1543  Loss: 0.61795  Accuracy: 0.662\n",
      "Iteration: 1544  Loss: 0.61793  Accuracy: 0.662\n",
      "Iteration: 1545  Loss: 0.61793  Accuracy: 0.662\n",
      "Iteration: 1546  Loss: 0.61791  Accuracy: 0.662\n",
      "Iteration: 1547  Loss: 0.61791  Accuracy: 0.662\n",
      "Iteration: 1548  Loss: 0.61790  Accuracy: 0.662\n",
      "Iteration: 1549  Loss: 0.61793  Accuracy: 0.662\n",
      "Iteration: 1550  Loss: 0.61793  Accuracy: 0.662\n",
      "Iteration: 1551  Loss: 0.61790  Accuracy: 0.662\n",
      "Iteration: 1552  Loss: 0.61790  Accuracy: 0.662\n",
      "Iteration: 1553  Loss: 0.61790  Accuracy: 0.662\n",
      "Iteration: 1554  Loss: 0.61789  Accuracy: 0.662\n",
      "Iteration: 1555  Loss: 0.61789  Accuracy: 0.662\n",
      "Iteration: 1556  Loss: 0.61790  Accuracy: 0.662\n",
      "Iteration: 1557  Loss: 0.61788  Accuracy: 0.662\n",
      "Iteration: 1558  Loss: 0.61787  Accuracy: 0.662\n",
      "Iteration: 1559  Loss: 0.61786  Accuracy: 0.662\n",
      "Iteration: 1560  Loss: 0.61785  Accuracy: 0.662\n",
      "Iteration: 1561  Loss: 0.61785  Accuracy: 0.662\n",
      "Iteration: 1562  Loss: 0.61784  Accuracy: 0.662\n",
      "Iteration: 1563  Loss: 0.61785  Accuracy: 0.662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1564  Loss: 0.61781  Accuracy: 0.662\n",
      "Iteration: 1565  Loss: 0.61782  Accuracy: 0.662\n",
      "Iteration: 1566  Loss: 0.61782  Accuracy: 0.662\n",
      "Iteration: 1567  Loss: 0.61781  Accuracy: 0.662\n",
      "Iteration: 1568  Loss: 0.61781  Accuracy: 0.662\n",
      "Iteration: 1569  Loss: 0.61780  Accuracy: 0.662\n",
      "Iteration: 1570  Loss: 0.61779  Accuracy: 0.662\n",
      "Iteration: 1571  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1572  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1573  Loss: 0.61777  Accuracy: 0.662\n",
      "Iteration: 1574  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1575  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1576  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1577  Loss: 0.61779  Accuracy: 0.662\n",
      "Iteration: 1578  Loss: 0.61779  Accuracy: 0.662\n",
      "Iteration: 1579  Loss: 0.61779  Accuracy: 0.662\n",
      "Iteration: 1580  Loss: 0.61778  Accuracy: 0.662\n",
      "Iteration: 1581  Loss: 0.61775  Accuracy: 0.662\n",
      "Iteration: 1582  Loss: 0.61775  Accuracy: 0.662\n",
      "Iteration: 1583  Loss: 0.61774  Accuracy: 0.662\n",
      "Iteration: 1584  Loss: 0.61773  Accuracy: 0.662\n",
      "Iteration: 1585  Loss: 0.61772  Accuracy: 0.662\n",
      "Iteration: 1586  Loss: 0.61771  Accuracy: 0.662\n",
      "Iteration: 1587  Loss: 0.61771  Accuracy: 0.662\n",
      "Iteration: 1588  Loss: 0.61771  Accuracy: 0.662\n",
      "Iteration: 1589  Loss: 0.61770  Accuracy: 0.662\n",
      "Iteration: 1590  Loss: 0.61770  Accuracy: 0.662\n",
      "Iteration: 1591  Loss: 0.61770  Accuracy: 0.663\n",
      "Iteration: 1592  Loss: 0.61769  Accuracy: 0.663\n",
      "Iteration: 1593  Loss: 0.61768  Accuracy: 0.663\n",
      "Iteration: 1594  Loss: 0.61768  Accuracy: 0.663\n",
      "Iteration: 1595  Loss: 0.61766  Accuracy: 0.663\n",
      "Iteration: 1596  Loss: 0.61764  Accuracy: 0.663\n",
      "Iteration: 1597  Loss: 0.61765  Accuracy: 0.663\n",
      "Iteration: 1598  Loss: 0.61763  Accuracy: 0.663\n",
      "Iteration: 1599  Loss: 0.61763  Accuracy: 0.663\n",
      "Iteration: 1600  Loss: 0.61763  Accuracy: 0.662\n",
      "Iteration: 1601  Loss: 0.61762  Accuracy: 0.663\n",
      "Iteration: 1602  Loss: 0.61759  Accuracy: 0.663\n",
      "Iteration: 1603  Loss: 0.61759  Accuracy: 0.663\n",
      "Iteration: 1604  Loss: 0.61760  Accuracy: 0.663\n",
      "Iteration: 1605  Loss: 0.61758  Accuracy: 0.663\n",
      "Iteration: 1606  Loss: 0.61757  Accuracy: 0.663\n",
      "Iteration: 1607  Loss: 0.61757  Accuracy: 0.663\n",
      "Iteration: 1608  Loss: 0.61756  Accuracy: 0.663\n",
      "Iteration: 1609  Loss: 0.61755  Accuracy: 0.663\n",
      "Iteration: 1610  Loss: 0.61754  Accuracy: 0.663\n",
      "Iteration: 1611  Loss: 0.61753  Accuracy: 0.663\n",
      "Iteration: 1612  Loss: 0.61751  Accuracy: 0.663\n",
      "Iteration: 1613  Loss: 0.61750  Accuracy: 0.663\n",
      "Iteration: 1614  Loss: 0.61748  Accuracy: 0.663\n",
      "Iteration: 1615  Loss: 0.61748  Accuracy: 0.663\n",
      "Iteration: 1616  Loss: 0.61748  Accuracy: 0.663\n",
      "Iteration: 1617  Loss: 0.61746  Accuracy: 0.663\n",
      "Iteration: 1618  Loss: 0.61744  Accuracy: 0.663\n",
      "Iteration: 1619  Loss: 0.61744  Accuracy: 0.663\n",
      "Iteration: 1620  Loss: 0.61744  Accuracy: 0.663\n",
      "Iteration: 1621  Loss: 0.61744  Accuracy: 0.663\n",
      "Iteration: 1622  Loss: 0.61744  Accuracy: 0.663\n",
      "Iteration: 1623  Loss: 0.61745  Accuracy: 0.663\n",
      "Iteration: 1624  Loss: 0.61746  Accuracy: 0.663\n",
      "Iteration: 1625  Loss: 0.61745  Accuracy: 0.663\n",
      "Iteration: 1626  Loss: 0.61745  Accuracy: 0.663\n",
      "Iteration: 1627  Loss: 0.61743  Accuracy: 0.663\n",
      "Iteration: 1628  Loss: 0.61742  Accuracy: 0.663\n",
      "Iteration: 1629  Loss: 0.61742  Accuracy: 0.663\n",
      "Iteration: 1630  Loss: 0.61743  Accuracy: 0.663\n",
      "Iteration: 1631  Loss: 0.61742  Accuracy: 0.663\n",
      "Iteration: 1632  Loss: 0.61740  Accuracy: 0.663\n",
      "Iteration: 1633  Loss: 0.61739  Accuracy: 0.663\n",
      "Iteration: 1634  Loss: 0.61738  Accuracy: 0.663\n",
      "Iteration: 1635  Loss: 0.61737  Accuracy: 0.663\n",
      "Iteration: 1636  Loss: 0.61737  Accuracy: 0.663\n",
      "Iteration: 1637  Loss: 0.61736  Accuracy: 0.663\n",
      "Iteration: 1638  Loss: 0.61735  Accuracy: 0.663\n",
      "Iteration: 1639  Loss: 0.61737  Accuracy: 0.663\n",
      "Iteration: 1640  Loss: 0.61736  Accuracy: 0.663\n",
      "Iteration: 1641  Loss: 0.61735  Accuracy: 0.663\n",
      "Iteration: 1642  Loss: 0.61734  Accuracy: 0.663\n",
      "Iteration: 1643  Loss: 0.61734  Accuracy: 0.663\n",
      "Iteration: 1644  Loss: 0.61733  Accuracy: 0.663\n",
      "Iteration: 1645  Loss: 0.61732  Accuracy: 0.663\n",
      "Iteration: 1646  Loss: 0.61731  Accuracy: 0.663\n",
      "Iteration: 1647  Loss: 0.61730  Accuracy: 0.663\n",
      "Iteration: 1648  Loss: 0.61729  Accuracy: 0.663\n",
      "Iteration: 1649  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1650  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1651  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1652  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1653  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1654  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1655  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1656  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1657  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1658  Loss: 0.61726  Accuracy: 0.663\n",
      "Iteration: 1659  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1660  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1661  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1662  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1663  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1664  Loss: 0.61728  Accuracy: 0.663\n",
      "Iteration: 1665  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1666  Loss: 0.61727  Accuracy: 0.663\n",
      "Iteration: 1667  Loss: 0.61726  Accuracy: 0.663\n",
      "Iteration: 1668  Loss: 0.61725  Accuracy: 0.663\n",
      "Iteration: 1669  Loss: 0.61724  Accuracy: 0.663\n",
      "Iteration: 1670  Loss: 0.61725  Accuracy: 0.663\n",
      "Iteration: 1671  Loss: 0.61725  Accuracy: 0.663\n",
      "Iteration: 1672  Loss: 0.61724  Accuracy: 0.663\n",
      "Iteration: 1673  Loss: 0.61725  Accuracy: 0.663\n",
      "Iteration: 1674  Loss: 0.61723  Accuracy: 0.663\n",
      "Iteration: 1675  Loss: 0.61723  Accuracy: 0.663\n",
      "Iteration: 1676  Loss: 0.61723  Accuracy: 0.663\n",
      "Iteration: 1677  Loss: 0.61722  Accuracy: 0.663\n",
      "Iteration: 1678  Loss: 0.61720  Accuracy: 0.663\n",
      "Iteration: 1679  Loss: 0.61719  Accuracy: 0.663\n",
      "Iteration: 1680  Loss: 0.61718  Accuracy: 0.663\n",
      "Iteration: 1681  Loss: 0.61716  Accuracy: 0.663\n",
      "Iteration: 1682  Loss: 0.61715  Accuracy: 0.663\n",
      "Iteration: 1683  Loss: 0.61713  Accuracy: 0.663\n",
      "Iteration: 1684  Loss: 0.61712  Accuracy: 0.663\n",
      "Iteration: 1685  Loss: 0.61711  Accuracy: 0.663\n",
      "Iteration: 1686  Loss: 0.61711  Accuracy: 0.663\n",
      "Iteration: 1687  Loss: 0.61712  Accuracy: 0.663\n",
      "Iteration: 1688  Loss: 0.61712  Accuracy: 0.663\n",
      "Iteration: 1689  Loss: 0.61713  Accuracy: 0.663\n",
      "Iteration: 1690  Loss: 0.61710  Accuracy: 0.663\n",
      "Iteration: 1691  Loss: 0.61709  Accuracy: 0.663\n",
      "Iteration: 1692  Loss: 0.61707  Accuracy: 0.663\n",
      "Iteration: 1693  Loss: 0.61707  Accuracy: 0.663\n",
      "Iteration: 1694  Loss: 0.61708  Accuracy: 0.663\n",
      "Iteration: 1695  Loss: 0.61707  Accuracy: 0.663\n",
      "Iteration: 1696  Loss: 0.61705  Accuracy: 0.664\n",
      "Iteration: 1697  Loss: 0.61706  Accuracy: 0.664\n",
      "Iteration: 1698  Loss: 0.61706  Accuracy: 0.663\n",
      "Iteration: 1699  Loss: 0.61705  Accuracy: 0.663\n",
      "Iteration: 1700  Loss: 0.61704  Accuracy: 0.663\n",
      "Iteration: 1701  Loss: 0.61700  Accuracy: 0.664\n",
      "Iteration: 1702  Loss: 0.61699  Accuracy: 0.664\n",
      "Iteration: 1703  Loss: 0.61700  Accuracy: 0.664\n",
      "Iteration: 1704  Loss: 0.61701  Accuracy: 0.664\n",
      "Iteration: 1705  Loss: 0.61700  Accuracy: 0.664\n",
      "Iteration: 1706  Loss: 0.61699  Accuracy: 0.664\n",
      "Iteration: 1707  Loss: 0.61697  Accuracy: 0.664\n",
      "Iteration: 1708  Loss: 0.61698  Accuracy: 0.664\n",
      "Iteration: 1709  Loss: 0.61698  Accuracy: 0.664\n",
      "Iteration: 1710  Loss: 0.61698  Accuracy: 0.664\n",
      "Iteration: 1711  Loss: 0.61698  Accuracy: 0.664\n",
      "Iteration: 1712  Loss: 0.61696  Accuracy: 0.664\n",
      "Iteration: 1713  Loss: 0.61695  Accuracy: 0.664\n",
      "Iteration: 1714  Loss: 0.61693  Accuracy: 0.664\n",
      "Iteration: 1715  Loss: 0.61695  Accuracy: 0.664\n",
      "Iteration: 1716  Loss: 0.61695  Accuracy: 0.664\n",
      "Iteration: 1717  Loss: 0.61694  Accuracy: 0.664\n",
      "Iteration: 1718  Loss: 0.61693  Accuracy: 0.664\n",
      "Iteration: 1719  Loss: 0.61692  Accuracy: 0.664\n",
      "Iteration: 1720  Loss: 0.61692  Accuracy: 0.664\n",
      "Iteration: 1721  Loss: 0.61692  Accuracy: 0.664\n",
      "Iteration: 1722  Loss: 0.61689  Accuracy: 0.664\n",
      "Iteration: 1723  Loss: 0.61690  Accuracy: 0.664\n",
      "Iteration: 1724  Loss: 0.61686  Accuracy: 0.664\n",
      "Iteration: 1725  Loss: 0.61686  Accuracy: 0.664\n",
      "Iteration: 1726  Loss: 0.61682  Accuracy: 0.664\n",
      "Iteration: 1727  Loss: 0.61679  Accuracy: 0.664\n",
      "Iteration: 1728  Loss: 0.61678  Accuracy: 0.664\n",
      "Iteration: 1729  Loss: 0.61680  Accuracy: 0.664\n",
      "Iteration: 1730  Loss: 0.61678  Accuracy: 0.664\n",
      "Iteration: 1731  Loss: 0.61677  Accuracy: 0.664\n",
      "Iteration: 1732  Loss: 0.61677  Accuracy: 0.664\n",
      "Iteration: 1733  Loss: 0.61677  Accuracy: 0.664\n",
      "Iteration: 1734  Loss: 0.61676  Accuracy: 0.664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1735  Loss: 0.61675  Accuracy: 0.664\n",
      "Iteration: 1736  Loss: 0.61674  Accuracy: 0.664\n",
      "Iteration: 1737  Loss: 0.61675  Accuracy: 0.664\n",
      "Iteration: 1738  Loss: 0.61673  Accuracy: 0.664\n",
      "Iteration: 1739  Loss: 0.61673  Accuracy: 0.664\n",
      "Iteration: 1740  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1741  Loss: 0.61670  Accuracy: 0.664\n",
      "Iteration: 1742  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1743  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1744  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1745  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1746  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1747  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1748  Loss: 0.61670  Accuracy: 0.664\n",
      "Iteration: 1749  Loss: 0.61669  Accuracy: 0.664\n",
      "Iteration: 1750  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1751  Loss: 0.61670  Accuracy: 0.664\n",
      "Iteration: 1752  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1753  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1754  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1755  Loss: 0.61671  Accuracy: 0.664\n",
      "Iteration: 1756  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1757  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1758  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1759  Loss: 0.61672  Accuracy: 0.664\n",
      "Iteration: 1760  Loss: 0.61670  Accuracy: 0.664\n",
      "Iteration: 1761  Loss: 0.61668  Accuracy: 0.664\n",
      "Iteration: 1762  Loss: 0.61668  Accuracy: 0.664\n",
      "Iteration: 1763  Loss: 0.61666  Accuracy: 0.664\n",
      "Iteration: 1764  Loss: 0.61664  Accuracy: 0.664\n",
      "Iteration: 1765  Loss: 0.61663  Accuracy: 0.664\n",
      "Iteration: 1766  Loss: 0.61662  Accuracy: 0.664\n",
      "Iteration: 1767  Loss: 0.61662  Accuracy: 0.664\n",
      "Iteration: 1768  Loss: 0.61661  Accuracy: 0.664\n",
      "Iteration: 1769  Loss: 0.61662  Accuracy: 0.664\n",
      "Iteration: 1770  Loss: 0.61661  Accuracy: 0.664\n",
      "Iteration: 1771  Loss: 0.61660  Accuracy: 0.664\n",
      "Iteration: 1772  Loss: 0.61659  Accuracy: 0.664\n",
      "Iteration: 1773  Loss: 0.61659  Accuracy: 0.665\n",
      "Iteration: 1774  Loss: 0.61658  Accuracy: 0.665\n",
      "Iteration: 1775  Loss: 0.61656  Accuracy: 0.664\n",
      "Iteration: 1776  Loss: 0.61655  Accuracy: 0.664\n",
      "Iteration: 1777  Loss: 0.61657  Accuracy: 0.665\n",
      "Iteration: 1778  Loss: 0.61658  Accuracy: 0.665\n",
      "Iteration: 1779  Loss: 0.61657  Accuracy: 0.665\n",
      "Iteration: 1780  Loss: 0.61656  Accuracy: 0.665\n",
      "Iteration: 1781  Loss: 0.61655  Accuracy: 0.665\n",
      "Iteration: 1782  Loss: 0.61655  Accuracy: 0.665\n",
      "Iteration: 1783  Loss: 0.61655  Accuracy: 0.664\n",
      "Iteration: 1784  Loss: 0.61655  Accuracy: 0.664\n",
      "Iteration: 1785  Loss: 0.61653  Accuracy: 0.665\n",
      "Iteration: 1786  Loss: 0.61653  Accuracy: 0.664\n",
      "Iteration: 1787  Loss: 0.61651  Accuracy: 0.664\n",
      "Iteration: 1788  Loss: 0.61650  Accuracy: 0.664\n",
      "Iteration: 1789  Loss: 0.61650  Accuracy: 0.664\n",
      "Iteration: 1790  Loss: 0.61646  Accuracy: 0.664\n",
      "Iteration: 1791  Loss: 0.61647  Accuracy: 0.664\n",
      "Iteration: 1792  Loss: 0.61646  Accuracy: 0.664\n",
      "Iteration: 1793  Loss: 0.61644  Accuracy: 0.664\n",
      "Iteration: 1794  Loss: 0.61644  Accuracy: 0.664\n",
      "Iteration: 1795  Loss: 0.61643  Accuracy: 0.664\n",
      "Iteration: 1796  Loss: 0.61642  Accuracy: 0.664\n",
      "Iteration: 1797  Loss: 0.61640  Accuracy: 0.664\n",
      "Iteration: 1798  Loss: 0.61639  Accuracy: 0.664\n",
      "Iteration: 1799  Loss: 0.61638  Accuracy: 0.664\n",
      "Iteration: 1800  Loss: 0.61637  Accuracy: 0.664\n",
      "Iteration: 1801  Loss: 0.61637  Accuracy: 0.664\n",
      "Iteration: 1802  Loss: 0.61636  Accuracy: 0.664\n",
      "Iteration: 1803  Loss: 0.61637  Accuracy: 0.664\n",
      "Iteration: 1804  Loss: 0.61636  Accuracy: 0.664\n",
      "Iteration: 1805  Loss: 0.61635  Accuracy: 0.664\n",
      "Iteration: 1806  Loss: 0.61634  Accuracy: 0.664\n",
      "Iteration: 1807  Loss: 0.61634  Accuracy: 0.664\n",
      "Iteration: 1808  Loss: 0.61634  Accuracy: 0.664\n",
      "Iteration: 1809  Loss: 0.61633  Accuracy: 0.664\n",
      "Iteration: 1810  Loss: 0.61631  Accuracy: 0.664\n",
      "Iteration: 1811  Loss: 0.61632  Accuracy: 0.664\n",
      "Iteration: 1812  Loss: 0.61631  Accuracy: 0.664\n",
      "Iteration: 1813  Loss: 0.61628  Accuracy: 0.664\n",
      "Iteration: 1814  Loss: 0.61629  Accuracy: 0.664\n",
      "Iteration: 1815  Loss: 0.61629  Accuracy: 0.664\n",
      "Iteration: 1816  Loss: 0.61629  Accuracy: 0.664\n",
      "Iteration: 1817  Loss: 0.61631  Accuracy: 0.664\n",
      "Iteration: 1818  Loss: 0.61628  Accuracy: 0.664\n",
      "Iteration: 1819  Loss: 0.61625  Accuracy: 0.664\n",
      "Iteration: 1820  Loss: 0.61625  Accuracy: 0.664\n",
      "Iteration: 1821  Loss: 0.61625  Accuracy: 0.664\n",
      "Iteration: 1822  Loss: 0.61623  Accuracy: 0.664\n",
      "Iteration: 1823  Loss: 0.61623  Accuracy: 0.664\n",
      "Iteration: 1824  Loss: 0.61623  Accuracy: 0.664\n",
      "Iteration: 1825  Loss: 0.61621  Accuracy: 0.664\n",
      "Iteration: 1826  Loss: 0.61620  Accuracy: 0.664\n",
      "Iteration: 1827  Loss: 0.61620  Accuracy: 0.664\n",
      "Iteration: 1828  Loss: 0.61619  Accuracy: 0.664\n",
      "Iteration: 1829  Loss: 0.61619  Accuracy: 0.664\n",
      "Iteration: 1830  Loss: 0.61618  Accuracy: 0.664\n",
      "Iteration: 1831  Loss: 0.61617  Accuracy: 0.664\n",
      "Iteration: 1832  Loss: 0.61618  Accuracy: 0.664\n",
      "Iteration: 1833  Loss: 0.61617  Accuracy: 0.664\n",
      "Iteration: 1834  Loss: 0.61616  Accuracy: 0.664\n",
      "Iteration: 1835  Loss: 0.61616  Accuracy: 0.664\n",
      "Iteration: 1836  Loss: 0.61615  Accuracy: 0.664\n",
      "Iteration: 1837  Loss: 0.61615  Accuracy: 0.665\n",
      "Iteration: 1838  Loss: 0.61614  Accuracy: 0.665\n",
      "Iteration: 1839  Loss: 0.61613  Accuracy: 0.665\n",
      "Iteration: 1840  Loss: 0.61613  Accuracy: 0.665\n",
      "Iteration: 1841  Loss: 0.61611  Accuracy: 0.665\n",
      "Iteration: 1842  Loss: 0.61611  Accuracy: 0.665\n",
      "Iteration: 1843  Loss: 0.61610  Accuracy: 0.665\n",
      "Iteration: 1844  Loss: 0.61608  Accuracy: 0.665\n",
      "Iteration: 1845  Loss: 0.61608  Accuracy: 0.665\n",
      "Iteration: 1846  Loss: 0.61608  Accuracy: 0.665\n",
      "Iteration: 1847  Loss: 0.61607  Accuracy: 0.665\n",
      "Iteration: 1848  Loss: 0.61605  Accuracy: 0.665\n",
      "Iteration: 1849  Loss: 0.61606  Accuracy: 0.665\n",
      "Iteration: 1850  Loss: 0.61606  Accuracy: 0.665\n",
      "Iteration: 1851  Loss: 0.61605  Accuracy: 0.665\n",
      "Iteration: 1852  Loss: 0.61604  Accuracy: 0.665\n",
      "Iteration: 1853  Loss: 0.61605  Accuracy: 0.665\n",
      "Iteration: 1854  Loss: 0.61603  Accuracy: 0.665\n",
      "Iteration: 1855  Loss: 0.61602  Accuracy: 0.665\n",
      "Iteration: 1856  Loss: 0.61602  Accuracy: 0.665\n",
      "Iteration: 1857  Loss: 0.61604  Accuracy: 0.665\n",
      "Iteration: 1858  Loss: 0.61601  Accuracy: 0.665\n",
      "Iteration: 1859  Loss: 0.61602  Accuracy: 0.665\n",
      "Iteration: 1860  Loss: 0.61601  Accuracy: 0.665\n",
      "Iteration: 1861  Loss: 0.61603  Accuracy: 0.665\n",
      "Iteration: 1862  Loss: 0.61601  Accuracy: 0.665\n",
      "Iteration: 1863  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1864  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1865  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1866  Loss: 0.61600  Accuracy: 0.665\n",
      "Iteration: 1867  Loss: 0.61598  Accuracy: 0.665\n",
      "Iteration: 1868  Loss: 0.61598  Accuracy: 0.665\n",
      "Iteration: 1869  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1870  Loss: 0.61600  Accuracy: 0.665\n",
      "Iteration: 1871  Loss: 0.61598  Accuracy: 0.665\n",
      "Iteration: 1872  Loss: 0.61598  Accuracy: 0.665\n",
      "Iteration: 1873  Loss: 0.61597  Accuracy: 0.665\n",
      "Iteration: 1874  Loss: 0.61597  Accuracy: 0.665\n",
      "Iteration: 1875  Loss: 0.61597  Accuracy: 0.665\n",
      "Iteration: 1876  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1877  Loss: 0.61599  Accuracy: 0.665\n",
      "Iteration: 1878  Loss: 0.61597  Accuracy: 0.665\n",
      "Iteration: 1879  Loss: 0.61596  Accuracy: 0.665\n",
      "Iteration: 1880  Loss: 0.61594  Accuracy: 0.665\n",
      "Iteration: 1881  Loss: 0.61594  Accuracy: 0.665\n",
      "Iteration: 1882  Loss: 0.61595  Accuracy: 0.665\n",
      "Iteration: 1883  Loss: 0.61596  Accuracy: 0.665\n",
      "Iteration: 1884  Loss: 0.61595  Accuracy: 0.665\n",
      "Iteration: 1885  Loss: 0.61594  Accuracy: 0.665\n",
      "Iteration: 1886  Loss: 0.61593  Accuracy: 0.665\n",
      "Iteration: 1887  Loss: 0.61592  Accuracy: 0.665\n",
      "Iteration: 1888  Loss: 0.61592  Accuracy: 0.665\n",
      "Iteration: 1889  Loss: 0.61591  Accuracy: 0.665\n",
      "Iteration: 1890  Loss: 0.61590  Accuracy: 0.665\n",
      "Iteration: 1891  Loss: 0.61590  Accuracy: 0.665\n",
      "Iteration: 1892  Loss: 0.61589  Accuracy: 0.665\n",
      "Iteration: 1893  Loss: 0.61588  Accuracy: 0.665\n",
      "Iteration: 1894  Loss: 0.61588  Accuracy: 0.665\n",
      "Iteration: 1895  Loss: 0.61588  Accuracy: 0.665\n",
      "Iteration: 1896  Loss: 0.61586  Accuracy: 0.665\n",
      "Iteration: 1897  Loss: 0.61584  Accuracy: 0.665\n",
      "Iteration: 1898  Loss: 0.61581  Accuracy: 0.665\n",
      "Iteration: 1899  Loss: 0.61580  Accuracy: 0.665\n",
      "Iteration: 1900  Loss: 0.61580  Accuracy: 0.665\n",
      "Iteration: 1901  Loss: 0.61580  Accuracy: 0.665\n",
      "Iteration: 1902  Loss: 0.61579  Accuracy: 0.665\n",
      "Iteration: 1903  Loss: 0.61580  Accuracy: 0.665\n",
      "Iteration: 1904  Loss: 0.61580  Accuracy: 0.665\n",
      "Iteration: 1905  Loss: 0.61579  Accuracy: 0.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1906  Loss: 0.61579  Accuracy: 0.665\n",
      "Iteration: 1907  Loss: 0.61577  Accuracy: 0.665\n",
      "Iteration: 1908  Loss: 0.61577  Accuracy: 0.665\n",
      "Iteration: 1909  Loss: 0.61576  Accuracy: 0.665\n",
      "Iteration: 1910  Loss: 0.61578  Accuracy: 0.665\n",
      "Iteration: 1911  Loss: 0.61577  Accuracy: 0.665\n",
      "Iteration: 1912  Loss: 0.61576  Accuracy: 0.665\n",
      "Iteration: 1913  Loss: 0.61575  Accuracy: 0.666\n",
      "Iteration: 1914  Loss: 0.61576  Accuracy: 0.665\n",
      "Iteration: 1915  Loss: 0.61576  Accuracy: 0.666\n",
      "Iteration: 1916  Loss: 0.61575  Accuracy: 0.666\n",
      "Iteration: 1917  Loss: 0.61575  Accuracy: 0.666\n",
      "Iteration: 1918  Loss: 0.61573  Accuracy: 0.666\n",
      "Iteration: 1919  Loss: 0.61572  Accuracy: 0.666\n",
      "Iteration: 1920  Loss: 0.61572  Accuracy: 0.666\n",
      "Iteration: 1921  Loss: 0.61570  Accuracy: 0.666\n",
      "Iteration: 1922  Loss: 0.61570  Accuracy: 0.666\n",
      "Iteration: 1923  Loss: 0.61570  Accuracy: 0.666\n",
      "Iteration: 1924  Loss: 0.61569  Accuracy: 0.666\n",
      "Iteration: 1925  Loss: 0.61568  Accuracy: 0.666\n",
      "Iteration: 1926  Loss: 0.61568  Accuracy: 0.666\n",
      "Iteration: 1927  Loss: 0.61568  Accuracy: 0.666\n",
      "Iteration: 1928  Loss: 0.61567  Accuracy: 0.666\n",
      "Iteration: 1929  Loss: 0.61567  Accuracy: 0.666\n",
      "Iteration: 1930  Loss: 0.61567  Accuracy: 0.666\n",
      "Iteration: 1931  Loss: 0.61565  Accuracy: 0.666\n",
      "Iteration: 1932  Loss: 0.61564  Accuracy: 0.666\n",
      "Iteration: 1933  Loss: 0.61565  Accuracy: 0.666\n",
      "Iteration: 1934  Loss: 0.61564  Accuracy: 0.666\n",
      "Iteration: 1935  Loss: 0.61564  Accuracy: 0.666\n",
      "Iteration: 1936  Loss: 0.61563  Accuracy: 0.666\n",
      "Iteration: 1937  Loss: 0.61561  Accuracy: 0.666\n",
      "Iteration: 1938  Loss: 0.61562  Accuracy: 0.666\n",
      "Iteration: 1939  Loss: 0.61561  Accuracy: 0.666\n",
      "Iteration: 1940  Loss: 0.61561  Accuracy: 0.666\n",
      "Iteration: 1941  Loss: 0.61558  Accuracy: 0.666\n",
      "Iteration: 1942  Loss: 0.61558  Accuracy: 0.666\n",
      "Iteration: 1943  Loss: 0.61556  Accuracy: 0.666\n",
      "Iteration: 1944  Loss: 0.61555  Accuracy: 0.666\n",
      "Iteration: 1945  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1946  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1947  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1948  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1949  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1950  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1951  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1952  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1953  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1954  Loss: 0.61555  Accuracy: 0.666\n",
      "Iteration: 1955  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1956  Loss: 0.61556  Accuracy: 0.666\n",
      "Iteration: 1957  Loss: 0.61557  Accuracy: 0.666\n",
      "Iteration: 1958  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1959  Loss: 0.61554  Accuracy: 0.666\n",
      "Iteration: 1960  Loss: 0.61555  Accuracy: 0.666\n",
      "Iteration: 1961  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1962  Loss: 0.61553  Accuracy: 0.666\n",
      "Iteration: 1963  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1964  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1965  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1966  Loss: 0.61549  Accuracy: 0.666\n",
      "Iteration: 1967  Loss: 0.61549  Accuracy: 0.666\n",
      "Iteration: 1968  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1969  Loss: 0.61551  Accuracy: 0.666\n",
      "Iteration: 1970  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1971  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1972  Loss: 0.61550  Accuracy: 0.666\n",
      "Iteration: 1973  Loss: 0.61548  Accuracy: 0.666\n",
      "Iteration: 1974  Loss: 0.61548  Accuracy: 0.666\n",
      "Iteration: 1975  Loss: 0.61547  Accuracy: 0.666\n",
      "Iteration: 1976  Loss: 0.61547  Accuracy: 0.666\n",
      "Iteration: 1977  Loss: 0.61545  Accuracy: 0.666\n",
      "Iteration: 1978  Loss: 0.61543  Accuracy: 0.666\n",
      "Iteration: 1979  Loss: 0.61542  Accuracy: 0.666\n",
      "Iteration: 1980  Loss: 0.61541  Accuracy: 0.666\n",
      "Iteration: 1981  Loss: 0.61542  Accuracy: 0.666\n",
      "Iteration: 1982  Loss: 0.61541  Accuracy: 0.666\n",
      "Iteration: 1983  Loss: 0.61540  Accuracy: 0.666\n",
      "Iteration: 1984  Loss: 0.61539  Accuracy: 0.666\n",
      "Iteration: 1985  Loss: 0.61538  Accuracy: 0.666\n",
      "Iteration: 1986  Loss: 0.61537  Accuracy: 0.666\n",
      "Iteration: 1987  Loss: 0.61536  Accuracy: 0.666\n",
      "Iteration: 1988  Loss: 0.61537  Accuracy: 0.666\n",
      "Iteration: 1989  Loss: 0.61537  Accuracy: 0.666\n",
      "Iteration: 1990  Loss: 0.61537  Accuracy: 0.666\n",
      "Iteration: 1991  Loss: 0.61536  Accuracy: 0.666\n",
      "Iteration: 1992  Loss: 0.61535  Accuracy: 0.666\n",
      "Iteration: 1993  Loss: 0.61535  Accuracy: 0.666\n",
      "Iteration: 1994  Loss: 0.61533  Accuracy: 0.666\n",
      "Iteration: 1995  Loss: 0.61531  Accuracy: 0.666\n",
      "Iteration: 1996  Loss: 0.61532  Accuracy: 0.666\n",
      "Iteration: 1997  Loss: 0.61530  Accuracy: 0.666\n",
      "Iteration: 1998  Loss: 0.61526  Accuracy: 0.666\n",
      "Iteration: 1999  Loss: 0.61525  Accuracy: 0.666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmb_stopper = EarlyStopper(features[\"test\"], test.funny.values.astype(np.int64), patience = 25, verbose = 1)\n",
    "gbm = GradientBoostingClassifier(n_estimators=2000, max_depth = 4, learning_rate=0.1)\n",
    "gbm.fit(features[\"train\"], train.funny.values.astype(np.int64), monitor = gmb_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
