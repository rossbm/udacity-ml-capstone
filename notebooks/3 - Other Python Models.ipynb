{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import luigi\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.features.dtm import CreateDTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wertu\\\\Documents\\\\Datascience\\\\udacity-ml-capstone\\\\notebooks\\\\..'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if CreateDTM() is complete\n",
      "INFO: Informed scheduler that task   CreateDTM__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=140461088, workers=1, host=DESKTOP-6UJS098, username=wertu, pid=2468) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 present dependencies were encountered:\n",
      "    - 1 CreateDTM()\n",
      "\n",
      "Did not run any tasks\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([CreateDTM()], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = joblib.load(\"data/processed/dtm_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = joblib.load(\"data/interim/train.pkl\")\n",
    "test = joblib.load(\"data/interim/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 50, max_depth = 500, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=500, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=5,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(features[\"train\"], train.funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99220151688142233"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(features[\"train\"], train.funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67140002854288572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(features[\"test\"], test.funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.57385008e-03,   1.52317068e-04,   4.77038963e-04, ...,\n",
       "         3.73054524e-07,   1.89129137e-07,   6.49569501e-07])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = np.argpartition(check, 100)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1821,   1812,   1751,   1733,   1690,   1687,   1653, 145591,\n",
       "         1597, 145593,   1594,   1540, 145596,   1537,   1492, 145599,\n",
       "         1462,   1457,   1436,   1434,   1420,   1409,   1395,   1384,\n",
       "       145608,   1381,   1377,   1282,   1242,   1227,   1201,   1200,\n",
       "         1161,   1160,   1158, 145619,   1134,   1120,   1110,   1101,\n",
       "         1077, 145625,   1076,   1072,   1064,   1055,   1053,    992,\n",
       "          970,    967,    949,    925,    857,    843,    797,    796,\n",
       "          768,    765,    760,    742,    723,    722,    714,    713,\n",
       "          681,    648,    647,    635,    633,    626,    620,    557,\n",
       "          543,    505, 145658,    483,    430,    405,    396,    389,\n",
       "          324,    312,    230,    217,    204, 145669,    201,    197,\n",
       "          196,    172,    143,     89,     87,     78,      6,      5,\n",
       "            4,      3,      2,      0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'T\", \"'Sure\", \"'So\", \"'Sir\", \"'See\", \"'Second\", \"'S\", 'üòÄ', \"'RE\",\n",
       "       'üòÅ', \"'R\", \"'Please\", 'üòÇ', \"'Plagiarism\", \"'P\", 'üòÇüòÇ', \"'Okay\",\n",
       "       \"'Oh\", \"'Now\", \"'Not\", \"'No\", \"'Netflix\", \"'NSFW\", \"'N\", 'üòÉ', \"'My\",\n",
       "       \"'Murica\", \"'M\", \"'Lee\", \"'LL\", \"'K\", \"'Just\", \"'Its\", \"'It\", \"'Is\",\n",
       "       'üòâ', \"'If\", \"'I\", \"'How\", \"'Honey\", \"'Hi\", 'üòé', \"'Hey\", \"'Here\",\n",
       "       \"'Hello\", \"'He\", \"'Have\", \"'Good\", \"'Get\", \"'Genius'\", \"'G\",\n",
       "       \"'Forget\", \"'Excuse\", \"'Ernie\", \"'E\", \"'Dwarf\", \"'Doctor\", \"'Do\",\n",
       "       \"'Did\", \"'Dear\", \"'Daddy\", \"'Dad\", \"'D\", \"'Cuz\", \"'Come\",\n",
       "       \"'Caution-\", \"'Cause\", \"'Can\", \"'Ca\", \"'C\", \"'But\", \"'Big\",\n",
       "       \"'Because\", \"'B\", 'üò≠', \"'Are\", \"'After\", \"'A\", \"'90s\", \"'80s\", \"'1\",\n",
       "       \"'-axis\", \"''You\", \"''What\", \"''Ugh\", 'üôÑ', \"''This\", \"''The\",\n",
       "       \"''That\", \"''Shut\", \"''Oh\", \"''I\", \"''How\", \"''He\", \"''\", \"'\", '&',\n",
       "       '%', '$', '!'],\n",
       "      dtype='<U700')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features[\"vectorizer\"].get_feature_names())[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "test.append(2)\n",
    "test.append(3)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper():\n",
    "    def __init__(self, validation_set, validation_labels, patience = 0):\n",
    "        self.validation_set = validation_set\n",
    "        self.validation_labels = validation_labels\n",
    "        self.accuracy = []\n",
    "        self.loss = [np.inf]\n",
    "        self.preds = None\n",
    "        self.probs = None\n",
    "        self.patience = patience\n",
    "        self.lowest_loss = np.inf\n",
    "        \n",
    "    def __call__(self, i, estimator, fit_stage_locals):\n",
    "        \n",
    "        if i == 0:\n",
    "            self.preds = estimator.staged_predict(self.validation_set)\n",
    "            self.probs = estimator.staged_predict_proba(self.validation_set)\n",
    "            \n",
    "        preds = next(self.preds)\n",
    "        probs = next(self.probs)\n",
    "        \n",
    "        acc = np.mean(preds == self.validation_labels )\n",
    "        loss = log_loss(self.validation_labels, probs)\n",
    "        \n",
    "        self.accuracy.append(acc)\n",
    "        self.loss.append(loss)\n",
    "        \n",
    "        print(\"Iteration: {}\".format(i))\n",
    "        print(\"Loss: {}\".format(loss))\n",
    "        print(\"Accuracy: {}\".format(acc))\n",
    "        \n",
    "        if (min(self.loss[max((i-self.patience),0):i+1]) > self.lowest_loss) & (i > self.patience):\n",
    "            return True\n",
    "        else:\n",
    "            self.lowest_loss = \n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmb_stopper = EarlyStopper(features[\"test\"], test.funny.values.astype(np.int64), patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Loss: 0.6875785372713787\n",
      "Accuracy: 0.5815256172399029\n",
      "Iteration: 1\n",
      "Loss: 0.6833037130572504\n",
      "Accuracy: 0.590124161552733\n",
      "Iteration: 2\n",
      "Loss: 0.6798110287852657\n",
      "Accuracy: 0.5942985585842728\n",
      "Iteration: 3\n",
      "Loss: 0.6769375297967754\n",
      "Accuracy: 0.59433423719138\n",
      "Iteration: 4\n",
      "Loss: 0.6742320839506454\n",
      "Accuracy: 0.5950121307264165\n",
      "Iteration: 5\n",
      "Loss: 0.6720937133634003\n",
      "Accuracy: 0.5952618809761667\n",
      "Iteration: 6\n",
      "Loss: 0.6700759942667311\n",
      "Accuracy: 0.5979020979020979\n",
      "Iteration: 7\n",
      "Loss: 0.6684481777278891\n",
      "Accuracy: 0.5996860282574569\n",
      "Iteration: 8\n",
      "Loss: 0.6670271098443951\n",
      "Accuracy: 0.6001498501498501\n",
      "Iteration: 9\n",
      "Loss: 0.665998646859855\n",
      "Accuracy: 0.6000428143285286\n",
      "Iteration: 10\n",
      "Loss: 0.6649313061300225\n",
      "Accuracy: 0.6022905665762809\n",
      "Iteration: 11\n",
      "Loss: 0.6639003049591506\n",
      "Accuracy: 0.6026830312544599\n",
      "Iteration: 12\n",
      "Loss: 0.6631447026236789\n",
      "Accuracy: 0.6018624232909947\n",
      "Iteration: 13\n",
      "Loss: 0.6624199684721305\n",
      "Accuracy: 0.6022548879691737\n",
      "Iteration: 14\n",
      "Loss: 0.6616344316083818\n",
      "Accuracy: 0.6026116740402455\n",
      "Iteration: 15\n",
      "Loss: 0.660998231974432\n",
      "Accuracy: 0.6035749964321393\n",
      "Iteration: 16\n",
      "Loss: 0.6604207175947183\n",
      "Accuracy: 0.6035393178250321\n",
      "Iteration: 17\n",
      "Loss: 0.659939812581545\n",
      "Accuracy: 0.6042172113600685\n",
      "Iteration: 18\n",
      "Loss: 0.6594489871511133\n",
      "Accuracy: 0.6047880690737834\n",
      "Iteration: 19\n",
      "Loss: 0.6589746405887993\n",
      "Accuracy: 0.6053232481803911\n",
      "Iteration: 20\n",
      "Loss: 0.6585240942394563\n",
      "Accuracy: 0.6049664621093193\n",
      "Iteration: 21\n",
      "Loss: 0.658186807006793\n",
      "Accuracy: 0.6053946053946054\n",
      "Iteration: 22\n",
      "Loss: 0.6577180706592625\n",
      "Accuracy: 0.6054659626088198\n",
      "Iteration: 23\n",
      "Loss: 0.6573537292951911\n",
      "Accuracy: 0.6053589267874983\n",
      "Iteration: 24\n",
      "Loss: 0.6570531639813264\n",
      "Accuracy: 0.6054302840017126\n",
      "Iteration: 25\n",
      "Loss: 0.656723050641453\n",
      "Accuracy: 0.6059654631083202\n",
      "Iteration: 26\n",
      "Loss: 0.6564782103550684\n",
      "Accuracy: 0.6058227486798915\n",
      "Iteration: 27\n",
      "Loss: 0.6561756772054835\n",
      "Accuracy: 0.6070714999286427\n",
      "Iteration: 28\n",
      "Loss: 0.6559396204357306\n",
      "Accuracy: 0.6077850720707864\n",
      "Iteration: 29\n",
      "Loss: 0.6556366650740842\n",
      "Accuracy: 0.6073926073926074\n",
      "Iteration: 30\n",
      "Loss: 0.6553328593240524\n",
      "Accuracy: 0.6079277864992151\n",
      "Iteration: 31\n",
      "Loss: 0.6551202666933179\n",
      "Accuracy: 0.6083559297845013\n",
      "Iteration: 32\n",
      "Loss: 0.6549089145894417\n",
      "Accuracy: 0.6084986442129299\n",
      "Iteration: 33\n",
      "Loss: 0.6546754215425166\n",
      "Accuracy: 0.6083916083916084\n",
      "Iteration: 34\n",
      "Loss: 0.6544651106393126\n",
      "Accuracy: 0.608712715855573\n",
      "Iteration: 35\n",
      "Loss: 0.6543220515617106\n",
      "Accuracy: 0.6095690024261453\n",
      "Iteration: 36\n",
      "Loss: 0.6541752274263731\n",
      "Accuracy: 0.6095690024261453\n",
      "Iteration: 37\n",
      "Loss: 0.6539722928347437\n",
      "Accuracy: 0.6098901098901099\n",
      "Iteration: 38\n",
      "Loss: 0.6537815761447879\n",
      "Accuracy: 0.6101398601398601\n",
      "Iteration: 39\n",
      "Loss: 0.6536011760248359\n",
      "Accuracy: 0.6101398601398601\n",
      "Iteration: 40\n",
      "Loss: 0.6534293839892378\n",
      "Accuracy: 0.610318253175396\n",
      "Iteration: 41\n",
      "Loss: 0.6533134030987404\n",
      "Accuracy: 0.610318253175396\n",
      "Iteration: 42\n",
      "Loss: 0.6531243560342\n",
      "Accuracy: 0.610710717853575\n",
      "Iteration: 43\n",
      "Loss: 0.6529700191960144\n",
      "Accuracy: 0.6104609676038247\n",
      "Iteration: 44\n",
      "Loss: 0.6528692234966185\n",
      "Accuracy: 0.6106036820322535\n",
      "Iteration: 45\n",
      "Loss: 0.6527363964171806\n",
      "Accuracy: 0.6109604681033253\n",
      "Iteration: 46\n",
      "Loss: 0.6525133632424241\n",
      "Accuracy: 0.6110675039246468\n",
      "Iteration: 47\n",
      "Loss: 0.6523958491271071\n",
      "Accuracy: 0.6111388611388612\n",
      "Iteration: 48\n",
      "Loss: 0.6522451727542883\n",
      "Accuracy: 0.6113886113886113\n",
      "Iteration: 49\n",
      "Loss: 0.6520763369833484\n",
      "Accuracy: 0.6111745397459684\n",
      "Iteration: 50\n",
      "Loss: 0.6519336381884016\n",
      "Accuracy: 0.611709718852576\n",
      "Iteration: 51\n",
      "Loss: 0.6518315938450482\n",
      "Accuracy: 0.6117810760667903\n",
      "Iteration: 52\n",
      "Loss: 0.6517115778303957\n",
      "Accuracy: 0.6128871128871128\n",
      "Iteration: 53\n",
      "Loss: 0.6515904802545228\n",
      "Accuracy: 0.61292279149422\n",
      "Iteration: 54\n",
      "Loss: 0.6514190677097075\n",
      "Accuracy: 0.6132438989581847\n",
      "Iteration: 55\n",
      "Loss: 0.6513018237878268\n",
      "Accuracy: 0.6132082203510775\n",
      "Iteration: 56\n",
      "Loss: 0.6511639968629661\n",
      "Accuracy: 0.6131011845297559\n",
      "Iteration: 57\n",
      "Loss: 0.6510325975076251\n",
      "Accuracy: 0.6136006850292565\n",
      "Iteration: 58\n",
      "Loss: 0.6509123785876096\n",
      "Accuracy: 0.6132438989581847\n",
      "Iteration: 59\n",
      "Loss: 0.6507763510452131\n",
      "Accuracy: 0.6136006850292565\n",
      "Iteration: 60\n",
      "Loss: 0.6506400321226171\n",
      "Accuracy: 0.6133866133866134\n",
      "Iteration: 61\n",
      "Loss: 0.650548825493111\n",
      "Accuracy: 0.6136363636363636\n",
      "Iteration: 62\n",
      "Loss: 0.6504806768439838\n",
      "Accuracy: 0.6137790780647924\n",
      "Iteration: 63\n",
      "Loss: 0.6503402347070757\n",
      "Accuracy: 0.6138861138861139\n",
      "Iteration: 64\n",
      "Loss: 0.6502568773659242\n",
      "Accuracy: 0.6141715427429714\n",
      "Iteration: 65\n",
      "Loss: 0.6501955114436127\n",
      "Accuracy: 0.6142785785642928\n",
      "Iteration: 66\n",
      "Loss: 0.6500987333884015\n",
      "Accuracy: 0.6138861138861139\n",
      "Iteration: 67\n",
      "Loss: 0.6500299446313734\n",
      "Accuracy: 0.6140288283145426\n",
      "Iteration: 68\n",
      "Loss: 0.6499436260743543\n",
      "Accuracy: 0.6145640074211502\n",
      "Iteration: 69\n",
      "Loss: 0.6498013944604069\n",
      "Accuracy: 0.614706721849579\n",
      "Iteration: 70\n",
      "Loss: 0.649713840271427\n",
      "Accuracy: 0.6147424004566862\n",
      "Iteration: 71\n",
      "Loss: 0.649640585654514\n",
      "Accuracy: 0.6147424004566862\n",
      "Iteration: 72\n",
      "Loss: 0.6495761680914096\n",
      "Accuracy: 0.6147424004566862\n",
      "Iteration: 73\n",
      "Loss: 0.6494484905640003\n",
      "Accuracy: 0.6149921507064364\n",
      "Iteration: 74\n",
      "Loss: 0.6492886070370446\n",
      "Accuracy: 0.6153846153846154\n",
      "Iteration: 75\n",
      "Loss: 0.649240922892887\n",
      "Accuracy: 0.6153132581704011\n",
      "Iteration: 76\n",
      "Loss: 0.6491713683526196\n",
      "Accuracy: 0.6158484372770087\n",
      "Iteration: 77\n",
      "Loss: 0.6490769742629342\n",
      "Accuracy: 0.6157057228485799\n",
      "Iteration: 78\n",
      "Loss: 0.6489902183547894\n",
      "Accuracy: 0.6161695447409733\n",
      "Iteration: 79\n",
      "Loss: 0.6488720981117722\n",
      "Accuracy: 0.615919794491223\n",
      "Iteration: 80\n",
      "Loss: 0.6488535541411385\n",
      "Accuracy: 0.6157414014556871\n",
      "Iteration: 81\n",
      "Loss: 0.6487845948949647\n",
      "Accuracy: 0.6159911517054374\n",
      "Iteration: 82\n",
      "Loss: 0.6487318393025826\n",
      "Accuracy: 0.6162409019551877\n",
      "Iteration: 83\n",
      "Loss: 0.6486469889741313\n",
      "Accuracy: 0.6161338661338661\n",
      "Iteration: 84\n",
      "Loss: 0.6485142138464105\n",
      "Accuracy: 0.6164192949907236\n",
      "Iteration: 85\n",
      "Loss: 0.6484437182741085\n",
      "Accuracy: 0.6166333666333667\n",
      "Iteration: 86\n",
      "Loss: 0.6483512345392726\n",
      "Accuracy: 0.6166690452404738\n",
      "Iteration: 87\n",
      "Loss: 0.6482593164625644\n",
      "Accuracy: 0.6169901527044385\n",
      "Iteration: 88\n",
      "Loss: 0.6481967844287035\n",
      "Accuracy: 0.6171685457399743\n",
      "Iteration: 89\n",
      "Loss: 0.6481271138860809\n",
      "Accuracy: 0.6172755815612958\n",
      "Iteration: 90\n",
      "Loss: 0.6480467743530991\n",
      "Accuracy: 0.6173469387755102\n",
      "Iteration: 91\n",
      "Loss: 0.6479729720010251\n",
      "Accuracy: 0.6172399029541886\n",
      "Iteration: 92\n",
      "Loss: 0.647891984474563\n",
      "Accuracy: 0.6176323676323676\n",
      "Iteration: 93\n",
      "Loss: 0.6478283060306221\n",
      "Accuracy: 0.6176680462394748\n",
      "Iteration: 94\n",
      "Loss: 0.6477270047590131\n",
      "Accuracy: 0.6175610104181533\n",
      "Iteration: 95\n",
      "Loss: 0.6476456841666426\n",
      "Accuracy: 0.6171328671328671\n",
      "Iteration: 96\n",
      "Loss: 0.6476128883693114\n",
      "Accuracy: 0.6172399029541886\n",
      "Iteration: 97\n",
      "Loss: 0.6475766300055406\n",
      "Accuracy: 0.6174182959897245\n",
      "Iteration: 98\n",
      "Loss: 0.6475579896462283\n",
      "Accuracy: 0.6172042243470814\n",
      "Iteration: 99\n",
      "Loss: 0.6474807857899088\n",
      "Accuracy: 0.6170258313115456\n",
      "Iteration: 100\n",
      "Loss: 0.6474168247369515\n",
      "Accuracy: 0.6170615099186528\n",
      "Iteration: 101\n",
      "Loss: 0.647365091358317\n",
      "Accuracy: 0.6174182959897245\n",
      "Iteration: 102\n",
      "Loss: 0.6472934310391042\n",
      "Accuracy: 0.6171685457399743\n",
      "Iteration: 103\n",
      "Loss: 0.6472348038412418\n",
      "Accuracy: 0.6174539745968317\n",
      "Iteration: 104\n",
      "Loss: 0.6471816220810184\n",
      "Accuracy: 0.6175610104181533\n",
      "Iteration: 105\n",
      "Loss: 0.6470646974462612\n",
      "Accuracy: 0.6175966890252604\n",
      "Iteration: 106\n",
      "Loss: 0.6470064996380329\n",
      "Accuracy: 0.6176680462394748\n",
      "Iteration: 107\n",
      "Loss: 0.6469643846496856\n",
      "Accuracy: 0.6178107606679035\n",
      "Iteration: 108\n",
      "Loss: 0.6469472300078973\n",
      "Accuracy: 0.6176323676323676\n",
      "Iteration: 109\n",
      "Loss: 0.6468539361983264\n",
      "Accuracy: 0.6178821178821179\n",
      "Iteration: 110\n",
      "Loss: 0.6467871301372036\n",
      "Accuracy: 0.6178107606679035\n",
      "Iteration: 111\n",
      "Loss: 0.6467246907539173\n",
      "Accuracy: 0.6178821178821179\n",
      "Iteration: 112\n",
      "Loss: 0.6466555720577072\n",
      "Accuracy: 0.6178821178821179\n",
      "Iteration: 113\n",
      "Loss: 0.6465951570367445\n",
      "Accuracy: 0.6177750820607963\n",
      "Iteration: 114\n",
      "Loss: 0.6465484041935106\n",
      "Accuracy: 0.6178464392750107\n",
      "Iteration: 115\n",
      "Loss: 0.6465005593770019\n",
      "Accuracy: 0.6181675467389753\n",
      "Iteration: 116\n",
      "Loss: 0.6464361922346565\n",
      "Accuracy: 0.6183816183816184\n",
      "Iteration: 117\n",
      "Loss: 0.6463783662144262\n",
      "Accuracy: 0.6186313686313686\n",
      "Iteration: 118\n",
      "Loss: 0.6463558572910914\n",
      "Accuracy: 0.6192379049521907\n",
      "Iteration: 119\n",
      "Loss: 0.6462527010471294\n",
      "Accuracy: 0.6196303696303697\n",
      "Iteration: 120\n",
      "Loss: 0.6461979205583372\n",
      "Accuracy: 0.6192735835592978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 121\n",
      "Loss: 0.6461332269954168\n",
      "Accuracy: 0.6194876552019409\n",
      "Iteration: 122\n",
      "Loss: 0.6461033917668759\n",
      "Accuracy: 0.6192379049521907\n",
      "Iteration: 123\n",
      "Loss: 0.6460176910859113\n",
      "Accuracy: 0.619309262166405\n",
      "Iteration: 124\n",
      "Loss: 0.6459686759592962\n",
      "Accuracy: 0.6195233338090481\n",
      "Iteration: 125\n",
      "Loss: 0.645934480445295\n",
      "Accuracy: 0.6191665477379763\n",
      "Iteration: 126\n",
      "Loss: 0.6458560445468351\n",
      "Accuracy: 0.6196303696303697\n",
      "Iteration: 127\n",
      "Loss: 0.6457644560062383\n",
      "Accuracy: 0.6197730840587984\n",
      "Iteration: 128\n",
      "Loss: 0.6457166598677557\n",
      "Accuracy: 0.6201655487369773\n",
      "Iteration: 129\n",
      "Loss: 0.6456307528615051\n",
      "Accuracy: 0.6200585129156557\n",
      "Iteration: 130\n",
      "Loss: 0.6456045040465609\n",
      "Accuracy: 0.6200228343085485\n",
      "Iteration: 131\n",
      "Loss: 0.64554302024918\n",
      "Accuracy: 0.6200941915227629\n",
      "Iteration: 132\n",
      "Loss: 0.6454916417399418\n",
      "Accuracy: 0.6201298701298701\n",
      "Iteration: 133\n",
      "Loss: 0.6454449460587773\n",
      "Accuracy: 0.6201655487369773\n",
      "Iteration: 134\n",
      "Loss: 0.6454073478236979\n",
      "Accuracy: 0.6203439417725132\n",
      "Iteration: 135\n",
      "Loss: 0.6453501561683831\n",
      "Accuracy: 0.6204152989867275\n",
      "Iteration: 136\n",
      "Loss: 0.6452921547455698\n",
      "Accuracy: 0.6206650492364778\n",
      "Iteration: 137\n",
      "Loss: 0.6452310889681524\n",
      "Accuracy: 0.6205223348080491\n",
      "Iteration: 138\n",
      "Loss: 0.6452142751221215\n",
      "Accuracy: 0.6204509775938347\n",
      "Iteration: 139\n",
      "Loss: 0.6451780519103749\n",
      "Accuracy: 0.6204866562009419\n",
      "Iteration: 140\n",
      "Loss: 0.6451222110713318\n",
      "Accuracy: 0.6204866562009419\n",
      "Iteration: 141\n",
      "Loss: 0.6450819215944871\n",
      "Accuracy: 0.6208434422720137\n",
      "Iteration: 142\n",
      "Loss: 0.6450283106456299\n",
      "Accuracy: 0.6211288711288712\n",
      "Iteration: 143\n",
      "Loss: 0.6449882250703792\n",
      "Accuracy: 0.6213429427715143\n",
      "Iteration: 144\n",
      "Loss: 0.6449290751585974\n",
      "Accuracy: 0.6213072641644071\n",
      "Iteration: 145\n",
      "Loss: 0.6448858234465766\n",
      "Accuracy: 0.6214499785928357\n",
      "Iteration: 146\n",
      "Loss: 0.6448351663911833\n",
      "Accuracy: 0.62152133580705\n",
      "Iteration: 147\n",
      "Loss: 0.6447794942720608\n",
      "Accuracy: 0.6218424432710147\n",
      "Iteration: 148\n",
      "Loss: 0.6447497655113634\n",
      "Accuracy: 0.6216640502354788\n",
      "Iteration: 149\n",
      "Loss: 0.644710611063092\n",
      "Accuracy: 0.6216640502354788\n",
      "Iteration: 150\n",
      "Loss: 0.6446678774555317\n",
      "Accuracy: 0.6217710860568003\n",
      "Iteration: 151\n",
      "Loss: 0.644628779817032\n",
      "Accuracy: 0.6219494790923362\n",
      "Iteration: 152\n",
      "Loss: 0.6446058668812706\n",
      "Accuracy: 0.6220208363065506\n",
      "Iteration: 153\n",
      "Loss: 0.6445858911229617\n",
      "Accuracy: 0.6221278721278721\n",
      "Iteration: 154\n",
      "Loss: 0.6445804177933288\n",
      "Accuracy: 0.6218424432710147\n",
      "Iteration: 155\n",
      "Loss: 0.6445439842629913\n",
      "Accuracy: 0.621913800485229\n",
      "Iteration: 156\n",
      "Loss: 0.6445195655921689\n",
      "Accuracy: 0.6220565149136578\n",
      "Iteration: 157\n",
      "Loss: 0.6444834873930118\n",
      "Accuracy: 0.6220208363065506\n",
      "Iteration: 158\n",
      "Loss: 0.6444250938177207\n",
      "Accuracy: 0.6221278721278721\n",
      "Iteration: 159\n",
      "Loss: 0.644382238016495\n",
      "Accuracy: 0.6220565149136578\n",
      "Iteration: 160\n",
      "Loss: 0.6443649063865386\n",
      "Accuracy: 0.6221635507349793\n",
      "Iteration: 161\n",
      "Loss: 0.6443645635860581\n",
      "Accuracy: 0.622306265163408\n",
      "Iteration: 162\n",
      "Loss: 0.6442832514118261\n",
      "Accuracy: 0.6226630512344798\n",
      "Iteration: 163\n",
      "Loss: 0.6442395669995837\n",
      "Accuracy: 0.6226630512344798\n",
      "Iteration: 164\n",
      "Loss: 0.6441793933991137\n",
      "Accuracy: 0.6227344084486942\n",
      "Iteration: 165\n",
      "Loss: 0.6441709627900427\n",
      "Accuracy: 0.622698729841587\n",
      "Iteration: 166\n",
      "Loss: 0.6441263015371614\n",
      "Accuracy: 0.6228057656629086\n",
      "Iteration: 167\n",
      "Loss: 0.6440880768887195\n",
      "Accuracy: 0.6230198373055515\n",
      "Iteration: 168\n",
      "Loss: 0.6440483161137194\n",
      "Accuracy: 0.6228771228771228\n",
      "Iteration: 169\n",
      "Loss: 0.644004954283202\n",
      "Accuracy: 0.62291280148423\n",
      "Iteration: 170\n",
      "Loss: 0.6439433384263534\n",
      "Accuracy: 0.6228414442700158\n",
      "Iteration: 171\n",
      "Loss: 0.6438965880608613\n",
      "Accuracy: 0.6227700870558014\n",
      "Iteration: 172\n",
      "Loss: 0.6438219486477887\n",
      "Accuracy: 0.6233409447695162\n",
      "Iteration: 173\n",
      "Loss: 0.6437624372903143\n",
      "Accuracy: 0.6234123019837305\n",
      "Iteration: 174\n",
      "Loss: 0.6437230592380465\n",
      "Accuracy: 0.6236263736263736\n",
      "Iteration: 175\n",
      "Loss: 0.6436528632982969\n",
      "Accuracy: 0.6237334094476952\n",
      "Iteration: 176\n",
      "Loss: 0.6436109703602567\n",
      "Accuracy: 0.6237690880548024\n",
      "Iteration: 177\n",
      "Loss: 0.643576748454538\n",
      "Accuracy: 0.6237690880548024\n",
      "Iteration: 178\n",
      "Loss: 0.6435184276747989\n",
      "Accuracy: 0.6242329099471957\n",
      "Iteration: 179\n",
      "Loss: 0.6434838123814408\n",
      "Accuracy: 0.6243399457685171\n",
      "Iteration: 180\n",
      "Loss: 0.6434587464312961\n",
      "Accuracy: 0.6246610532324818\n",
      "Iteration: 181\n",
      "Loss: 0.6434126446668744\n",
      "Accuracy: 0.6248037676609105\n",
      "Iteration: 182\n",
      "Loss: 0.6433676388678266\n",
      "Accuracy: 0.6246253746253746\n",
      "Iteration: 183\n",
      "Loss: 0.6433240382566218\n",
      "Accuracy: 0.6246253746253746\n",
      "Iteration: 184\n",
      "Loss: 0.6432490198334573\n",
      "Accuracy: 0.6248394462680177\n",
      "Iteration: 185\n",
      "Loss: 0.6432048855322745\n",
      "Accuracy: 0.6250178393035536\n",
      "Iteration: 186\n",
      "Loss: 0.643177911287095\n",
      "Accuracy: 0.6248751248751249\n",
      "Iteration: 187\n",
      "Loss: 0.6431088463000887\n",
      "Accuracy: 0.6248394462680177\n",
      "Iteration: 188\n",
      "Loss: 0.6430602387563547\n",
      "Accuracy: 0.6248037676609105\n",
      "Iteration: 189\n",
      "Loss: 0.6430480072998154\n",
      "Accuracy: 0.625089196517768\n",
      "Iteration: 190\n",
      "Loss: 0.6430133351774517\n",
      "Accuracy: 0.6251605537319823\n",
      "Iteration: 191\n",
      "Loss: 0.6429269380828578\n",
      "Accuracy: 0.6253746253746254\n",
      "Iteration: 192\n",
      "Loss: 0.6428938883648252\n",
      "Accuracy: 0.6253746253746254\n",
      "Iteration: 193\n",
      "Loss: 0.6428736622175436\n",
      "Accuracy: 0.6252319109461967\n",
      "Iteration: 194\n",
      "Loss: 0.6428467578657379\n",
      "Accuracy: 0.6252675895533039\n",
      "Iteration: 195\n",
      "Loss: 0.642815424037347\n",
      "Accuracy: 0.6251248751248751\n",
      "Iteration: 196\n",
      "Loss: 0.6427931611922848\n",
      "Accuracy: 0.6252675895533039\n",
      "Iteration: 197\n",
      "Loss: 0.6427811699821416\n",
      "Accuracy: 0.62569573283859\n",
      "Iteration: 198\n",
      "Loss: 0.6427168299342558\n",
      "Accuracy: 0.6257314114456971\n",
      "Iteration: 199\n",
      "Loss: 0.6426864571797836\n",
      "Accuracy: 0.6257314114456971\n",
      "Iteration: 200\n",
      "Loss: 0.6426881128599505\n",
      "Accuracy: 0.6255886970172685\n",
      "Iteration: 201\n",
      "Loss: 0.6426034192414302\n",
      "Accuracy: 0.6256600542314829\n",
      "Iteration: 202\n",
      "Loss: 0.642563152455478\n",
      "Accuracy: 0.6259454830883402\n",
      "Iteration: 203\n",
      "Loss: 0.6425379776146833\n",
      "Accuracy: 0.6259454830883402\n",
      "Iteration: 204\n",
      "Loss: 0.6425147351138689\n",
      "Accuracy: 0.6259811616954474\n",
      "Iteration: 205\n",
      "Loss: 0.6424749935460417\n",
      "Accuracy: 0.6260881975167689\n",
      "Iteration: 206\n",
      "Loss: 0.6424279257710441\n",
      "Accuracy: 0.6263736263736264\n",
      "Iteration: 207\n",
      "Loss: 0.6424054166176882\n",
      "Accuracy: 0.6264093049807336\n",
      "Iteration: 208\n",
      "Loss: 0.6423636401087112\n",
      "Accuracy: 0.6264093049807336\n",
      "Iteration: 209\n",
      "Loss: 0.6423262689199974\n",
      "Accuracy: 0.6264806621949479\n",
      "Iteration: 210\n",
      "Loss: 0.6422981922934399\n",
      "Accuracy: 0.6264806621949479\n",
      "Iteration: 211\n",
      "Loss: 0.6422657836459114\n",
      "Accuracy: 0.6264806621949479\n",
      "Iteration: 212\n",
      "Loss: 0.6422417427694733\n",
      "Accuracy: 0.6264093049807336\n",
      "Iteration: 213\n",
      "Loss: 0.6422222647762285\n",
      "Accuracy: 0.626302269159412\n",
      "Iteration: 214\n",
      "Loss: 0.6422012173554982\n",
      "Accuracy: 0.6263379477665192\n",
      "Iteration: 215\n",
      "Loss: 0.6421582788291067\n",
      "Accuracy: 0.6264093049807336\n",
      "Iteration: 216\n",
      "Loss: 0.6421276959506083\n",
      "Accuracy: 0.6260881975167689\n",
      "Iteration: 217\n",
      "Loss: 0.6420970542026795\n",
      "Accuracy: 0.6264449835878407\n",
      "Iteration: 218\n",
      "Loss: 0.6420387918096393\n",
      "Accuracy: 0.6265520194091623\n",
      "Iteration: 219\n",
      "Loss: 0.6420108084040683\n",
      "Accuracy: 0.6263379477665192\n",
      "Iteration: 220\n",
      "Loss: 0.6419733091769915\n",
      "Accuracy: 0.6263736263736264\n",
      "Iteration: 221\n",
      "Loss: 0.6419278869464838\n",
      "Accuracy: 0.6265876980162695\n",
      "Iteration: 222\n",
      "Loss: 0.6418935006889395\n",
      "Accuracy: 0.6264093049807336\n",
      "Iteration: 223\n",
      "Loss: 0.6418356122351208\n",
      "Accuracy: 0.6265876980162695\n",
      "Iteration: 224\n",
      "Loss: 0.6417871769751141\n",
      "Accuracy: 0.6268374482660197\n",
      "Iteration: 225\n",
      "Loss: 0.6417419613483314\n",
      "Accuracy: 0.6271585557299842\n",
      "Iteration: 226\n",
      "Loss: 0.6417070091947845\n",
      "Accuracy: 0.627301270158413\n",
      "Iteration: 227\n",
      "Loss: 0.6416814904960316\n",
      "Accuracy: 0.6272299129441986\n",
      "Iteration: 228\n",
      "Loss: 0.6416492273750136\n",
      "Accuracy: 0.6274083059797345\n",
      "Iteration: 229\n",
      "Loss: 0.6416362476105416\n",
      "Accuracy: 0.6275153418010561\n",
      "Iteration: 230\n",
      "Loss: 0.6416068621562654\n",
      "Accuracy: 0.6275510204081632\n",
      "Iteration: 231\n",
      "Loss: 0.641563988533689\n",
      "Accuracy: 0.6270515199086628\n",
      "Iteration: 232\n",
      "Loss: 0.6415484542772134\n",
      "Accuracy: 0.62708719851577\n",
      "Iteration: 233\n",
      "Loss: 0.6415493457376582\n",
      "Accuracy: 0.6270515199086628\n",
      "Iteration: 234\n",
      "Loss: 0.6415254745875238\n",
      "Accuracy: 0.6270515199086628\n",
      "Iteration: 235\n",
      "Loss: 0.6415113425387988\n",
      "Accuracy: 0.6273369487655202\n",
      "Iteration: 236\n",
      "Loss: 0.6414674653043289\n",
      "Accuracy: 0.6274083059797345\n",
      "Iteration: 237\n",
      "Loss: 0.641450672004988\n",
      "Accuracy: 0.6275153418010561\n",
      "Iteration: 238\n",
      "Loss: 0.6414320011300304\n",
      "Accuracy: 0.627693734836592\n",
      "Iteration: 239\n",
      "Loss: 0.6414180390488657\n",
      "Accuracy: 0.6278721278721279\n",
      "Iteration: 240\n",
      "Loss: 0.6413844279385889\n",
      "Accuracy: 0.6280148423005566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 241\n",
      "Loss: 0.641351989744757\n",
      "Accuracy: 0.6280148423005566\n",
      "Iteration: 242\n",
      "Loss: 0.6413341980515591\n",
      "Accuracy: 0.6281575567289853\n",
      "Iteration: 243\n",
      "Loss: 0.6413390342576697\n",
      "Accuracy: 0.6281575567289853\n",
      "Iteration: 244\n",
      "Loss: 0.6413116749951998\n",
      "Accuracy: 0.628300271157414\n",
      "Iteration: 245\n",
      "Loss: 0.6412671413117368\n",
      "Accuracy: 0.6283359497645212\n",
      "Iteration: 246\n",
      "Loss: 0.6412354711395437\n",
      "Accuracy: 0.6283716283716284\n",
      "Iteration: 247\n",
      "Loss: 0.6412169738283755\n",
      "Accuracy: 0.6283716283716284\n",
      "Iteration: 248\n",
      "Loss: 0.6411977258351729\n",
      "Accuracy: 0.6285857000142715\n",
      "Iteration: 249\n",
      "Loss: 0.6411471725207983\n",
      "Accuracy: 0.6285857000142715\n",
      "Iteration: 250\n",
      "Loss: 0.6411463087316128\n",
      "Accuracy: 0.6286213786213786\n",
      "Iteration: 251\n",
      "Loss: 0.6411301832017622\n",
      "Accuracy: 0.6284073069787356\n",
      "Iteration: 252\n",
      "Loss: 0.6411047380175936\n",
      "Accuracy: 0.6286213786213786\n",
      "Iteration: 253\n",
      "Loss: 0.6411056147455382\n",
      "Accuracy: 0.6286213786213786\n",
      "Iteration: 254\n",
      "Loss: 0.6410844823601293\n",
      "Accuracy: 0.6290852005137719\n",
      "Iteration: 255\n",
      "Loss: 0.6410455890003357\n",
      "Accuracy: 0.6292279149422007\n",
      "Iteration: 256\n",
      "Loss: 0.6410112616964456\n",
      "Accuracy: 0.6292279149422007\n",
      "Iteration: 257\n",
      "Loss: 0.6409572789053591\n",
      "Accuracy: 0.6292279149422007\n",
      "Iteration: 258\n",
      "Loss: 0.6409546136416027\n",
      "Accuracy: 0.6290138432995576\n",
      "Iteration: 259\n",
      "Loss: 0.6409286194923959\n",
      "Accuracy: 0.6290852005137719\n",
      "Iteration: 260\n",
      "Loss: 0.6408825519699887\n",
      "Accuracy: 0.6291565577279863\n",
      "Iteration: 261\n",
      "Loss: 0.6408409631906599\n",
      "Accuracy: 0.6293349507635222\n",
      "Iteration: 262\n",
      "Loss: 0.6408055128686394\n",
      "Accuracy: 0.6296203796203796\n",
      "Iteration: 263\n",
      "Loss: 0.6408008349697324\n",
      "Accuracy: 0.6295133437990581\n",
      "Iteration: 264\n",
      "Loss: 0.640766396858895\n",
      "Accuracy: 0.6296203796203796\n",
      "Iteration: 265\n",
      "Loss: 0.6407543020677968\n",
      "Accuracy: 0.6297274154417012\n",
      "Iteration: 266\n",
      "Loss: 0.6407322155490933\n",
      "Accuracy: 0.6299058084772371\n",
      "Iteration: 267\n",
      "Loss: 0.6407231404277979\n",
      "Accuracy: 0.6299414870843443\n",
      "Iteration: 268\n",
      "Loss: 0.6407116958885758\n",
      "Accuracy: 0.6301912373340944\n",
      "Iteration: 269\n",
      "Loss: 0.6406814597579226\n",
      "Accuracy: 0.6302625945483088\n",
      "Iteration: 270\n",
      "Loss: 0.6406850109399589\n",
      "Accuracy: 0.6301198801198801\n",
      "Iteration: 271\n",
      "Loss: 0.6406401734392995\n",
      "Accuracy: 0.6302625945483088\n",
      "Iteration: 272\n",
      "Loss: 0.6406230637077382\n",
      "Accuracy: 0.6304053089767375\n",
      "Iteration: 273\n",
      "Loss: 0.6405884062712679\n",
      "Accuracy: 0.6304053089767375\n",
      "Iteration: 274\n",
      "Loss: 0.6405824899871195\n",
      "Accuracy: 0.6303696303696303\n",
      "Iteration: 275\n",
      "Loss: 0.6405769623263554\n",
      "Accuracy: 0.6304409875838447\n",
      "Iteration: 276\n",
      "Loss: 0.6405447990083577\n",
      "Accuracy: 0.6304409875838447\n",
      "Iteration: 277\n",
      "Loss: 0.6405176537173041\n",
      "Accuracy: 0.6304766661909519\n",
      "Iteration: 278\n",
      "Loss: 0.6404991679342513\n",
      "Accuracy: 0.6303696303696303\n",
      "Iteration: 279\n",
      "Loss: 0.6404831108641346\n",
      "Accuracy: 0.630690737833595\n",
      "Iteration: 280\n",
      "Loss: 0.6404643745492682\n",
      "Accuracy: 0.6306193806193806\n",
      "Iteration: 281\n",
      "Loss: 0.6404194811023413\n",
      "Accuracy: 0.6307264164407022\n",
      "Iteration: 282\n",
      "Loss: 0.6404018897047882\n",
      "Accuracy: 0.6307264164407022\n",
      "Iteration: 283\n",
      "Loss: 0.640374206138399\n",
      "Accuracy: 0.6307620950478093\n",
      "Iteration: 284\n",
      "Loss: 0.6403392932290207\n",
      "Accuracy: 0.630690737833595\n",
      "Iteration: 285\n",
      "Loss: 0.6403063379005853\n",
      "Accuracy: 0.6307620950478093\n",
      "Iteration: 286\n",
      "Loss: 0.640253093279143\n",
      "Accuracy: 0.6307620950478093\n",
      "Iteration: 287\n",
      "Loss: 0.6402612745213302\n",
      "Accuracy: 0.6306193806193806\n",
      "Iteration: 288\n",
      "Loss: 0.6402401531367491\n",
      "Accuracy: 0.6310118452975596\n",
      "Iteration: 289\n",
      "Loss: 0.6402032690948933\n",
      "Accuracy: 0.6311545597259883\n",
      "Iteration: 290\n",
      "Loss: 0.640158481046478\n",
      "Accuracy: 0.6309761666904524\n",
      "Iteration: 291\n",
      "Loss: 0.6400876810541911\n",
      "Accuracy: 0.6307977736549165\n",
      "Iteration: 292\n",
      "Loss: 0.6400428813791543\n",
      "Accuracy: 0.6308691308691309\n",
      "Iteration: 293\n",
      "Loss: 0.6400100701936678\n",
      "Accuracy: 0.6310475239046668\n",
      "Iteration: 294\n",
      "Loss: 0.6399766717911793\n",
      "Accuracy: 0.6311545597259883\n",
      "Iteration: 295\n",
      "Loss: 0.6399462462200217\n",
      "Accuracy: 0.6313686313686314\n",
      "Iteration: 296\n",
      "Loss: 0.6398863836719731\n",
      "Accuracy: 0.6314399885828458\n",
      "Iteration: 297\n",
      "Loss: 0.6398810287187547\n",
      "Accuracy: 0.6314043099757386\n",
      "Iteration: 298\n",
      "Loss: 0.639861206846958\n",
      "Accuracy: 0.6314399885828458\n",
      "Iteration: 299\n",
      "Loss: 0.6398000954435891\n",
      "Accuracy: 0.6314756671899528\n",
      "Iteration: 300\n",
      "Loss: 0.6397562572034311\n",
      "Accuracy: 0.63151134579706\n",
      "Iteration: 301\n",
      "Loss: 0.6397439109770009\n",
      "Accuracy: 0.6314399885828458\n",
      "Iteration: 302\n",
      "Loss: 0.6396978014075716\n",
      "Accuracy: 0.63151134579706\n",
      "Iteration: 303\n",
      "Loss: 0.6396882085262392\n",
      "Accuracy: 0.6314756671899528\n",
      "Iteration: 304\n",
      "Loss: 0.6396875442312834\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 305\n",
      "Loss: 0.6396577419743229\n",
      "Accuracy: 0.6317610960468103\n",
      "Iteration: 306\n",
      "Loss: 0.6396343200091831\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 307\n",
      "Loss: 0.639614651370825\n",
      "Accuracy: 0.6315827030112744\n",
      "Iteration: 308\n",
      "Loss: 0.639579687968367\n",
      "Accuracy: 0.6314399885828458\n",
      "Iteration: 309\n",
      "Loss: 0.6395356210486304\n",
      "Accuracy: 0.6313686313686314\n",
      "Iteration: 310\n",
      "Loss: 0.6394939682233475\n",
      "Accuracy: 0.6314043099757386\n",
      "Iteration: 311\n",
      "Loss: 0.6394831579723155\n",
      "Accuracy: 0.6312615955473099\n",
      "Iteration: 312\n",
      "Loss: 0.639451824648553\n",
      "Accuracy: 0.6311545597259883\n",
      "Iteration: 313\n",
      "Loss: 0.6394289757741674\n",
      "Accuracy: 0.6314399885828458\n",
      "Iteration: 314\n",
      "Loss: 0.6393968868623422\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 315\n",
      "Loss: 0.6393766699286132\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 316\n",
      "Loss: 0.6393658656146274\n",
      "Accuracy: 0.6313329527615242\n",
      "Iteration: 317\n",
      "Loss: 0.6393567328260589\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 318\n",
      "Loss: 0.6393252984010356\n",
      "Accuracy: 0.63151134579706\n",
      "Iteration: 319\n",
      "Loss: 0.6393031113560035\n",
      "Accuracy: 0.6315470244041672\n",
      "Iteration: 320\n",
      "Loss: 0.6392507179643809\n",
      "Accuracy: 0.6315827030112744\n",
      "Iteration: 321\n",
      "Loss: 0.6391977356171056\n",
      "Accuracy: 0.6317610960468103\n",
      "Iteration: 322\n",
      "Loss: 0.6391482229687451\n",
      "Accuracy: 0.6320108462965606\n",
      "Iteration: 323\n",
      "Loss: 0.6391067781153844\n",
      "Accuracy: 0.6321178821178821\n",
      "Iteration: 324\n",
      "Loss: 0.63908070023874\n",
      "Accuracy: 0.6322249179392037\n",
      "Iteration: 325\n",
      "Loss: 0.6390319847987993\n",
      "Accuracy: 0.6324389895818467\n",
      "Iteration: 326\n",
      "Loss: 0.6390024078876687\n",
      "Accuracy: 0.6322249179392037\n",
      "Iteration: 327\n",
      "Loss: 0.6389787407146853\n",
      "Accuracy: 0.6322249179392037\n",
      "Iteration: 328\n",
      "Loss: 0.6389592299888796\n",
      "Accuracy: 0.6321535607249893\n",
      "Iteration: 329\n",
      "Loss: 0.6389414855972895\n",
      "Accuracy: 0.6322249179392037\n",
      "Iteration: 330\n",
      "Loss: 0.638900155547271\n",
      "Accuracy: 0.6323319537605252\n",
      "Iteration: 331\n",
      "Loss: 0.6388576636401178\n",
      "Accuracy: 0.6324389895818467\n",
      "Iteration: 332\n",
      "Loss: 0.6388347638299633\n",
      "Accuracy: 0.6324746681889539\n",
      "Iteration: 333\n",
      "Loss: 0.6387988100000507\n",
      "Accuracy: 0.6325103467960611\n",
      "Iteration: 334\n",
      "Loss: 0.6387741708598751\n",
      "Accuracy: 0.6326173826173827\n",
      "Iteration: 335\n",
      "Loss: 0.6387453180042266\n",
      "Accuracy: 0.6326173826173827\n",
      "Iteration: 336\n",
      "Loss: 0.6387128582238308\n",
      "Accuracy: 0.63290281147424\n",
      "Iteration: 337\n",
      "Loss: 0.6386984505620724\n",
      "Accuracy: 0.6327600970458114\n",
      "Iteration: 338\n",
      "Loss: 0.6386552106784622\n",
      "Accuracy: 0.63290281147424\n",
      "Iteration: 339\n",
      "Loss: 0.6386333671145442\n",
      "Accuracy: 0.6327957756529186\n",
      "Iteration: 340\n",
      "Loss: 0.6385950271985311\n",
      "Accuracy: 0.6327957756529186\n",
      "Iteration: 341\n",
      "Loss: 0.6385895793850365\n",
      "Accuracy: 0.6329384900813472\n",
      "Iteration: 342\n",
      "Loss: 0.6385733030588403\n",
      "Accuracy: 0.63290281147424\n",
      "Iteration: 343\n",
      "Loss: 0.6385538735017543\n",
      "Accuracy: 0.6330455259026687\n",
      "Iteration: 344\n",
      "Loss: 0.6385516485079776\n",
      "Accuracy: 0.6330812045097759\n",
      "Iteration: 345\n",
      "Loss: 0.638503598072431\n",
      "Accuracy: 0.6331168831168831\n",
      "Iteration: 346\n",
      "Loss: 0.6384484130218242\n",
      "Accuracy: 0.6330098472955615\n",
      "Iteration: 347\n",
      "Loss: 0.6384408498463153\n",
      "Accuracy: 0.6329384900813472\n",
      "Iteration: 348\n",
      "Loss: 0.6384126432007956\n",
      "Accuracy: 0.6328671328671329\n",
      "Iteration: 349\n",
      "Loss: 0.6383456909527397\n",
      "Accuracy: 0.6330455259026687\n",
      "Iteration: 350\n",
      "Loss: 0.6383267777855308\n",
      "Accuracy: 0.6330098472955615\n",
      "Iteration: 351\n",
      "Loss: 0.6382972743381576\n",
      "Accuracy: 0.6330098472955615\n",
      "Iteration: 352\n",
      "Loss: 0.6382742487063046\n",
      "Accuracy: 0.6329741686884544\n",
      "Iteration: 353\n",
      "Loss: 0.6382594632179476\n",
      "Accuracy: 0.6331882403310974\n",
      "Iteration: 354\n",
      "Loss: 0.638215827886754\n",
      "Accuracy: 0.633295276152419\n",
      "Iteration: 355\n",
      "Loss: 0.6382043706736432\n",
      "Accuracy: 0.6333666333666333\n",
      "Iteration: 356\n",
      "Loss: 0.6381784470300824\n",
      "Accuracy: 0.6335807050092764\n",
      "Iteration: 357\n",
      "Loss: 0.6381662691549923\n",
      "Accuracy: 0.6335093477950621\n",
      "Iteration: 358\n",
      "Loss: 0.6381485162978064\n",
      "Accuracy: 0.6335093477950621\n",
      "Iteration: 359\n",
      "Loss: 0.6381318904983577\n",
      "Accuracy: 0.6334379905808477\n",
      "Iteration: 360\n",
      "Loss: 0.638136269096358\n",
      "Accuracy: 0.6335093477950621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 361\n",
      "Loss: 0.638122503615329\n",
      "Accuracy: 0.6336163836163836\n",
      "Iteration: 362\n",
      "Loss: 0.6381029688104286\n",
      "Accuracy: 0.6335450264021693\n",
      "Iteration: 363\n",
      "Loss: 0.6380587673419066\n",
      "Accuracy: 0.6339018124732411\n",
      "Iteration: 364\n",
      "Loss: 0.6380505891350079\n",
      "Accuracy: 0.6339731696874554\n",
      "Iteration: 365\n",
      "Loss: 0.6380711651147037\n",
      "Accuracy: 0.6340088482945626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(features[\"train\"], train.funny.values.astype(np.int64), monitor = gmb_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.inf, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = [np.inf, 2, -9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(check[11:-3:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
