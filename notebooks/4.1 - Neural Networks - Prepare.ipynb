{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the files that will be used to train the neural networks. It saves \n",
    "\n",
    "develops artificial neural network (ANN) models. The focus will be on recurrent neural networks (RNNs). The idea is that in order to properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Load Packages and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cleaned train and test data sets if they are present, otherwise raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "try:\n",
    "    train = joblib.load('data/interim/train.pkl')\n",
    "    test = joblib.load('data/interim/test.pkl')\n",
    "except FileNotFoundError:\n",
    "    #need to run earlier notebook if files not present\n",
    "    raise Exception(\"Files not found. Run Notebook 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Create Emedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn countvectorizer to create a dictionary of tokens with indexes. Use nltk tokenizer instead of sklearn's built in tokenizer since nltk doesn't, comeply igonore puntation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=wordpunct_tokenize, min_df=5)\n",
    "vectorizer.fit(train[\"full_text\"])\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function download the Glove embeddings\n",
    "#it will only be called if nececary\n",
    "def download_Glove():\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    out_name='data/external/GloveVectors'\n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    z.extractall(path = out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function takes two inputs, path to embedding file and the index\n",
    "#word index is a dictionary, with key a word and item as index number \n",
    "#this funtion reads the embedding file into a dictionary, one key perline/word\n",
    "#item is ebedding\n",
    "#after doing this, creates an embedding matrix with nuimber of rows equal to number of word ined\n",
    "# it then iterates over \n",
    "def create_embedmatrix(embedding_file, word_index):\n",
    "    #word embedding\n",
    "    embeddings_index = {}\n",
    "    not_found = {}\n",
    "    try:\n",
    "        f = open(embedding_file, encoding=\"utf8\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pretrain Glove Embedding not found. Downloading them.\")\n",
    "        download_Glove()\n",
    "        f = open(embedding_file, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    embedding_dim = next(iter(embeddings_index.values())).shape[0]\n",
    "    \n",
    "    #now make embedding\n",
    "    #reserving row 0 for for the padding character - will be all 0s so can be masked laer\n",
    "    #reserving row 1 for words that are not in vocab (will be all 0s and masked from neural network)\n",
    "    #words that are in vocab but not in the embeddings will get their own rows\n",
    "    #might want to train them later (intitliaze with random)\n",
    "    #make from truncated normal, parameters from loading the embddeing\n",
    "    lower = -2\n",
    "    upper = 2\n",
    "    mu = 0.0 # mean\n",
    "    sigma = 0.5 #standard deviation\n",
    "    embedding_matrix = scipy.stats.truncnorm.rvs(\n",
    "              (lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma,size=(len(word_index),embedding_dim))\n",
    "    #make first row all zeroes, for masking of padding\n",
    "    embedding_matrix[0,:] = 0.\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i+1] = embedding_vector\n",
    "        else:\n",
    "            not_found[word] = not_found.get(word, 0) + 1   \n",
    "    return embedding_matrix, embedding_dim, not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, embedding_dim, not_found = create_embedmatrix('data/external/GloveVectors/glove.6B.50d.txt', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there were 1507 words that did not have a corresponding entry in the pretrained vectors. Their values have been initilziaed randomly\n"
     ]
    }
   ],
   "source": [
    "print(\"there were {} words that did not have a corresponding entry in the pretrained vectors. Their values \"\n",
    "      \"have been initilziaed randomly\".format(len(not_found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows the need to be able to update word embeddings, isntead of just ignoring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/interim/embeddings50.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now output\n",
    "joblib.dump(embedding_matrix, \"data/interim/embeddings50.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, embedding_dim, not_found = create_embedmatrix('data/external/GloveVectors/glove.6B.300d.txt', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/interim/embeddings300.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now output\n",
    "joblib.dump(embedding_matrix, \"data/interim/embeddings300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform into sequences. Should really make into pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, need to decide on a max length\n",
    "#actually, do this later, sin e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to justify why max length of 300\n",
    "#longer length doesn't really costs anything...\n",
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_seqs(texts, vocab, max_len):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.append(wordpunct_tokenize(text.lower()))\n",
    "    seqs = np.zeros((len(tokens), max_len), dtype=np.int32)\n",
    "    \n",
    "    for i, text in enumerate(tokens):\n",
    "        for j, word in enumerate(text):\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            #need to increment by 1 since first row in embedding matrix is reserved\n",
    "            #if word doesn't exist, it will return -1, whicll be incrmented to 1\n",
    "            seqs[i,j] = vocab.get(word, -1) + 2\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqs = create_seqs(train[\"full_text\"], vocab, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need validation set\n",
    "seqs_train, seqs_valid, y_train, y_valid = train_test_split(seqs, train.funny.values, test_size = 0.125, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/processed/train_nn.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output train seqs\n",
    "joblib.dump({\"seqs\":seqs_train, \"labels\":y_train},\"data/processed/train_nn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/processed/valid_nn.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output validation seqs\n",
    "joblib.dump({\"seqs\":seqs_valid, \"labels\":y_valid},\"data/processed/valid_nn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create test seqs\n",
    "seqs_test = create_seqs(test[\"full_text\"], vocab, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/processed/test_nn.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output test seqs\n",
    "joblib.dump({\"seqs\":seqs_test, \"labels\":test.funny.values},\"data/processed/test_nn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
